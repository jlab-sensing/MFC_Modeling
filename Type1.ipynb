{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlab-sensing/MFC_Modeling/blob/main/Type1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcF-Dv23N_ON",
        "outputId": "56bb1895-7748-43ad-b2ae-e8322e1fb75e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hepml\n",
            "  Downloading hepml-0.0.12-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from hepml) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from hepml) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hepml) (1.25.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from hepml) (0.13.1)\n",
            "Collecting black (from hepml)\n",
            "  Downloading black-24.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hepml) (4.66.4)\n",
            "Collecting wget (from hepml)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nbdev (from hepml)\n",
            "  Downloading nbdev-2.3.25-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sklearn-pandas in /usr/local/lib/python3.10/dist-packages (from hepml) (2.2.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from hepml) (0.20.3)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from hepml) (5.1.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from hepml) (14.0.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from hepml) (0.58.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from hepml) (3.0.10)\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.10/dist-packages (from hepml) (1.0.3)\n",
            "Collecting giotto-tda (from hepml)\n",
            "  Downloading giotto_tda-0.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from hepml) (9.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->hepml)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (24.0)\n",
            "Collecting pathspec>=0.9.0 (from black->hepml)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (4.12.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->hepml) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->hepml) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->hepml) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (1.4.2)\n",
            "Collecting scikit-learn (from hepml)\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting giotto-ph>=0.2.1 (from giotto-tda->hepml)\n",
            "  Downloading giotto_ph-0.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (554 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.6/554.6 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyflagser>=0.4.3 (from giotto-tda->hepml)\n",
            "  Downloading pyflagser-0.4.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting igraph>=0.9.8 (from giotto-tda->hepml)\n",
            "  Downloading igraph-0.11.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.8.2 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (5.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (7.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->hepml) (3.5.0)\n",
            "Requirement already satisfied: fastcore>=1.5.27 in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (1.5.43)\n",
            "Collecting execnb>=0.1.4 (from nbdev->hepml)\n",
            "  Downloading execnb-0.1.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (1.6.3)\n",
            "Collecting ghapi>=1.0.3 (from nbdev->hepml)\n",
            "  Downloading ghapi-1.0.5-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchdog (from nbdev->hepml)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asttokens (from nbdev->hepml)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (6.0.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->hepml) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->hepml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hepml) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hepml) (2024.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->hepml) (3.7.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from execnb>=0.1.4->nbdev->hepml) (7.34.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from ghapi>=1.0.3->nbdev->hepml) (23.1.2)\n",
            "Collecting texttable>=1.6.2 (from igraph>=0.9.8->giotto-tda->hepml)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (3.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.8.2->giotto-tda->hepml) (8.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->hepml) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->nbdev->hepml) (0.43.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->hepml) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (1.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->giotto-tda->hepml) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->giotto-tda->hepml) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->execnb>=0.1.4->nbdev->hepml)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (3.0.45)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->execnb>=0.1.4->nbdev->hepml) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->execnb>=0.1.4->nbdev->hepml) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->execnb>=0.1.4->nbdev->hepml) (0.2.13)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (4.9.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.18.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.2.1)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=e9e29dadeab7e7591db8f94c9fd6fdb9e8806292c02218ff18636a6eaafd9a27\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, texttable, watchdog, pathspec, mypy-extensions, jedi, igraph, asttokens, scikit-learn, pyflagser, ghapi, black, giotto-ph, execnb, nbdev, giotto-tda, hepml\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed asttokens-2.4.1 black-24.4.2 execnb-0.1.6 ghapi-1.0.5 giotto-ph-0.2.4 giotto-tda-0.6.2 hepml-0.0.12 igraph-0.11.5 jedi-0.19.1 mypy-extensions-1.0.0 nbdev-2.3.25 pathspec-0.12.1 pyflagser-0.4.7 scikit-learn-1.3.2 texttable-1.7.0 watchdog-4.0.1 wget-3.2\n",
            "Collecting arrow\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow) (1.16.0)\n",
            "Installing collected packages: types-python-dateutil, arrow\n",
            "Successfully installed arrow-1.3.0 types-python-dateutil-2.9.0.20240316\n",
            "Collecting keras_lr_finder\n",
            "  Downloading keras_lr_finder-0.1-py2.py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from keras_lr_finder) (2.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from keras_lr_finder) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->keras_lr_finder) (1.16.0)\n",
            "Installing collected packages: keras_lr_finder\n",
            "Successfully installed keras_lr_finder-0.1\n",
            "Mounted at /content/drive\n",
            "Archive:  drive/MyDrive/jLab Shared Docs/MFC Modeling/stanfordMFCDataset.zip\n",
            "   creating: rocket4/\n",
            "  inflating: rocket4/TEROSoutput-1622841510-f10.csv  \n",
            "  inflating: rocket4/soil_20210712-131708_23.csv  \n",
            "  inflating: rocket4/TEROSoutput-1624897043-f20.csv  \n",
            "  inflating: rocket4/soil_20210720-141711_24.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622841597-f14.csv  \n",
            "  inflating: rocket4/TEROSoutput-1639124239-f40.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622841510-f13.csv  \n",
            "  inflating: rocket4/soil_20211122-091709_37.csv  \n",
            "  inflating: rocket4/soil_20210706-151713_22.csv  \n",
            "   creating: rocket4/30s/\n",
            "  inflating: rocket4/TEROSoutput-1635243439-f31.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622841447-f12.csv  \n",
            "  inflating: rocket4/TEROSoutput-1639718260-f41.csv  \n",
            "  inflating: rocket4/TEROSoutput-1632482240-f29.csv  \n",
            "  inflating: rocket4/soil_20210609-161708_15.csv  \n",
            "  inflating: rocket4/soil_20211015-091709_30.csv  \n",
            "  inflating: rocket4/soil_20210625-131716_19.csv  \n",
            "  inflating: rocket4/soil_20210618-141709_17.csv  \n",
            "  inflating: rocket4/soil_20210604-201708_7.csv  \n",
            "  inflating: rocket4/soil_20210604-201819_6.csv  \n",
            "  inflating: rocket4/soil_20210907-121709_28.csv  \n",
            "  inflating: rocket4/TEROSoutput-1637734644-f38.csv  \n",
            "  inflating: rocket4/soil_20210827-111709_26.csv  \n",
            "  inflating: rocket4/soil_20210505-215837_3.csv  \n",
            "  inflating: rocket4/TEROSoutput-1636107444-f32.csv  \n",
            "  inflating: rocket4/soil_20210604-211832_9.csv  \n",
            "  inflating: rocket4/soil_20211111-091715_33.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622840881-f8.csv  \n",
            "  inflating: rocket4/soil_20210604-211948_14.csv  \n",
            "  inflating: rocket4/soil_20210604-211718_12.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622841524-f11.csv  \n",
            "  inflating: rocket4/soil_20220128-071708_42.csv  \n",
            "  inflating: rocket4/TEROSoutput-1626790640-f24.csv  \n",
            "  inflating: rocket4/soil_20210628-161713_20.csv  \n",
            "  inflating: rocket4/soil_20210604-211820_13.csv  \n",
            "  inflating: rocket4/soil_20211111-091819_35.csv  \n",
            "  inflating: rocket4/soil_20210614-161711_16.csv  \n",
            "  inflating: rocket4/TEROSoutput-1636622244-f33.csv  \n",
            "  inflating: rocket4/soil_20210702-131719_21.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622773815-f5.csv  \n",
            "  inflating: rocket4/TEROSoutput-1624295841-f18.csv  \n",
            "  inflating: rocket4/soil_20211026-101710_31.csv  \n",
            "  inflating: rocket4/soil_20210604-211822_10.csv  \n",
            "  inflating: rocket4/TEROSoutput-1638249493-f39.csv  \n",
            "  inflating: rocket4/TEROSoutput-1620251923-f3.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622841522-f9.csv  \n",
            "  inflating: rocket4/soil_20210901-121710_27.csv  \n",
            "  inflating: rocket4/soil_20210924-111711_29.csv  \n",
            "  inflating: rocket4/TEROSoutput-1637572639-f37.csv  \n",
            "  inflating: rocket4/TEROSoutput-1636622246-f34.csv  \n",
            "  inflating: rocket4/soil_20210413-225754_2.csv  \n",
            "  inflating: rocket4/soil_20210604-211834_11.csv  \n",
            "  inflating: rocket4/TEROSoutput-1625584642-f22.csv  \n",
            "  inflating: rocket4/soil_20210604-022956_5.csv  \n",
            "  inflating: rocket4/soil_20211210-081709_40.csv  \n",
            "  inflating: rocket4/soil_20210603-010213_4.csv  \n",
            "  inflating: rocket4/soil_20210621-171712_18.csv  \n",
            "  inflating: rocket4/TEROSoutput-1624627044-f19.csv  \n",
            "  inflating: rocket4/TEROSoutput-1636622308-f35.csv  \n",
            "  inflating: rocket4/TEROSoutput-1636623222-f36.csv  \n",
            "  inflating: rocket4/soil_20211217-051731_41.csv  \n",
            "  inflating: rocket4/soil_20211124-061714_38.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622837909-f6.csv  \n",
            "  inflating: rocket4/soil_20210604-201708_8.csv  \n",
            "  inflating: rocket4/soil_20210817-131708_25.csv  \n",
            "   creating: rocket4/tmp/\n",
            "  inflating: rocket4/TEROSoutput-1643354237-f42.csv  \n",
            "  inflating: rocket4/soil_20211111-091716_34.csv  \n",
            "  inflating: rocket4/TEROSoutput-1618354681-f2.csv  \n",
            "  inflating: rocket4/soil_20211111-093334_36.csv  \n",
            "  inflating: rocket4/soil_20211130-051804_39.csv  \n",
            "  inflating: rocket4/TEROSoutput-1622682139-f4.csv  \n",
            "  inflating: rocket4/soil_20211105-101714_32.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1639124239-f40.csv  \n",
            "  inflating: rocket4/30s/soil_20211122-091709_37.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1635243439-f31.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1639718260-f41.csv  \n",
            "  inflating: rocket4/30s/soil_20211015-091709_30.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1637734644-f38.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1636107444-f32.csv  \n",
            "  inflating: rocket4/30s/soil_20211111-091715_33.csv  \n",
            "  inflating: rocket4/30s/soil_20211111-091819_35.csv  \n",
            "  inflating: rocket4/30s/soil_20211026-101710_31.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1638249493-f39.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1637572639-f37.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1636622246-f34.csv  \n",
            "  inflating: rocket4/30s/soil_20211210-081709_40.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1636622308-f35.csv  \n",
            "  inflating: rocket4/30s/TEROSoutput-1636623222-f36.csv  \n",
            "  inflating: rocket4/30s/soil_20211217-051731_41.csv  \n",
            "  inflating: rocket4/30s/soil_20211124-061714_38.csv  \n",
            "  inflating: rocket4/30s/soil_20211111-091716_34.csv  \n",
            "  inflating: rocket4/30s/soil_20211111-093334_36.csv  \n",
            "  inflating: rocket4/30s/soil_20211130-051804_39.csv  \n",
            "  inflating: rocket4/30s/soil_20211105-101714_32.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622841510-f10.csv  \n",
            "  inflating: rocket4/tmp/soil_20210712-131708_23.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1624897043-f20.csv  \n",
            "  inflating: rocket4/tmp/soil_20210720-141711_24.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622841597-f14.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622841510-f13.csv  \n",
            "  inflating: rocket4/tmp/soil_20210706-151713_22.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622841447-f12.csv  \n",
            "  inflating: rocket4/tmp/soil_20210609-161708_15.csv  \n",
            "  inflating: rocket4/tmp/soil_20210625-131716_19.csv  \n",
            "  inflating: rocket4/tmp/soil_20210618-141709_17.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-201708_7.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-201819_6.csv  \n",
            "   creating: rocket4/tmp/figs/\n",
            "  inflating: rocket4/tmp/soil_20210827-111709_26.csv  \n",
            "  inflating: rocket4/tmp/soil_20210505-215837_3.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-211832_9.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622840881-f8.csv  \n",
            "   creating: rocket4/tmp/pkls/\n",
            "   creating: rocket4/tmp/old/\n",
            "  inflating: rocket4/tmp/soil_20210604-211948_14.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-211718_12.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622841524-f11.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1626790640-f24.csv  \n",
            "  inflating: rocket4/tmp/soil_20210628-161713_20.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-211820_13.csv  \n",
            "  inflating: rocket4/tmp/soil_20210614-161711_16.csv  \n",
            "  inflating: rocket4/tmp/soil_20210702-131719_21.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622773815-f5.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1624295841-f18.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-211822_10.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1620251923-f3.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622841522-f9.csv  \n",
            "  inflating: rocket4/tmp/soil_20210413-225754_2.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-211834_11.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1625584642-f22.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-022956_5.csv  \n",
            "  inflating: rocket4/tmp/soil_20210603-010213_4.csv  \n",
            "  inflating: rocket4/tmp/soil_20210621-171712_18.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1624627044-f19.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622837909-f6.csv  \n",
            "  inflating: rocket4/tmp/soil_20210604-201708_8.csv  \n",
            "  inflating: rocket4/tmp/soil_20210817-131708_25.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1618354681-f2.csv  \n",
            "  inflating: rocket4/tmp/TEROSoutput-1622682139-f4.csv  \n",
            "  inflating: rocket4/tmp/figs/sensor-0_from-2021-04-13 15:58:01-07:00_to-2021-05-05 14:59:28-07:00.png  \n",
            "  inflating: __MACOSX/rocket4/tmp/figs/._sensor-0_from-2021-04-13 15:58:01-07:00_to-2021-05-05 14:59:28-07:00.png  \n",
            "  inflating: rocket4/tmp/figs/sensor-1_from-2021-04-13 15:58:12-07:00_to-2021-05-05 14:59:38-07:00.png  \n",
            "  inflating: rocket4/tmp/pkls/teros_data.pkl  \n",
            "  inflating: rocket4/tmp/pkls/seen_datasets.txt  \n",
            "  inflating: rocket4/tmp/old/TEROSoutput-1615346184-f1.csv  \n",
            "  inflating: rocket4/tmp/old/soil_20210413-225754_2.csv  \n",
            "  inflating: rocket4/tmp/old/TEROSoutput-1618354681-f2.csv  \n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade hepml\n",
        "!pip install arrow\n",
        "!pip install keras_lr_finder\n",
        "# reload modules before executing user code\n",
        "#%load_ext autoreload\n",
        "# reload all modules every time before executing Python code\n",
        "#%autoreload 2\n",
        "# render plots in notebook\n",
        "%matplotlib inline\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from hepml.core import plot_regression_tree\n",
        "sns.set(color_codes=True)\n",
        "sns.set_palette(sns.color_palette(\"muted\"))\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7PoS8fwOS-Q"
      },
      "outputs": [],
      "source": [
        "#Load teros data\n",
        "import glob\n",
        "teros_files = glob.glob(\"rocket4/TEROSoutput*.csv\")\n",
        "X = pd.DataFrame()\n",
        "for f in teros_files:\n",
        "  try:\n",
        "    csv = pd.read_csv(f, index_col=False).dropna()\n",
        "    X = pd.concat([X, csv])\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaaHwFRvXtN4"
      },
      "outputs": [],
      "source": [
        "#Load power data\n",
        "power_files = glob.glob(\"rocket4/soil*.csv\")\n",
        "y = pd.DataFrame()\n",
        "for f in sorted(power_files, key=lambda x: int(x.split('.')[0].split('_')[-1])):\n",
        "#in power_files:\n",
        "  try:\n",
        "    csv = pd.read_csv(f, on_bad_lines='skip', skiprows=10).dropna(how='all')\n",
        "    csv = csv.rename({'Unnamed: 0': 'timestamp'}, axis='columns')\n",
        "    y = pd.concat([y,csv])\n",
        "  except:\n",
        "    continue\n",
        "y[\"timestamp\"] = y[\"timestamp\"].round(decimals = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViS0TjC4n6lk"
      },
      "outputs": [],
      "source": [
        "#Convert current to amps, voltage to volts\n",
        "y[\"I1L [10pA]\"] = np.abs(y[\"I1L [10pA]\"] * 1E-11)\n",
        "y[\"V1 [10nV]\"] = np.abs(y[\"V1 [10nV]\"] * 1E-8)\n",
        "y[\"I1H [nA]\"] = np.abs(y[\"I1H [nA]\"] * 1E-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0aqozsZpSOB",
        "outputId": "54031d0e-97e6-4f2d-e86c-4f0bbfef4a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-38f7a5b5fe7c>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n"
          ]
        }
      ],
      "source": [
        "#Sort data by timestamp, convert to datetime\n",
        "X = X.sort_values(['timestamp'])\n",
        "y = y.sort_values(['timestamp'])\n",
        "X['timestamp'] = pd.to_datetime(X['timestamp'], unit='s')\n",
        "y['timestamp'] = pd.to_datetime(y['timestamp'], unit='s')\n",
        "\n",
        "#Merge data by timestamp\n",
        "uncut_df = pd.merge_asof(left=X,right=y,direction='nearest',tolerance=pd.Timedelta('1 sec'), on = 'timestamp').dropna(how='all')\n",
        "\n",
        "#Isolate data from cell0\n",
        "df = uncut_df.loc[uncut_df['sensorID'] == 0]\n",
        "\n",
        "#Localize timestamp\n",
        "df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBAfOHB61Jwc"
      },
      "outputs": [],
      "source": [
        "#Use only data from after deployment date\n",
        "#df = df.loc[(df['timestamp'] > '2021-09-24') & (df['timestamp'] < '2021-10-15')] #Future of Clean Computing Graph\n",
        "df = df.loc[(df['timestamp'] > '2021-06-04')] #Deployment date\n",
        "\n",
        "#Power drop\n",
        "#df = df.loc[(df['timestamp'] > '2021-11-01') & (df['timestamp'] < '2021-11-22')]\n",
        "\n",
        "#Drop data outages\n",
        "df = df.drop(df[(df.timestamp > '2021-11-11') & (df.timestamp < '2021-11-22 01:00:00')].index)\n",
        "df = df.drop(df[(df.timestamp > '2022-01-27')].index)\n",
        "#df = df.set_index('timestamp')\n",
        "df = df[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.set_index('timestamp')"
      ],
      "metadata": {
        "id": "p8fFsoLNfrFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kNxDOBwk7IN"
      },
      "outputs": [],
      "source": [
        "#Get time since deployement\n",
        "df['tsd'] = (df.index - df.index[0]).days\n",
        "df['hour'] = (df.index).hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pfyv0fM3te1"
      },
      "outputs": [],
      "source": [
        "#Calculate power\n",
        "df[\"power\"] = np.abs(np.multiply(df.iloc[:, 7], df.iloc[:, 8]))\n",
        "#df[\"power\"] = np.abs(np.multiply(df[\"I1L [10pA]\"], df[\"V1 [10nV]\"]))\n",
        "\n",
        "#Convert to nW\n",
        "df['power'] = df['power']*1E9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA-WVzVh2-lf"
      },
      "outputs": [],
      "source": [
        "#Convert to 10 nanoamps, 10 microvolts\n",
        "df[\"I1L [10pA]\"] = np.abs(df[\"I1L [10pA]\"] * 1E8)\n",
        "df[\"V1 [10nV]\"] = np.abs(df[\"V1 [10nV]\"] * 1E5)\n",
        "df[\"I1H [nA]\"] = np.abs(df[\"I1H [nA]\"] * 1E8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reset_index()"
      ],
      "metadata": {
        "id": "XxAlm4FXh57m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sHtXZCVrsQJ"
      },
      "outputs": [],
      "source": [
        "#Add power time series\n",
        "df['power - 1h'] = df['power'].shift(1).dropna()\n",
        "df['power - 2h'] = df['power'].shift(2).dropna()\n",
        "df['power - 3h'] = df['power'].shift(3).dropna()\n",
        "#df['power - 2h'] = df['power'].shift(2).dropna()\n",
        "#df['previous_power - 3'] = df['power'].shift(3).dropna()\n",
        "#df['previous_power - 4'] = df['power'].shift(4).dropna()\n",
        "\n",
        "#Add teros time series\n",
        "df['EC - 1h'] = df['EC'].shift(1).dropna()\n",
        "df['EC - 2h'] = df['EC'].shift(2).dropna()\n",
        "df['EC - 3h'] = df['EC'].shift(3).dropna()\n",
        "\n",
        "df['temp - 1h'] = df['temp'].shift(1).dropna()\n",
        "df['temp - 2h'] = df['temp'].shift(2).dropna()\n",
        "df['temp - 3h'] = df['temp'].shift(3).dropna()\n",
        "\n",
        "df['raw_VWC - 1h'] = df['raw_VWC'].shift(1).dropna()\n",
        "df['raw_VWC - 2h'] = df['raw_VWC'].shift(2).dropna()\n",
        "df['raw_VWC - 3h'] = df['raw_VWC'].shift(3).dropna()\n",
        "\n",
        "#Add voltage and current time series\n",
        "df['V1 - 1h'] = df['V1 [10nV]'].shift(1).dropna()\n",
        "df['V1 - 2h'] = df['V1 [10nV]'].shift(2).dropna()\n",
        "df['V1 - 3h'] = df['V1 [10nV]'].shift(3).dropna()\n",
        "\n",
        "df['I1L - 1h'] = df['I1L [10pA]'].shift(1).dropna()\n",
        "df['I1L - 2h'] = df['I1L [10pA]'].shift(2).dropna()\n",
        "df['I1L - 3h'] = df['I1L [10pA]'].shift(3).dropna()\n",
        "\n",
        "df['I1H - 1h'] = df['I1H [nA]'].shift(1).dropna()\n",
        "df['I1H - 2h'] = df['I1H [nA]'].shift(2).dropna()\n",
        "df['I1H - 3h'] = df['I1H [nA]'].shift(3).dropna()\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4SM1_EvGS6y"
      },
      "outputs": [],
      "source": [
        "#df = df.rename(columns={'power': 'power [μW]'})\n",
        "df = df.rename(columns={'I1L [10pA]': 'Current (uA)', 'V1 [10nV]' : 'Voltage (mV)', 'power' : 'Power (uW)'})\n",
        "df = df.set_index('timestamp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "9z02Ua7NOMzq",
        "outputId": "524d4c89-7cbc-4163-a45e-d88cc072ccde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-efccdb65165b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm8_60min_quant5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mv lstm8_60min_quant5 'drive/MyDrive/jLab Shared Docs/MFC Modeling'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "model.save(\"lstm8_60min_quant5\", overwrite=True, save_format=None)\n",
        "!mv lstm8_60min_quant5 'drive/MyDrive/jLab Shared Docs/MFC Modeling'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#New runtime calculation\n",
        "import math\n",
        "from dateutil import parser\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def internal_R_v3(R=2000): #return internal resistance of v3 cells in ohms\n",
        "    #https://www.jstage.jst.go.jp/article/jwet/20/1/20_21-087/_pdf\n",
        "    v0_oc = 48.5e-3 #48.5 mV\n",
        "    v0_cc = 4.8e-3\n",
        "    v0_r = R*((v0_oc/v0_cc)-1)\n",
        "\n",
        "    v1_oc = 43.8e-3\n",
        "    v1_cc = 20.9e-3\n",
        "    v1_r = R*((v1_oc/v1_cc)-1)\n",
        "\n",
        "    v2_oc = 45.2e-3\n",
        "    v2_cc = 23.5e-3\n",
        "    v2_r = R*((v2_oc/v2_cc)-1)\n",
        "\n",
        "    return (v0_r+v1_r+v2_r)/3\n",
        "\n",
        "def internal_R_v0(R=2000): #return internal resistance of v0 cells in ohms\n",
        "    v3_oc = 41.7e-3 #41.7mV\n",
        "    v3_cc = 5.1e-3\n",
        "    v3_r = R*((v3_oc/v3_cc)-1)\n",
        "\n",
        "    v4_oc = 48.7e-3\n",
        "    v4_cc = 16.8e-3\n",
        "    v4_r = R*((v4_oc/v4_cc)-1)\n",
        "\n",
        "    v5_oc = 39.1e-3\n",
        "    v5_cc = 16.9e-3\n",
        "    v5_r = R*((v5_oc/v5_cc)-1)\n",
        "\n",
        "    return (v3_r+v4_r+v5_r)/3\n",
        "\n",
        "def SMFC_current(v, R):\n",
        "    return v/R\n",
        "\n",
        "#MODEL\n",
        "def cap_leakage(E_cap_tn, timestep):\n",
        "    #Spec for KEMET T491\n",
        "    return 0.01e-6 * E_cap_tn * timestep\n",
        "\n",
        "def Matrix_Power(V, R):\n",
        "    #efficiency interpolated from https://www.analog.com/media/en/technical-documentation/data-sheets/ADP5091-5092.pdf\n",
        "    #given I_in = 100 uA and SYS = 3V\n",
        "    #V is the voltage (V) of the SMFC we captured\n",
        "    #R is the resistance (ohms) of the load we used to get that voltage trace\n",
        "    #Eta = -292.25665*V**4 + 784.30311*V**3 - 770.71691*V**2 + 342.00502*V + 15.83307\n",
        "    #Eta = Eta/100\n",
        "    Eta = 0.60\n",
        "    Pmax = (V**2)/R\n",
        "    Pout = Eta*Pmax\n",
        "    #assert((Eta > 0) & (Eta < 1))\n",
        "    #assert(Pout < 12000e-6)\n",
        "    return Pout\n",
        "\n",
        "def update_capEnergy(e0, V_applied, R, C, dt):\n",
        "    # e0: initial energy stored\n",
        "    # V_applied: voltage from SMFC\n",
        "    # R: internal resistance of SMFC\n",
        "    # C: capacitance of capacitor\n",
        "    # dt: time step since last data point\n",
        "    e_cap = e0 + Matrix_Power(V_applied, R)*dt - cap_leakage(e0, dt)\n",
        "    v_cap = math.sqrt(2*e_cap/C)\n",
        "    if e_cap < 0: #Not charging if leakage is greater than energy\n",
        "        e_cap = 0\n",
        "\n",
        "    return e_cap, v_cap #output final e and v\n",
        "\n",
        "def Advanced_energy():\n",
        "    #Now representing \"Advanced\"\n",
        "    #startup time of 2500 ms\n",
        "    t = 2500e-3\n",
        "    e = 2.4 * 128e-3 * t\n",
        "    e_startup = 2.4 * 128e-3 * 5e-3\n",
        "    return e+e_startup\n",
        "\n",
        "def Minimal_energy():\n",
        "    #Now representing \"Minimal\"\n",
        "    t = 0.888e-3 #tentative time\n",
        "    e = 0.9 * 4.8e-3 * t #this uses average current\n",
        "    e_startup = 0#assume negligible, no known startup time given\n",
        "    return  e + e_startup\n",
        "\n",
        "def Analog_energy():\n",
        "    #Now representing Analog\n",
        "    t = 1e-3 #estimated operating time\n",
        "    e = 0.11 * 2.15e-6 * t\n",
        "    e_startup = 0 #analog device, no startup needed :)\n",
        "    return e + e_startup\n",
        "\n",
        "#STEP 3:\n",
        "# For each day:\n",
        "#   on_Minimal, on_Advanced, on_Analog = 0\n",
        "#   For each time step (like every 60 s given our logging freq):\n",
        "#       - Update the energy in our capacitor (put fcn in models.py) given (1) input voltage, (2) time step, (3) capacitance (prob 10 uF), this will be an integral\n",
        "#       - Check if energy is enough to turn on (1) 1 uJ load, (2) 10 uJ load, and (3) 20 uJ load (will tweak later to reflect real energy cost of each system)\n",
        "#       - If so, add to on_Minimal, on_Advanced, and on_Analog and reset capacitor energy to 0 J (might tweak this value)\n",
        "#   Append on_Minimal, on_Advanced, on_Analog to on_Minimal_list, on_Advanced_list, on_Analog_list. This will be a list of how many sensor readings we are able to take with each of these systems every day given the energy we got\n",
        "#STEP 4: Visualize the daily # of readings with 3 bar graphs, y axis is # of readings and x axis is days.\n",
        "#   - Given 3 lists of integer values, plot them on bar graphs\n",
        "\n",
        "def group_util(test_date1, test_date2, N):\n",
        "    diff = (test_date2 - test_date1) / N\n",
        "    return [test_date1 + diff * idx for idx in range(N)] + [test_date2]\n",
        "\n",
        "def oracle_simulate(v_list, C_h, time_frame_seconds):\n",
        "    #Calculate maximum energy\n",
        "    total_E = 0\n",
        "    for i in range(len(v_list) - 1):\n",
        "        t = (v_list.index[i+1] - v_list.index[i]).total_seconds()\n",
        "        if t > 9000:\n",
        "          print(\"Discontinuity\")\n",
        "          print(v_list.index[i+1], v_list.index[i])\n",
        "          print(v_list['Voltage (mV)'][i+1], v_list['Voltage (mV)'][i])\n",
        "          #total_E, ignore = update_capEnergy(total_E, V_applied=(v_list['V1 [mV]'][i+1] + v_list['V1 [mV]'][i])/2, R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "        else:\n",
        "          total_E, ignore = update_capEnergy(total_E, V_applied=max(v_list['Voltage (mV)'][i], v_list['Voltage (mV)'][i+1]), R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "    print(\"Oracle activations:\", math.floor(total_E/Minimal_energy()))\n",
        "    return(math.floor(total_E/Minimal_energy()))\n",
        "\n",
        "def naive_simulate(t_list, v_list, v_list_naive, v_list_fine, C_h):\n",
        "    # t_list: list of decimal time stamps in unit of days (e.g. 71.85893518518519 day), same length as v_list\n",
        "    # v_list: list of voltage values from SFMC\n",
        "    # C_h: capacitance of the capacitor being filled up by harvester\n",
        "\n",
        "    #assume capacitor is completely discharged at start\n",
        "    e_minimal_stored = 0\n",
        "    e_minimal_stored_theo = 0\n",
        "\n",
        "    #Initialize evaluation metrics\n",
        "    false_act = 0\n",
        "    max_act = 0\n",
        "    pred_act = 0\n",
        "    succ_act = 0\n",
        "\n",
        "    total_E = 0\n",
        "    total_E_naive = 0\n",
        "\n",
        "    #Calculate maximum energy\n",
        "    #for i in range(len(v_list_fine) - 1):\n",
        "    #    t = (v_list_fine.index[i+1] - v_list_fine.index[i]).total_seconds()\n",
        "    #    total_E, ignore = update_capEnergy(total_E, V_applied=v_list_fine['V1 [10nV]'][i], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "    #print(total_E/Minimal_energy())\n",
        "    v = v_list_naive.mean()\n",
        "    #for each voltage data point\n",
        "    for jj in range(len(v_list) - 1): #last data point was at 71.85893518518519 day\n",
        "        t = (v_list.index[jj+1] - v_list.index[jj]).total_seconds()\n",
        "        if t <= time_frame_seconds:\n",
        "          #Total predicted vs. actual energy stored\n",
        "          #Predict energy stored during scheduled sub-interval\n",
        "          total_E, ignore = update_capEnergy(total_E, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "          total_E_naive, ignore = update_capEnergy(total_E_naive, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "\n",
        "          E_Minimal_pred, v_minimal_pred = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = t) #set dt as length of prediction interval, in seconds\n",
        "          pred_act += math.floor(E_Minimal_pred/Minimal_energy()) #Update number of activations predicted\n",
        "          itn = 0\n",
        "          if math.floor(E_Minimal_pred/Minimal_energy()) > 0:\n",
        "              minimal_intervals = [date for date in group_util(v_list.index[jj], v_list.index[jj] + timedelta(seconds=t), math.floor(E_Minimal_pred/Minimal_energy()))]\n",
        "              #Calculate desired interval\n",
        "              int_len = time_frame_seconds /  math.floor(E_Minimal_pred/Minimal_energy())\n",
        "              for i in range(len(minimal_intervals) - 1):\n",
        "                  #Determine actual energy stored during scheduled sub-interval\n",
        "                  start = v_list_fine.index.searchsorted(minimal_intervals[i])\n",
        "                  end =  v_list_fine.index.searchsorted(minimal_intervals[i+1])\n",
        "\n",
        "                  E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list_fine.iloc[start:end]['Voltage (mV)'].mean(), R=internal_R_v0(), C=C_h[0], dt = int_len)\n",
        "                  if not math.isnan(v_list_fine.iloc[start:end]['Voltage (mV)'].mean()):\n",
        "                    if E_Minimal < Minimal_energy():\n",
        "                        false_act += 1\n",
        "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
        "                        itn += 1\n",
        "\n",
        "                    elif E_Minimal >= Minimal_energy():\n",
        "                        succ_act += 1\n",
        "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
        "                        itn+= 1\n",
        "\n",
        "                    else:\n",
        "                      print('Error')\n",
        "                      print(e_minimal_stored, v)\n",
        "\n",
        "                  #Unit test\n",
        "                  #else:\n",
        "                  #  print(\"?\")\n",
        "                  #  print(v_list_fine.index[start])\n",
        "                  #  print(v_list_fine.index[end])\n",
        "                  #  print(minimal_intervals[i], minimal_intervals[i+1])\n",
        "\n",
        "              #Unit test\n",
        "              #if itn != math.floor(E_Minimal_pred/Minimal_energy()):\n",
        "              #    print(\"itn not matching\")\n",
        "              #    print(itn, math.floor(E_Minimal_pred/Minimal_energy()))\n",
        "              #    continue\n",
        "\n",
        "          else:\n",
        "              e_minimal_stored, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "              #Added this\n",
        "              #start = v_list_fine.index.searchsorted(v_list.index[jj])\n",
        "              #end =  v_list_fine.index.searchsorted(v_list.index[jj+1])\n",
        "              #for h in range(start, end):\n",
        "              #    v = v_list_fine.iloc[h]['V1 [mV]']\n",
        "              #    interval_length = ((v_list_fine.index[h+1]) - (v_list_fine.index[h])).total_seconds()\n",
        "              #    E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = interval_length)\n",
        "              #    e_minimal_stored = E_Minimal\n",
        "\n",
        "\n",
        "        else:\n",
        "          print(\"It's over 9000!\", v_list.index[jj], v_list.index[jj+1])\n",
        "\n",
        "    print(\"Naive total_E activations:\", total_E/Minimal_energy())\n",
        "    print(\"Naive total_E_pred activations:\", total_E_naive/Minimal_energy())\n",
        "    return pred_act, false_act, succ_act, total_E_naive\n",
        "\n",
        "def getMax(c_list, input_list):\n",
        "    max_value = max(input_list)\n",
        "    i = [index for index, item in enumerate(input_list) if item == max_value][0]\n",
        "    return i, max_value, c_list[i]\n",
        "\n",
        "\n",
        "#SMFC\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "from scipy.signal import butter, lfilter\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "def butter_lowpass(cutoff, fs, order=5):\n",
        "        return butter(order, cutoff, fs=fs, btype='low', analog=False)\n",
        "\n",
        "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def getMFC_data(y_test, test_pred):\n",
        "    unix_time = y_test.index\n",
        "    d0 = unix_time[0]\n",
        "    days = []\n",
        "    for d in unix_time:\n",
        "        day = d\n",
        "        day_from_start = day-d0\n",
        "        decimal_day = day_from_start.total_seconds()/(24 * 3600)\n",
        "        days.append(decimal_day)\n",
        "\n",
        "    return days\n",
        "\n",
        "def simulate(t_list, v_list, v_list_pred, v_list_fine, C_h):\n",
        "    # t_list: list of decimal time stamps in unit of days (e.g. 71.85893518518519 day), same length as v_list\n",
        "    # v_list: list of voltage values from SFMC\n",
        "    # C_h: capacitance of the capacitor being filled up by harvester\n",
        "\n",
        "    #assume capacitor is completely discharged at start\n",
        "    e_minimal_stored = 0\n",
        "    e_minimal_stored_theo = 0\n",
        "\n",
        "    #Initialize evaluation metrics\n",
        "    false_act = 0\n",
        "    max_act = 0\n",
        "    pred_act = 0\n",
        "    succ_act = 0\n",
        "\n",
        "    total_E = 0\n",
        "    total_E_pred = 0\n",
        "\n",
        "    #Calculate maximum energy\n",
        "    #for i in range(len(v_list_fine) - 1):\n",
        "    #    t = (v_list_fine.index[i+1] - v_list_fine.index[i]).total_seconds()\n",
        "    #    total_E, ignore = update_capEnergy(total_E, V_applied=v_list_fine['V1 [10nV]'][i], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "    #print(total_E/Minimal_energy())\n",
        "    #for each voltage data point\n",
        "    for jj in range(len(v_list) - 1): #last data point was at 71.85893518518519 day\n",
        "        t = (v_list.index[jj+1] - v_list.index[jj]).total_seconds()\n",
        "        total_E, ignore = update_capEnergy(total_E, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "        total_E_pred, ignore = update_capEnergy(total_E_pred, V_applied=v_list_pred[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "        if t <= time_frame_seconds:\n",
        "          #Total predicted vs. actual energy stored\n",
        "          #Predict energy stored during scheduled sub-interval\n",
        "          E_Minimal_pred, v_minimal_pred = update_capEnergy(e_minimal_stored, V_applied=v_list_pred[jj], R=internal_R_v0(), C=C_h[0], dt = t) #set dt as length of prediction interval, in seconds\n",
        "          pred_act += math.floor(E_Minimal_pred/Minimal_energy()) #Update number of activations predicted\n",
        "          itn = 0\n",
        "          if math.floor(E_Minimal_pred/Minimal_energy()) > 0:\n",
        "              minimal_intervals = [date for date in group_util(v_list_pred.index[jj], v_list_pred.index[jj] + timedelta(seconds=t), math.floor(E_Minimal_pred/Minimal_energy()))]\n",
        "              #Calculate desired interval\n",
        "              int_len = time_frame_seconds /  math.floor(E_Minimal_pred/Minimal_energy())\n",
        "              for i in range(len(minimal_intervals) - 1):\n",
        "                  #Determine actual energy stored during scheduled sub-interval\n",
        "                  start = v_list_fine.index.searchsorted(minimal_intervals[i])\n",
        "                  end =  v_list_fine.index.searchsorted(minimal_intervals[i+1])\n",
        "                  v = v_list_fine.iloc[start:end]['Voltage (mV)'].mean()\n",
        "\n",
        "                  #interval_length = ((v_list_fine.index[end]) - (v_list_fine.index[start])).total_seconds()\n",
        "                  #if interval_length > int_len:\n",
        "                  #  print('interval_length > int_len')\n",
        "                  #  print('interval_length, int_len:', interval_length, int_len)\n",
        "                  #  print(v_list_fine.index[start], v_list_fine.index[end])\n",
        "                  #else:\n",
        "                  #  print('interval_length <= int_len')\n",
        "                  #  print('interval_length, int_len:', interval_length, int_len)\n",
        "                  #  print(v_list_fine.index[start], v_list_fine.index[end])\n",
        "\n",
        "                  E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = int_len)\n",
        "                  if not math.isnan(v_list_fine.iloc[start:end]['Voltage (mV)'].mean()):\n",
        "                    if E_Minimal < Minimal_energy():\n",
        "                        false_act += 1\n",
        "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
        "                        itn += 1\n",
        "\n",
        "                    elif E_Minimal >= Minimal_energy():\n",
        "                        succ_act += 1\n",
        "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
        "                        itn+= 1\n",
        "\n",
        "                    else:\n",
        "                      print('Error')\n",
        "                      print(e_minimal_stored, v)\n",
        "\n",
        "                  #Unit test\n",
        "                  #else:\n",
        "                  #  print(\"?\")\n",
        "                  #  print(v_list_fine.index[start])\n",
        "                  #  print(v_list_fine.index[end])\n",
        "                  #  print(minimal_intervals[i], minimal_intervals[i+1])\n",
        "\n",
        "              #Unit test\n",
        "              #if itn != math.floor(E_Minimal_pred/Minimal_energy()):\n",
        "              #    print(\"itn not matching\")\n",
        "              #    print(itn, math.floor(E_Minimal_pred/Minimal_energy()))\n",
        "              #    continue\n",
        "\n",
        "          else:\n",
        "              e_minimal_stored, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "              #Added this\n",
        "              #start = v_list_fine.index.searchsorted(v_list.index[jj])\n",
        "              #end =  v_list_fine.index.searchsorted(v_list.index[jj+1])\n",
        "              #for h in range(start, end):\n",
        "              #    v = v_list_fine.iloc[h]['V1 [mV]']\n",
        "              #    interval_length = ((v_list_fine.index[h+1]) - (v_list_fine.index[h])).total_seconds()\n",
        "              #    E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = interval_length)\n",
        "              #    e_minimal_stored = E_Minimal\n",
        "\n",
        "\n",
        "        else:\n",
        "          print(\"It's over 9000!\", v_list.index[jj], v_list.index[jj+1])\n",
        "\n",
        "    print(\"Runtime total_E activations:\", total_E/Minimal_energy())\n",
        "    print(\"Runtime total_E_pred activations:\", total_E_pred/Minimal_energy())\n",
        "    return pred_act, false_act, succ_act, total_E, total_E_pred"
      ],
      "metadata": {
        "id": "nVL47WOhTdYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5qjf1wD9yhQ"
      },
      "outputs": [],
      "source": [
        "#Re-split data for training\n",
        "#X = pd.concat([df.iloc[:, 1:2], df.iloc[:, 15:16], df.iloc[:, 17:19], df.iloc[:, 11:15]], axis = 1)#.dropna()\n",
        "#y = df.iloc[:, 10:11]#.dropna()\n",
        "\n",
        "#Creating training and testing sets (without rolling)\n",
        "#X_train, X_test = train_test_split(pd.concat([df.iloc[:, 2:5], df.iloc[:, 13:14], df.iloc[:, 15:19]], axis = 1), test_size=0.3, shuffle=False)\n",
        "#y_train, y_test = train_test_split(df.iloc[:, 14:15], test_size=0.3, shuffle=False)\n",
        "\n",
        "#Creating training and testing sets (with roll)\n",
        "#X_train, X_test = train_test_split(pd.concat([df.iloc[:, 1:2], df.iloc[:, 17:18], df.iloc[:, 19:20], df.iloc[:, 3:4], df.iloc[:, 9:10], df.iloc[:, 13:17]], axis = 1), test_size=0.3, shuffle=False)\n",
        "#y_train, y_test = train_test_split(pd.concat([df.iloc[:, 12:13]], axis = 1), test_size=0.3, shuffle=False)\n",
        "\n",
        "#Creating training and testing sets (with roll)\n",
        "#X_train, X_test = train_test_split(pd.concat([df.iloc[:, 17:21], df.iloc[:, 1:2], df.iloc[:, 21:22], df.iloc[:, 23:24], df.iloc[:, 2:3], df.iloc[:, 4:5], df.iloc[:, 10:11], df.iloc[:, 25:26], df.iloc[:, 15:16], df.iloc[:, 13:15], df.iloc[:, 24:25]], axis = 1), test_size=0.3, shuffle=False)\n",
        "#y_train, y_test = train_test_split(pd.concat([df.iloc[:, 16:17]], axis = 1), test_size=0.3, shuffle=False)\n",
        "\n",
        "#Creating training and testing sets (with roll, hour)\n",
        "#X_train, X_test = train_test_split(pd.concat([df.iloc[:, 15:19], df.iloc[:, 1:2], df.iloc[:, 19:20], df.iloc[:, 21:22], df.iloc[:, 2:3], df.iloc[:, 4:5], df.iloc[:, 10:11], df.iloc[:, 13:14], df.iloc[:, 22:23]], axis = 1), test_size=0.3, shuffle=False)\n",
        "#y_train, y_test = train_test_split(pd.concat([df.iloc[:, 14:15]], axis = 1), test_size=0.3, shuffle=False)\n",
        "\n",
        "#Creating training and testing sets (with roll, no hour)\n",
        "#X_train, X_test = train_test_split(pd.concat([df.iloc[:, 15:19], df.iloc[:, 2:3], df.iloc[:, 20:21], df.iloc[:, 23:24]], axis = 1), test_size=0.3, shuffle=False)\n",
        "#y_train, y_test = train_test_split(pd.concat([df.iloc[:, 14:15]], axis = 1), test_size=0.3, shuffle=False)\n",
        "\n",
        "X_train, X_test = train_test_split(pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1), test_size=0.3, shuffle=False)\n",
        "y_train, y_test = train_test_split(pd.concat([df['Power (uW)'], df['Voltage (mV)'], df['Current (uA)']], axis = 1), test_size=0.3, shuffle=False)\n",
        "#y_train, y_test = train_test_split(pd.concat([df[\"power\"], df['V1 [mV]'], df['I1L [μA]']], axis = 1), test_size=0.3, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fEIWPla0Ry3"
      },
      "outputs": [],
      "source": [
        "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#All Data (For Type 1 and 2 Models)\n",
        "X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "\n",
        "#Electricity Data Omitted (For Type 1A and 2A Models)\n",
        "#X = pd.concat([df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "\n",
        "#Environmental Data Omitted (For Type 1B and 2B Models)\n",
        "#X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"]], axis = 1)\n",
        "\n",
        "y = pd.concat([df['Power (uW)'], df['Voltage (mV)'], df['Current (uA)']], axis = 1)"
      ],
      "metadata": {
        "id": "qhzlX2syGHOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TSRV on Type 1 Models\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "\n",
        "power_mape = []\n",
        "voltage_mape = []\n",
        "current_mape = []\n",
        "\n",
        "E_actual_list = []\n",
        "E_pred_list = []\n",
        "\n",
        "max_act_list = []\n",
        "pred_act_list = []\n",
        "succ_act_list = []\n",
        "\n",
        "pred_act_naive_list = []\n",
        "false_act_naive_list = []\n",
        "succ_act_naive_list = []\n",
        "\n",
        "#Set parameters\n",
        "batchsize = 8\n",
        "time_frame = '60min'\n",
        "time_frame_seconds = 3600\n",
        "n = 0\n",
        "splits = TimeSeriesSplit(n_splits=4)\n",
        "for valid_index, test_index in splits.split(X):\n",
        "  n += 1\n",
        "  if n >= 1:\n",
        "    #Split train and test sets\n",
        "    X_train = X.iloc[valid_index]\n",
        "    X_test = X.iloc[test_index]\n",
        "    y_train = y.iloc[valid_index]\n",
        "    y_test = y.iloc[test_index]\n",
        "\n",
        "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
        "\n",
        "    #Set dataset bounds\n",
        "    train_bound_lower = y_train.index[0]\n",
        "    train_bound_upper = y_train.index[-1]\n",
        "    valid_bound_lower = y_valid.index[0]\n",
        "    valid_bound_upper = y_valid.index[-1]\n",
        "    test_bound_lower = y_test.index[0]\n",
        "    test_bound_upper = y_test.index[-1]\n",
        "\n",
        "    E_actual = 0\n",
        "    for i in range(len(y_test) - 1):\n",
        "      t = (y_test.index[i+1] - y_test.index[i]).total_seconds()\n",
        "      if t < 180:\n",
        "        E_actual += y_test['Power (uW)'][i] * t\n",
        "\n",
        "    v_test = y_test.loc[(((y_test.index >= test_bound_lower)) & (y_test.index <= test_bound_upper))]['Voltage (mV)']\n",
        "    #v_test = v_test.drop(v_test[(v_test.index > '2021-11-11') & (v_test.index < '2021-11-22 01:00:00')].index)\n",
        "    v_test = pd.DataFrame(v_test)/1E5\n",
        "    v_avg_true = v_test['Voltage (mV)'].resample(time_frame).mean().dropna()\n",
        "    C0 = [0.007000000000000006, 0.007000000000000006, 0.007000000000000006]\n",
        "\n",
        "\n",
        "    #Resample data\n",
        "    X_train = X_train.resample(time_frame).mean().dropna()\n",
        "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
        "    X_test = X_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "    y_train = y_train.resample(time_frame).mean().dropna()\n",
        "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
        "    y_test = y_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "    print(v_test)\n",
        "\n",
        "    #Reshape data\n",
        "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
        "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "\n",
        "    print(\"Train set size:\", len(y_train))\n",
        "    print(\"Test set size:\", len(y_test))\n",
        "\n",
        "    def quantile_loss(y_true, y_pred, quantile = 0.05):\n",
        "      error = y_true - y_pred\n",
        "      return K.mean(K.maximum(quantile * error, (quantile - 1) * error), axis=-1)\n",
        "\n",
        "    # design network\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(3))\n",
        "    model.compile(loss=quantile_loss, metrics=['mape'], optimizer='adam')\n",
        "\n",
        "    # fit network\n",
        "    model.fit(X_train, y_train, epochs=25, validation_data=(X_valid, y_valid), batch_size=batchsize, verbose=2, shuffle=False)\n",
        "    train_pred = model.predict(X_train, batch_size=batchsize)\n",
        "    test_pred = model.predict(X_test, batch_size=batchsize)\n",
        "\n",
        "    #Save model if wanted\n",
        "    model_name = 'type1B_' + time_frame + '_quant50'\n",
        "    model.save(model_name, overwrite=True, save_format=None)\n",
        "\n",
        "    train_pred = model.predict(X_train, batch_size=batchsize)\n",
        "    test_pred = model.predict(X_test, batch_size=batchsize)\n",
        "\n",
        "    #Prepare data for runtime simulation\n",
        "    y_test['power pred'] = test_pred[:, 0]\n",
        "    y_test['Voltage (mV) pred'] = test_pred[:, 1]\n",
        "    y_test['Current (uA) pred'] = test_pred[:, 2]\n",
        "\n",
        "    v_avg_pred = y_test['Voltage (mV) pred']/1E5\n",
        "\n",
        "    days  = getMFC_data(y_test, test_pred)\n",
        "\n",
        "    #Remove first and last entries of averaged data to prevent overestimation of available energy\n",
        "    #v_avg_true = v_avg_true[1:][:-1]\n",
        "    #v_avg_pred = v_avg_pred[1:][:-1]\n",
        "\n",
        "    #Run oracle model\n",
        "    max_act = oracle_simulate(v_test, C0, time_frame_seconds)\n",
        "\n",
        "    #Call simulate function\n",
        "    pred_act, false_act, succ_act, total_E, total_E_pred = simulate(days, v_avg_true, v_avg_pred, v_test, C0)\n",
        "\n",
        "    #Run naive model\n",
        "    v_valid = y.loc[(((y.index >= valid_bound_lower)) & (y.index < valid_bound_upper))]['Voltage (mV)']/1E5\n",
        "    pred_act_naive, false_act_naive, succ_act_naive, total_E = naive_simulate(days, v_avg_true, v_valid, v_test, C0)\n",
        "    print(\"Dataset, train set, and test set size:\", len(y_train) + len(y_valid) + len(y_test), len(y_train), len(y_test))\n",
        "    print('Timeframe:', time_frame)\n",
        "\n",
        "    print('Minimal Application')\n",
        "    if succ_act_naive > 0:\n",
        "      print(\"Naive vs. DL succesful activations:\", succ_act/succ_act_naive)\n",
        "    else:\n",
        "      print(\"No succesful naive activation\")\n",
        "    #print('Predicted vs. Actual percent difference: %.3f%%' % ((total_E * 100 / total_E_pred) - 100))\n",
        "    print('Maximum possible activations:', max_act)\n",
        "    print('Predicted activations:', pred_act)\n",
        "    print('Successful activations: %d, %.3f%%' % (succ_act, succ_act * 100/pred_act))\n",
        "    print('Failed activations: %d, %.3f%%' % (false_act, false_act * 100/pred_act))\n",
        "    print('Missed activations: %d, %.3f%%' % (max_act - succ_act, (max_act - succ_act) * 100/max_act))\n",
        "\n",
        "    print('Successful activations compared to max: %d, %.3f%%' % (succ_act, succ_act * 100/max_act))\n",
        "\n",
        "    #Naive model\n",
        "    print('Naive predicted activations (usual actual energy average):', pred_act_naive)\n",
        "    print('Naive successful activations (usual actual energy average): %d, %.3f%%' % (succ_act_naive, succ_act_naive * 100/pred_act_naive))\n",
        "    print('Naive failed activations (usual actual energy average): %d, %.3f%%' % (false_act_naive, false_act_naive * 100/pred_act_naive))\n",
        "    print('Naive missed activations (usual actual energy average): %d, %.3f%%' % (max_act - succ_act_naive, (max_act - succ_act_naive) * 100/max_act))\n",
        "\n",
        "    print('Naive successful activations (compared to max): %d, %.3f%%' % (succ_act_naive, succ_act_naive * 100/max_act))\n",
        "\n",
        "\n",
        "    print('Voltage overestimation rate: %.3f%%' % ((y_test['Voltage (mV)'].values <= y_test['Voltage (mV) pred']).mean() * 100))\n",
        "    print(\"Test MAPE power: %3f\" %  MAPE(y_test['Power (uW)'].values.ravel(), y_test['power pred']))\n",
        "    print(\"Test MAPE voltage: %3f\" % MAPE(y_test['Voltage (mV)'], y_test['Voltage (mV) pred']))\n",
        "    print(\"Test MAPE current: %3f\" % MAPE(y_test['Current (uA)'], y_test['Current (uA) pred']))\n",
        "\n",
        "    E_pred = 0\n",
        "    for i in range(len(y_test) - 1):\n",
        "      t = (y_test.index[i+1] - y_test.index[i]).total_seconds()\n",
        "      if t <= time_frame_seconds + 50:\n",
        "        E_pred += y_test['power pred'][i] * t\n",
        "\n",
        "    print('Predicted vs. Actual Total Energy Percent Difference: %.3f%%' % ((E_pred - E_actual) * 100 / E_actual))\n",
        "    print(\"Test MAPE power:\\n\", MAPE(y_test['Power (uW)'].values.ravel(), y_test['power pred'].values.ravel()))\n",
        "\n",
        "    V_actual = y_test['Voltage (mV)'].mean()\n",
        "    V_pred = y_test['Voltage (mV) pred'].mean()\n",
        "    print('Predicted vs. Actual Total Voltage Percent Difference: %.3f%%' % ((V_pred - V_actual) * 100 / V_actual))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjBTxqRRELJp",
        "outputId": "91b9b403-6e55-4302-8c72-b73a6e4975bb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2021-08-12 23:17:47-07:00      0.023669\n",
            "2021-08-12 23:18:01-07:00      0.023068\n",
            "2021-08-12 23:18:14-07:00      0.022208\n",
            "2021-08-12 23:18:28-07:00      0.021762\n",
            "2021-08-12 23:18:42-07:00      0.022792\n",
            "...                                 ...\n",
            "2021-10-05 04:46:39-07:00      0.029982\n",
            "2021-10-05 04:46:53-07:00      0.030001\n",
            "2021-10-05 04:47:07-07:00      0.029929\n",
            "2021-10-05 04:47:20-07:00      0.030001\n",
            "2021-10-05 04:47:34-07:00      0.030092\n",
            "\n",
            "[96965 rows x 1 columns]\n",
            "Train set size: 738\n",
            "Test set size: 369\n",
            "Epoch 1/25\n",
            "93/93 - 4s - loss: 89.8368 - mape: 48.4394 - val_loss: 29.6216 - val_mape: 21.2295 - 4s/epoch - 42ms/step\n",
            "Epoch 2/25\n",
            "93/93 - 1s - loss: 63.8743 - mape: 36.5019 - val_loss: 28.4095 - val_mape: 27.9058 - 1s/epoch - 11ms/step\n",
            "Epoch 3/25\n",
            "93/93 - 1s - loss: 45.5409 - mape: 33.0039 - val_loss: 48.7512 - val_mape: 17.3142 - 1s/epoch - 12ms/step\n",
            "Epoch 4/25\n",
            "93/93 - 1s - loss: 48.7225 - mape: 29.0174 - val_loss: 13.8155 - val_mape: 8.0974 - 857ms/epoch - 9ms/step\n",
            "Epoch 5/25\n",
            "93/93 - 1s - loss: 40.8385 - mape: 30.6224 - val_loss: 7.2571 - val_mape: 11.6893 - 892ms/epoch - 10ms/step\n",
            "Epoch 6/25\n",
            "93/93 - 1s - loss: 29.0642 - mape: 31.9927 - val_loss: 12.6634 - val_mape: 25.4074 - 1s/epoch - 11ms/step\n",
            "Epoch 7/25\n",
            "93/93 - 1s - loss: 53.1513 - mape: 31.2890 - val_loss: 16.1215 - val_mape: 23.3076 - 838ms/epoch - 9ms/step\n",
            "Epoch 8/25\n",
            "93/93 - 1s - loss: 36.8613 - mape: 22.8892 - val_loss: 8.9404 - val_mape: 19.2134 - 829ms/epoch - 9ms/step\n",
            "Epoch 9/25\n",
            "93/93 - 1s - loss: 32.8034 - mape: 27.5876 - val_loss: 16.4330 - val_mape: 27.8625 - 641ms/epoch - 7ms/step\n",
            "Epoch 10/25\n",
            "93/93 - 1s - loss: 27.8023 - mape: 31.5676 - val_loss: 25.8340 - val_mape: 21.0947 - 571ms/epoch - 6ms/step\n",
            "Epoch 11/25\n",
            "93/93 - 1s - loss: 37.0611 - mape: 59.3729 - val_loss: 18.0703 - val_mape: 23.9508 - 533ms/epoch - 6ms/step\n",
            "Epoch 12/25\n",
            "93/93 - 1s - loss: 30.9058 - mape: 18.5861 - val_loss: 31.3318 - val_mape: 9.0461 - 545ms/epoch - 6ms/step\n",
            "Epoch 13/25\n",
            "93/93 - 0s - loss: 30.2886 - mape: 24.7343 - val_loss: 8.2622 - val_mape: 7.4513 - 479ms/epoch - 5ms/step\n",
            "Epoch 14/25\n",
            "93/93 - 1s - loss: 64.2853 - mape: 40.2793 - val_loss: 24.0590 - val_mape: 23.7107 - 548ms/epoch - 6ms/step\n",
            "Epoch 15/25\n",
            "93/93 - 0s - loss: 35.6273 - mape: 23.3175 - val_loss: 8.4339 - val_mape: 11.9545 - 488ms/epoch - 5ms/step\n",
            "Epoch 16/25\n",
            "93/93 - 1s - loss: 37.7796 - mape: 20.6725 - val_loss: 10.6230 - val_mape: 11.0652 - 557ms/epoch - 6ms/step\n",
            "Epoch 17/25\n",
            "93/93 - 1s - loss: 36.7831 - mape: 23.5010 - val_loss: 12.6666 - val_mape: 31.7065 - 553ms/epoch - 6ms/step\n",
            "Epoch 18/25\n",
            "93/93 - 1s - loss: 29.0382 - mape: 18.9961 - val_loss: 6.9965 - val_mape: 12.6834 - 562ms/epoch - 6ms/step\n",
            "Epoch 19/25\n",
            "93/93 - 0s - loss: 22.5176 - mape: 15.0569 - val_loss: 7.9617 - val_mape: 9.2397 - 486ms/epoch - 5ms/step\n",
            "Epoch 20/25\n",
            "93/93 - 1s - loss: 23.5853 - mape: 22.1467 - val_loss: 9.6407 - val_mape: 8.1029 - 550ms/epoch - 6ms/step\n",
            "Epoch 21/25\n",
            "93/93 - 1s - loss: 27.6463 - mape: 18.0981 - val_loss: 8.7036 - val_mape: 12.6481 - 540ms/epoch - 6ms/step\n",
            "Epoch 22/25\n",
            "93/93 - 0s - loss: 32.6094 - mape: 18.7116 - val_loss: 23.2372 - val_mape: 6.9893 - 482ms/epoch - 5ms/step\n",
            "Epoch 23/25\n",
            "93/93 - 1s - loss: 33.4554 - mape: 17.8287 - val_loss: 10.7778 - val_mape: 12.1410 - 555ms/epoch - 6ms/step\n",
            "Epoch 24/25\n",
            "93/93 - 1s - loss: 27.3743 - mape: 21.1659 - val_loss: 13.0489 - val_mape: 4.7359 - 539ms/epoch - 6ms/step\n",
            "Epoch 25/25\n",
            "93/93 - 0s - loss: 28.0305 - mape: 31.1747 - val_loss: 15.4772 - val_mape: 6.0098 - 497ms/epoch - 5ms/step\n",
            "93/93 [==============================] - 0s 2ms/step\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "93/93 [==============================] - 0s 2ms/step\n",
            "47/47 [==============================] - 0s 2ms/step\n",
            "Discontinuity\n",
            "2021-09-24 04:17:21-07:00 2021-08-17 06:37:13-07:00\n",
            "0.04926536 0.0012214300000000001\n",
            "Oracle activations: 34077\n",
            "It's over 9000! 2021-08-17 06:00:00-07:00 2021-09-24 04:00:00-07:00\n",
            "Runtime total_E activations: 94631.23278454383\n",
            "Runtime total_E_pred activations: 94385.78602865722\n",
            "It's over 9000! 2021-08-17 06:00:00-07:00 2021-09-24 04:00:00-07:00\n",
            "Naive total_E activations: 33248.8927455674\n",
            "Naive total_E_pred activations: 39666.25937503173\n",
            "Dataset, train set, and test set size: 1476 738 369\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 1.9025134766512957\n",
            "Maximum possible activations: 34077\n",
            "Predicted activations: 33897\n",
            "Successful activations: 29293, 86.418%\n",
            "Failed activations: 3246, 9.576%\n",
            "Missed activations: 4784, 14.039%\n",
            "Successful activations compared to max: 29293, 85.961%\n",
            "Naive predicted activations (usual actual energy average): 46694\n",
            "Naive successful activations (usual actual energy average): 15397, 32.974%\n",
            "Naive failed activations (usual actual energy average): 29953, 64.147%\n",
            "Naive missed activations (usual actual energy average): 18680, 54.817%\n",
            "Naive successful activations (compared to max): 15397, 45.183%\n",
            "Voltage overestimation rate: 30.623%\n",
            "Test MAPE power: 0.114024\n",
            "Test MAPE voltage: 0.019201\n",
            "Test MAPE current: 0.033082\n",
            "Predicted vs. Actual Total Energy Percent Difference: -11.240%\n",
            "Test MAPE power:\n",
            " 0.11402355952307452\n",
            "Predicted vs. Actual Total Voltage Percent Difference: -1.019%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2021-10-31 12:52:51-07:00      0.021357\n",
            "2021-10-31 12:53:04-07:00      0.022567\n",
            "2021-10-31 12:53:18-07:00      0.024236\n",
            "2021-10-31 12:53:32-07:00      0.024086\n",
            "2021-10-31 12:53:45-07:00      0.022803\n",
            "...                                 ...\n",
            "2021-11-26 18:36:47-08:00      0.010503\n",
            "2021-11-26 18:37:01-08:00      0.010482\n",
            "2021-11-26 18:37:15-08:00      0.010859\n",
            "2021-11-26 18:37:28-08:00      0.009953\n",
            "2021-11-26 18:37:42-08:00      0.009322\n",
            "\n",
            "[96965 rows x 1 columns]\n",
            "Train set size: 1474\n",
            "Test set size: 367\n",
            "Epoch 1/25\n",
            "185/185 - 4s - loss: 59.5810 - mape: 36.0053 - val_loss: 18.2813 - val_mape: 28.4803 - 4s/epoch - 19ms/step\n",
            "Epoch 2/25\n",
            "185/185 - 1s - loss: 30.0119 - mape: 24.4312 - val_loss: 6.3536 - val_mape: 7.3785 - 1s/epoch - 8ms/step\n",
            "Epoch 3/25\n",
            "185/185 - 2s - loss: 66.1183 - mape: 34.2256 - val_loss: 8.3677 - val_mape: 9.8127 - 2s/epoch - 9ms/step\n",
            "Epoch 4/25\n",
            "185/185 - 2s - loss: 30.0805 - mape: 26.2219 - val_loss: 9.6606 - val_mape: 16.3123 - 2s/epoch - 9ms/step\n",
            "Epoch 5/25\n",
            "185/185 - 2s - loss: 22.2633 - mape: 21.5632 - val_loss: 15.0688 - val_mape: 4.4286 - 2s/epoch - 9ms/step\n",
            "Epoch 6/25\n",
            "185/185 - 2s - loss: 46.4375 - mape: 30.8704 - val_loss: 8.7735 - val_mape: 16.3581 - 2s/epoch - 8ms/step\n",
            "Epoch 7/25\n",
            "185/185 - 2s - loss: 33.0844 - mape: 25.0897 - val_loss: 7.1265 - val_mape: 8.2077 - 2s/epoch - 8ms/step\n",
            "Epoch 8/25\n",
            "185/185 - 2s - loss: 22.4658 - mape: 17.0863 - val_loss: 6.2201 - val_mape: 6.9141 - 2s/epoch - 8ms/step\n",
            "Epoch 9/25\n",
            "185/185 - 2s - loss: 24.9253 - mape: 22.3592 - val_loss: 4.6598 - val_mape: 9.0444 - 2s/epoch - 8ms/step\n",
            "Epoch 10/25\n",
            "185/185 - 1s - loss: 23.3239 - mape: 12.9954 - val_loss: 19.2962 - val_mape: 8.7333 - 1s/epoch - 6ms/step\n",
            "Epoch 11/25\n",
            "185/185 - 1s - loss: 22.2165 - mape: 15.5028 - val_loss: 12.2845 - val_mape: 8.4659 - 925ms/epoch - 5ms/step\n",
            "Epoch 12/25\n",
            "185/185 - 1s - loss: 17.2049 - mape: 15.0516 - val_loss: 6.3047 - val_mape: 2.8950 - 909ms/epoch - 5ms/step\n",
            "Epoch 13/25\n",
            "185/185 - 1s - loss: 19.7145 - mape: 16.0938 - val_loss: 4.7650 - val_mape: 4.7711 - 927ms/epoch - 5ms/step\n",
            "Epoch 14/25\n",
            "185/185 - 1s - loss: 21.6722 - mape: 19.3320 - val_loss: 4.5856 - val_mape: 7.2003 - 878ms/epoch - 5ms/step\n",
            "Epoch 15/25\n",
            "185/185 - 1s - loss: 24.4325 - mape: 26.7292 - val_loss: 5.7729 - val_mape: 9.9713 - 902ms/epoch - 5ms/step\n",
            "Epoch 16/25\n",
            "185/185 - 1s - loss: 22.6868 - mape: 16.2263 - val_loss: 3.3862 - val_mape: 4.9065 - 901ms/epoch - 5ms/step\n",
            "Epoch 17/25\n",
            "185/185 - 1s - loss: 19.2991 - mape: 16.2042 - val_loss: 4.1849 - val_mape: 4.8794 - 911ms/epoch - 5ms/step\n",
            "Epoch 18/25\n",
            "185/185 - 1s - loss: 12.8652 - mape: 11.0086 - val_loss: 8.1855 - val_mape: 7.8381 - 919ms/epoch - 5ms/step\n",
            "Epoch 19/25\n",
            "185/185 - 1s - loss: 19.8132 - mape: 15.4602 - val_loss: 3.6359 - val_mape: 4.3959 - 952ms/epoch - 5ms/step\n",
            "Epoch 20/25\n",
            "185/185 - 1s - loss: 27.3949 - mape: 17.0928 - val_loss: 6.1990 - val_mape: 10.8166 - 948ms/epoch - 5ms/step\n",
            "Epoch 21/25\n",
            "185/185 - 1s - loss: 17.6585 - mape: 10.8638 - val_loss: 4.2634 - val_mape: 9.3675 - 1s/epoch - 6ms/step\n",
            "Epoch 22/25\n",
            "185/185 - 1s - loss: 19.9633 - mape: 17.2744 - val_loss: 3.7502 - val_mape: 7.9692 - 1s/epoch - 7ms/step\n",
            "Epoch 23/25\n",
            "185/185 - 1s - loss: 20.4543 - mape: 13.9279 - val_loss: 3.6377 - val_mape: 3.9516 - 1s/epoch - 7ms/step\n",
            "Epoch 24/25\n",
            "185/185 - 2s - loss: 21.6909 - mape: 20.3519 - val_loss: 4.6553 - val_mape: 9.0704 - 2s/epoch - 9ms/step\n",
            "Epoch 25/25\n",
            "185/185 - 1s - loss: 11.7670 - mape: 11.9457 - val_loss: 4.9630 - val_mape: 8.5343 - 1s/epoch - 8ms/step\n",
            "185/185 [==============================] - 0s 2ms/step\n",
            "46/46 [==============================] - 0s 2ms/step\n",
            "185/185 [==============================] - 0s 2ms/step\n",
            "46/46 [==============================] - 0s 2ms/step\n",
            "Discontinuity\n",
            "2021-11-22 01:00:02-08:00 2021-11-10 23:59:53-08:00\n",
            "0.0012214300000000001 0.02535399\n",
            "Oracle activations: 10996\n",
            "It's over 9000! 2021-11-10 23:00:00-08:00 2021-11-22 01:00:00-08:00\n",
            "Runtime total_E activations: 24008.306551420952\n",
            "Runtime total_E_pred activations: 21494.045956221355\n",
            "It's over 9000! 2021-11-10 23:00:00-08:00 2021-11-22 01:00:00-08:00\n",
            "Naive total_E activations: 10393.90931985667\n",
            "Naive total_E_pred activations: 14159.066003080077\n",
            "Dataset, train set, and test set size: 2211 1474 367\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 1.795704467353952\n",
            "Maximum possible activations: 10996\n",
            "Predicted activations: 10482\n",
            "Successful activations: 10451, 99.704%\n",
            "Failed activations: 0, 0.000%\n",
            "Missed activations: 545, 4.956%\n",
            "Successful activations compared to max: 10451, 95.044%\n",
            "Naive predicted activations (usual actual energy average): 15295\n",
            "Naive successful activations (usual actual energy average): 5820, 38.052%\n",
            "Naive failed activations (usual actual energy average): 9441, 61.726%\n",
            "Naive missed activations (usual actual energy average): 5176, 47.072%\n",
            "Naive successful activations (compared to max): 5820, 52.928%\n",
            "Voltage overestimation rate: 0.000%\n",
            "Test MAPE power: 0.207955\n",
            "Test MAPE voltage: 0.103792\n",
            "Test MAPE current: 0.043294\n",
            "Predicted vs. Actual Total Energy Percent Difference: -17.725%\n",
            "Test MAPE power:\n",
            " 0.2079545785510549\n",
            "Predicted vs. Actual Total Voltage Percent Difference: -8.793%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2021-12-12 01:50:03-08:00      0.041934\n",
            "2021-12-12 01:50:17-08:00      0.043989\n",
            "2021-12-12 01:50:31-08:00      0.042013\n",
            "2021-12-12 01:50:44-08:00      0.041475\n",
            "2021-12-12 01:50:58-08:00      0.043989\n",
            "...                                 ...\n",
            "2021-12-27 08:43:53-08:00      0.014330\n",
            "2021-12-27 08:44:07-08:00      0.011629\n",
            "2021-12-27 08:44:21-08:00      0.012309\n",
            "2021-12-27 08:44:34-08:00      0.011223\n",
            "2021-12-27 08:44:48-08:00      0.011524\n",
            "\n",
            "[96965 rows x 1 columns]\n",
            "Train set size: 2209\n",
            "Test set size: 368\n",
            "Epoch 1/25\n",
            "277/277 - 4s - loss: 47.2154 - mape: 33.4888 - val_loss: 19.1024 - val_mape: 72.9789 - 4s/epoch - 15ms/step\n",
            "Epoch 2/25\n",
            "277/277 - 1s - loss: 33.3838 - mape: 27.2376 - val_loss: 6.5409 - val_mape: 14.8415 - 1s/epoch - 5ms/step\n",
            "Epoch 3/25\n",
            "277/277 - 1s - loss: 33.1901 - mape: 24.2711 - val_loss: 16.2774 - val_mape: 35.1268 - 1s/epoch - 4ms/step\n",
            "Epoch 4/25\n",
            "277/277 - 1s - loss: 20.0547 - mape: 18.5310 - val_loss: 14.6689 - val_mape: 25.1072 - 1s/epoch - 4ms/step\n",
            "Epoch 5/25\n",
            "277/277 - 1s - loss: 17.8894 - mape: 16.0086 - val_loss: 4.8812 - val_mape: 12.4128 - 1s/epoch - 5ms/step\n",
            "Epoch 6/25\n",
            "277/277 - 1s - loss: 18.2511 - mape: 14.9819 - val_loss: 5.7571 - val_mape: 12.6341 - 1s/epoch - 4ms/step\n",
            "Epoch 7/25\n",
            "277/277 - 1s - loss: 16.9408 - mape: 15.6363 - val_loss: 4.4050 - val_mape: 11.5328 - 1s/epoch - 4ms/step\n",
            "Epoch 8/25\n",
            "277/277 - 1s - loss: 14.4395 - mape: 16.2800 - val_loss: 8.8177 - val_mape: 17.6927 - 1s/epoch - 5ms/step\n",
            "Epoch 9/25\n",
            "277/277 - 2s - loss: 12.9815 - mape: 14.4780 - val_loss: 12.2939 - val_mape: 8.3061 - 2s/epoch - 7ms/step\n",
            "Epoch 10/25\n",
            "277/277 - 2s - loss: 21.0097 - mape: 15.0796 - val_loss: 12.6227 - val_mape: 20.2555 - 2s/epoch - 7ms/step\n",
            "Epoch 11/25\n",
            "277/277 - 2s - loss: 13.5401 - mape: 11.4729 - val_loss: 6.7044 - val_mape: 12.7367 - 2s/epoch - 8ms/step\n",
            "Epoch 12/25\n",
            "277/277 - 2s - loss: 18.5073 - mape: 15.7303 - val_loss: 5.0279 - val_mape: 9.8393 - 2s/epoch - 8ms/step\n",
            "Epoch 13/25\n",
            "277/277 - 3s - loss: 13.6866 - mape: 12.0535 - val_loss: 3.4296 - val_mape: 9.4767 - 3s/epoch - 10ms/step\n",
            "Epoch 14/25\n",
            "277/277 - 2s - loss: 11.7203 - mape: 10.2904 - val_loss: 2.0793 - val_mape: 3.0925 - 2s/epoch - 8ms/step\n",
            "Epoch 15/25\n",
            "277/277 - 2s - loss: 11.4833 - mape: 12.5609 - val_loss: 5.3750 - val_mape: 10.4199 - 2s/epoch - 7ms/step\n",
            "Epoch 16/25\n",
            "277/277 - 1s - loss: 13.6896 - mape: 14.3028 - val_loss: 3.9918 - val_mape: 8.8735 - 1s/epoch - 5ms/step\n",
            "Epoch 17/25\n",
            "277/277 - 1s - loss: 20.2545 - mape: 21.3209 - val_loss: 9.2598 - val_mape: 28.9479 - 1s/epoch - 5ms/step\n",
            "Epoch 18/25\n",
            "277/277 - 1s - loss: 17.0787 - mape: 12.5913 - val_loss: 4.6222 - val_mape: 11.9838 - 1s/epoch - 5ms/step\n",
            "Epoch 19/25\n",
            "277/277 - 1s - loss: 14.5868 - mape: 13.1574 - val_loss: 5.2443 - val_mape: 9.5073 - 1s/epoch - 5ms/step\n",
            "Epoch 20/25\n",
            "277/277 - 1s - loss: 19.2810 - mape: 15.0383 - val_loss: 6.0221 - val_mape: 15.3448 - 1s/epoch - 4ms/step\n",
            "Epoch 21/25\n",
            "277/277 - 1s - loss: 14.4072 - mape: 12.2935 - val_loss: 3.0930 - val_mape: 5.6322 - 1s/epoch - 5ms/step\n",
            "Epoch 22/25\n",
            "277/277 - 1s - loss: 13.8229 - mape: 13.1915 - val_loss: 5.4921 - val_mape: 10.3784 - 1s/epoch - 5ms/step\n",
            "Epoch 23/25\n",
            "277/277 - 2s - loss: 11.6301 - mape: 12.7723 - val_loss: 7.1586 - val_mape: 21.0982 - 2s/epoch - 8ms/step\n",
            "Epoch 24/25\n",
            "277/277 - 2s - loss: 16.3983 - mape: 14.2739 - val_loss: 4.7181 - val_mape: 11.0548 - 2s/epoch - 8ms/step\n",
            "Epoch 25/25\n",
            "277/277 - 2s - loss: 13.6502 - mape: 12.5222 - val_loss: 5.6486 - val_mape: 18.2900 - 2s/epoch - 7ms/step\n",
            "277/277 [==============================] - 1s 2ms/step\n",
            "46/46 [==============================] - 0s 2ms/step\n",
            "277/277 [==============================] - 1s 2ms/step\n",
            "46/46 [==============================] - 0s 2ms/step\n",
            "Oracle activations: 13933\n",
            "Runtime total_E activations: 13338.78727859937\n",
            "Runtime total_E_pred activations: 11740.419697054447\n",
            "Naive total_E activations: 13338.78727859937\n",
            "Naive total_E_pred activations: 5181.683956079009\n",
            "Dataset, train set, and test set size: 2945 2209 368\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 1.078983572064417\n",
            "Maximum possible activations: 13933\n",
            "Predicted activations: 13448\n",
            "Successful activations: 13333, 99.145%\n",
            "Failed activations: 2, 0.015%\n",
            "Missed activations: 600, 4.306%\n",
            "Successful activations compared to max: 13333, 95.694%\n",
            "Naive predicted activations (usual actual energy average): 13786\n",
            "Naive successful activations (usual actual energy average): 12357, 89.634%\n",
            "Naive failed activations (usual actual energy average): 1418, 10.286%\n",
            "Naive missed activations (usual actual energy average): 1576, 11.311%\n",
            "Naive successful activations (compared to max): 12357, 88.689%\n",
            "Voltage overestimation rate: 0.000%\n",
            "Test MAPE power: 0.357042\n",
            "Test MAPE voltage: 0.076991\n",
            "Test MAPE current: 0.027803\n",
            "Predicted vs. Actual Total Energy Percent Difference: -30.478%\n",
            "Test MAPE power:\n",
            " 0.357042442954301\n",
            "Predicted vs. Actual Total Voltage Percent Difference: -6.997%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2022-01-11 16:19:51-08:00      0.009767\n",
            "2022-01-11 16:20:04-08:00      0.007492\n",
            "2022-01-11 16:20:18-08:00      0.007287\n",
            "2022-01-11 16:20:32-08:00      0.009400\n",
            "2022-01-11 16:20:45-08:00      0.011821\n",
            "...                                 ...\n",
            "2022-01-26 23:58:52-08:00      0.024386\n",
            "2022-01-26 23:59:06-08:00      0.008550\n",
            "2022-01-26 23:59:19-08:00      0.008552\n",
            "2022-01-26 23:59:33-08:00      0.008208\n",
            "2022-01-26 23:59:47-08:00      0.007138\n",
            "\n",
            "[96965 rows x 1 columns]\n",
            "Train set size: 2943\n",
            "Test set size: 368\n",
            "Epoch 1/25\n",
            "368/368 - 3s - loss: 41.3530 - mape: 32.4053 - val_loss: 13.4728 - val_mape: 9.3175 - 3s/epoch - 9ms/step\n",
            "Epoch 2/25\n",
            "368/368 - 2s - loss: 25.8037 - mape: 23.6393 - val_loss: 9.7504 - val_mape: 26.9422 - 2s/epoch - 4ms/step\n",
            "Epoch 3/25\n",
            "368/368 - 2s - loss: 20.4773 - mape: 20.3618 - val_loss: 7.6620 - val_mape: 27.0488 - 2s/epoch - 5ms/step\n",
            "Epoch 4/25\n",
            "368/368 - 2s - loss: 19.4126 - mape: 20.1704 - val_loss: 4.0448 - val_mape: 8.1446 - 2s/epoch - 7ms/step\n",
            "Epoch 5/25\n",
            "368/368 - 3s - loss: 20.6584 - mape: 17.6495 - val_loss: 2.7860 - val_mape: 9.0222 - 3s/epoch - 8ms/step\n",
            "Epoch 6/25\n",
            "368/368 - 3s - loss: 15.5961 - mape: 16.2538 - val_loss: 9.6988 - val_mape: 6.4796 - 3s/epoch - 8ms/step\n",
            "Epoch 7/25\n",
            "368/368 - 3s - loss: 13.5637 - mape: 15.8993 - val_loss: 2.7146 - val_mape: 4.2481 - 3s/epoch - 8ms/step\n",
            "Epoch 8/25\n",
            "368/368 - 3s - loss: 15.9583 - mape: 18.9715 - val_loss: 6.6515 - val_mape: 7.3117 - 3s/epoch - 8ms/step\n",
            "Epoch 9/25\n",
            "368/368 - 2s - loss: 14.3267 - mape: 13.2809 - val_loss: 8.5683 - val_mape: 5.2245 - 2s/epoch - 6ms/step\n",
            "Epoch 10/25\n",
            "368/368 - 2s - loss: 18.9445 - mape: 17.7813 - val_loss: 5.0978 - val_mape: 14.4036 - 2s/epoch - 5ms/step\n",
            "Epoch 11/25\n",
            "368/368 - 2s - loss: 14.4509 - mape: 13.8600 - val_loss: 4.2145 - val_mape: 7.4460 - 2s/epoch - 4ms/step\n",
            "Epoch 12/25\n",
            "368/368 - 2s - loss: 12.9427 - mape: 12.4248 - val_loss: 4.3107 - val_mape: 3.6815 - 2s/epoch - 4ms/step\n",
            "Epoch 13/25\n",
            "368/368 - 2s - loss: 15.2447 - mape: 12.9798 - val_loss: 5.4129 - val_mape: 4.8930 - 2s/epoch - 4ms/step\n",
            "Epoch 14/25\n",
            "368/368 - 2s - loss: 11.5693 - mape: 9.6142 - val_loss: 3.6667 - val_mape: 14.1372 - 2s/epoch - 4ms/step\n",
            "Epoch 15/25\n",
            "368/368 - 2s - loss: 16.2705 - mape: 20.7596 - val_loss: 4.9773 - val_mape: 6.1776 - 2s/epoch - 5ms/step\n",
            "Epoch 16/25\n",
            "368/368 - 2s - loss: 14.6343 - mape: 12.4982 - val_loss: 3.7023 - val_mape: 2.3756 - 2s/epoch - 7ms/step\n",
            "Epoch 17/25\n",
            "368/368 - 3s - loss: 11.6618 - mape: 11.0979 - val_loss: 4.0339 - val_mape: 8.7624 - 3s/epoch - 7ms/step\n",
            "Epoch 18/25\n",
            "368/368 - 3s - loss: 12.3156 - mape: 14.1308 - val_loss: 6.0508 - val_mape: 7.9102 - 3s/epoch - 8ms/step\n",
            "Epoch 19/25\n",
            "368/368 - 3s - loss: 13.4134 - mape: 13.4123 - val_loss: 3.6583 - val_mape: 11.0207 - 3s/epoch - 8ms/step\n",
            "Epoch 20/25\n",
            "368/368 - 3s - loss: 10.5386 - mape: 11.0882 - val_loss: 6.1949 - val_mape: 21.1593 - 3s/epoch - 8ms/step\n",
            "Epoch 21/25\n",
            "368/368 - 2s - loss: 12.4532 - mape: 12.8800 - val_loss: 2.6600 - val_mape: 2.6855 - 2s/epoch - 5ms/step\n",
            "Epoch 22/25\n",
            "368/368 - 2s - loss: 11.5097 - mape: 11.4586 - val_loss: 3.3329 - val_mape: 8.1456 - 2s/epoch - 4ms/step\n",
            "Epoch 23/25\n",
            "368/368 - 2s - loss: 9.8466 - mape: 9.6164 - val_loss: 9.0980 - val_mape: 16.4430 - 2s/epoch - 5ms/step\n",
            "Epoch 24/25\n",
            "368/368 - 2s - loss: 18.8804 - mape: 19.2361 - val_loss: 3.2025 - val_mape: 5.8850 - 2s/epoch - 4ms/step\n",
            "Epoch 25/25\n",
            "368/368 - 2s - loss: 13.1457 - mape: 11.2044 - val_loss: 5.7217 - val_mape: 4.6503 - 2s/epoch - 4ms/step\n",
            "368/368 [==============================] - 1s 3ms/step\n",
            "46/46 [==============================] - 0s 3ms/step\n",
            "368/368 [==============================] - 1s 3ms/step\n",
            "46/46 [==============================] - 0s 3ms/step\n",
            "Oracle activations: 3948\n",
            "Runtime total_E activations: 3467.280176122225\n",
            "Runtime total_E_pred activations: 2982.012297195067\n",
            "Naive total_E activations: 3467.280176122225\n",
            "Naive total_E_pred activations: 4766.515859566406\n",
            "Dataset, train set, and test set size: 3680 2943 368\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 3.3298379408960916\n",
            "Maximum possible activations: 3948\n",
            "Predicted activations: 3495\n",
            "Successful activations: 3493, 99.943%\n",
            "Failed activations: 0, 0.000%\n",
            "Missed activations: 455, 11.525%\n",
            "Successful activations compared to max: 3493, 88.475%\n",
            "Naive predicted activations (usual actual energy average): 4907\n",
            "Naive successful activations (usual actual energy average): 1049, 21.378%\n",
            "Naive failed activations (usual actual energy average): 3854, 78.541%\n",
            "Naive missed activations (usual actual energy average): 2899, 73.430%\n",
            "Naive successful activations (compared to max): 1049, 26.570%\n",
            "Voltage overestimation rate: 0.000%\n",
            "Test MAPE power: 0.072950\n",
            "Test MAPE voltage: 0.072811\n",
            "Test MAPE current: 0.008564\n",
            "Predicted vs. Actual Total Energy Percent Difference: 3.898%\n",
            "Test MAPE power:\n",
            " 0.0729497345391971\n",
            "Predicted vs. Actual Total Voltage Percent Difference: -7.281%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save trained model\n",
        "!mv 'type1B_3min_quant50' 'drive/MyDrive/jLab Shared Docs/MFC Modeling'"
      ],
      "metadata": {
        "id": "tJoKmSm1UYw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Type 1 Models, validate on Dataset 1\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "\n",
        "power_mape = []\n",
        "voltage_mape = []\n",
        "current_mape = []\n",
        "\n",
        "E_actual_list = []\n",
        "E_pred_list = []\n",
        "\n",
        "max_act_list = []\n",
        "pred_act_list = []\n",
        "succ_act_list = []\n",
        "\n",
        "pred_act_naive_list = []\n",
        "false_act_naive_list = []\n",
        "succ_act_naive_list = []\n",
        "\n",
        "#Set parameters\n",
        "batchsize_list = [300, 150, 50, 20, 8]\n",
        "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
        "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
        "n = 0\n",
        "\n",
        "for j in range(len(batchsize_list)):\n",
        "  if j >=0:\n",
        "\n",
        "    X_train, X_test = train_test_split(X, test_size=0.3, shuffle=False)\n",
        "    y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
        "\n",
        "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
        "\n",
        "    batchsize = batchsize_list[j]\n",
        "    time_frame = time_frame_list[j]\n",
        "    time_frame_seconds = time_frame_seconds_list[j]\n",
        "\n",
        "    E_actual = 0\n",
        "    for i in range(len(y_test) - 1):\n",
        "      t = (y_test.index[i+1] - y_test.index[i]).total_seconds()\n",
        "      if t < 180:\n",
        "        E_actual += y_test['Power (uW)'][i] * t\n",
        "\n",
        "    #Set dataset bounds\n",
        "    train_bound_lower = y_train.index[0]\n",
        "    train_bound_upper = y_train.index[-1]\n",
        "    valid_bound_lower = y_valid.index[0]\n",
        "    valid_bound_upper = y_valid.index[-1]\n",
        "    test_bound_lower = y_test.index[0]\n",
        "    test_bound_upper = y_test.index[-1]\n",
        "\n",
        "    v_test = y_test.loc[(((y_test.index >= test_bound_lower)) & (y_test.index <= test_bound_upper))]['Voltage (mV)']\n",
        "    #v_test = v_test.drop(v_test[(v_test.index > '2021-11-11') & (v_test.index < '2021-11-22 01:00:00')].index)\n",
        "    v_test = pd.DataFrame(v_test)/1E5\n",
        "    v_avg_true = v_test['Voltage (mV)'].resample(time_frame).mean().dropna()\n",
        "    C0 = [0.007000000000000006, 0.007000000000000006, 0.007000000000000006]\n",
        "\n",
        "\n",
        "    #Resample data\n",
        "    X_train = X_train.resample(time_frame).mean().dropna()\n",
        "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
        "    X_test = X_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "    y_train = y_train.resample(time_frame).mean().dropna()\n",
        "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
        "    y_test = y_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "    print(v_test)\n",
        "\n",
        "    #Reshape data\n",
        "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
        "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    def quantile_loss(y_true, y_pred, quantile = 0.05):\n",
        "      error = y_true - y_pred\n",
        "      return K.mean(K.maximum(quantile * error, (quantile - 1) * error), axis=-1)\n",
        "\n",
        "    # design network\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(3))\n",
        "    model.compile(loss=quantile_loss, metrics=['mape'], optimizer='adam')\n",
        "\n",
        "    #Select model to load\n",
        "    file = 'drive/MyDrive/jLab Shared Docs/MFC Modeling/type1B_' + time_frame_list[j] + '_quant50'\n",
        "    print(file)\n",
        "    model = load_model(file, custom_objects={'quantile_loss': quantile_loss})\n",
        "\n",
        "\n",
        "    train_pred = model.predict(X_train, batch_size=batchsize)\n",
        "    test_pred = model.predict(X_test, batch_size=batchsize)\n",
        "\n",
        "    days  = getMFC_data(y_test, test_pred)\n",
        "\n",
        "    #Prepare data for runtime simulation\n",
        "    y_test['power pred'] = test_pred[:, 0]\n",
        "    y_test['Voltage (mV) pred'] = test_pred[:, 1]\n",
        "    y_test['Current (uA) pred'] = test_pred[:, 2]\n",
        "\n",
        "    v_avg_pred = y_test['Voltage (mV) pred']/1E5\n",
        "    days  = getMFC_data(y_test, test_pred)\n",
        "\n",
        "\n",
        "    #Remove first and last entries of averaged data to prevent overestimation of available energy\n",
        "    v_avg_true = v_avg_true[1:][:-1]\n",
        "    v_avg_pred = v_avg_pred[1:][:-1]\n",
        "\n",
        "\n",
        "    #Run oracle model\n",
        "    max_act = oracle_simulate(v_test, C0, time_frame_seconds)\n",
        "\n",
        "    #Call simulate function\n",
        "    pred_act, false_act, succ_act, total_E, total_E_pred = simulate(days, v_avg_true, v_avg_pred, v_test, C0)\n",
        "\n",
        "    #Run naive model\n",
        "    v_valid = y_valid.loc[(((y_valid.index >= valid_bound_lower)) & (y_valid.index < valid_bound_upper))]['Voltage (mV)']/1E5\n",
        "    pred_act_naive, false_act_naive, succ_act_naive, total_E = naive_simulate(days, v_avg_true, v_valid, v_test, C0)\n",
        "    print(\"Dataset, train set, and test set size:\", len(y_train) + len(y_valid) + len(y_test), len(y_train), len(y_test))\n",
        "    print('Timeframe:', time_frame)\n",
        "\n",
        "    print('Minimal Application')\n",
        "    if succ_act_naive > 0:\n",
        "      print(\"Naive vs. DL succesful activations:\", succ_act/succ_act_naive)\n",
        "    else:\n",
        "      print(\"No succesful naive activation\")\n",
        "\n",
        "    #print('Predicted vs. Actual percent difference: %.3f%%' % ((total_E * 100 / total_E_pred) - 100))\n",
        "    print('Maximum possible activations:', max_act)\n",
        "    print('Predicted activations:', pred_act)\n",
        "    print('Successful activations: %d, %.3f%%' % (succ_act, succ_act * 100/pred_act))\n",
        "    print('Failed activations: %d, %.3f%%' % (false_act, false_act * 100/pred_act))\n",
        "    print('Missed activations: %d, %.3f%%' % (max_act - succ_act, (max_act - succ_act) * 100/max_act))\n",
        "    print('Successful activations compared to max: %d, %.3f%%' % (succ_act, succ_act * 100/max_act))\n",
        "\n",
        "    #Naive model\n",
        "    print('Naive predicted activations (usual actual energy average):', pred_act_naive)\n",
        "    print('Naive successful activations (usual actual energy average): %d, %.3f%%' % (succ_act_naive, succ_act_naive * 100/pred_act_naive))\n",
        "    print('Naive failed activations (usual actual energy average): %d, %.3f%%' % (false_act_naive, false_act_naive * 100/pred_act_naive))\n",
        "    print('Naive missed activations (usual actual energy average): %d, %.3f%%' % (max_act - succ_act_naive, (max_act - succ_act_naive) * 100/max_act))\n",
        "\n",
        "    print('Naive successful activations (compared to max): %d, %.3f%%' % (succ_act_naive, succ_act_naive * 100/max_act))\n",
        "\n",
        "    print('Voltage overestimation rate: %.3f%%' % ((y_test['Voltage (mV)'].values <= y_test['Voltage (mV) pred']).mean() * 100))\n",
        "    print(\"Test MAPE power: %3f\" %  MAPE(y_test['Power (uW)'].values.ravel(), y_test['power pred']))\n",
        "    print(\"Test MAPE voltage: %3f\" % MAPE(y_test['Voltage (mV)'], y_test['Voltage (mV) pred']))\n",
        "    print(\"Test MAPE current: %3f\" % MAPE(y_test['Current (uA)'], y_test['Current (uA) pred']))\n",
        "\n",
        "    E_pred = 0\n",
        "    for i in range(len(y_test) - 1):\n",
        "      t = (y_test.index[i+1] - y_test.index[i]).total_seconds()\n",
        "      if t <= time_frame_seconds + 50:\n",
        "        E_pred += y_test['power pred'][i] * t\n",
        "\n",
        "    print('Predicted vs. Actual Total Energy Percent Difference: %.3f%%' % ((E_pred - E_actual) * 100 / E_actual))\n",
        "    print(\"Test MAPE power:\\n\", MAPE(y_test['Power (uW)'].values.ravel(), y_test['power pred'].values.ravel()))\n",
        "\n",
        "    V_actual = y_test['Voltage (mV)'].mean()\n",
        "    V_pred = y_test['Voltage (mV) pred'].mean()\n",
        "    print('Predicted vs. Actual Total Voltage Percent Difference: %.3f%%' % ((V_pred - V_actual) * 100 / V_actual))\n",
        "    print(\"Test MAPE voltage:\\n\", MAPE(y_test['Voltage (mV)'].values.ravel(), y_test['Voltage (mV) pred'].values.ravel()))\n",
        "\n",
        "\n",
        "\n",
        "    print('Voltage overestimation rate: %.3f%%' % ((y_test['Voltage (mV)'].values <= y_test['Voltage (mV) pred']).mean() * 100))\n",
        "    print(\"Test MAPE power: %3f\" %  MAPE(y_test['Power (uW)'].values.ravel(), y_test['power pred']))\n",
        "    print(\"Test MAPE voltage: %3f\" % MAPE(y_test['Voltage (mV)'], y_test['Voltage (mV) pred']))\n",
        "    print(\"Test MAPE current: %3f\" % MAPE(y_test['Current (uA)'], y_test['Current (uA) pred']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-NVRyqBS3FE",
        "outputId": "dc277ba8-7d33-476f-e83d-5eac4ca69ec2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2022-01-04 00:31:25-08:00      0.019359\n",
            "2022-01-04 00:31:39-08:00      0.017503\n",
            "2022-01-04 00:31:53-08:00      0.017273\n",
            "2022-01-04 00:32:06-08:00      0.018733\n",
            "2022-01-04 00:32:20-08:00      0.019295\n",
            "...                                 ...\n",
            "2022-01-26 23:58:52-08:00      0.024386\n",
            "2022-01-26 23:59:06-08:00      0.008550\n",
            "2022-01-26 23:59:19-08:00      0.008552\n",
            "2022-01-26 23:59:33-08:00      0.008208\n",
            "2022-01-26 23:59:47-08:00      0.007138\n",
            "\n",
            "[145448 rows x 1 columns]\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/type1B_3min_quant50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 1s 2ms/step\n",
            "37/37 [==============================] - 0s 2ms/step\n",
            "Oracle activations: 6675\n",
            "Runtime total_E activations: 5940.843624018538\n",
            "Runtime total_E_pred activations: 6021.188967615226\n",
            "Naive total_E activations: 5940.843624018538\n",
            "Naive total_E_pred activations: 12478.700330276833\n",
            "Dataset, train set, and test set size: 73456 51411 11030\n",
            "Timeframe: 3min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 13.306997742663658\n",
            "Maximum possible activations: 6675\n",
            "Predicted activations: 6035\n",
            "Successful activations: 5895, 97.680%\n",
            "Failed activations: 129, 2.138%\n",
            "Missed activations: 780, 11.685%\n",
            "Successful activations compared to max: 5895, 88.315%\n",
            "Naive predicted activations (usual actual energy average): 11060\n",
            "Naive successful activations (usual actual energy average): 443, 4.005%\n",
            "Naive failed activations (usual actual energy average): 10617, 95.995%\n",
            "Naive missed activations (usual actual energy average): 6232, 93.363%\n",
            "Naive successful activations (compared to max): 443, 6.637%\n",
            "Voltage overestimation rate: 59.338%\n",
            "Test MAPE power: 0.123999\n",
            "Test MAPE voltage: 0.018319\n",
            "Test MAPE current: 0.091034\n",
            "Predicted vs. Actual Total Energy Percent Difference: 4.363%\n",
            "Test MAPE power:\n",
            " 0.12399926056171036\n",
            "Predicted vs. Actual Total Voltage Percent Difference: 0.653%\n",
            "Test MAPE voltage:\n",
            " 0.01831870316324112\n",
            "Voltage overestimation rate: 59.338%\n",
            "Test MAPE power: 0.123999\n",
            "Test MAPE voltage: 0.018319\n",
            "Test MAPE current: 0.091034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2022-01-04 00:31:25-08:00      0.019359\n",
            "2022-01-04 00:31:39-08:00      0.017503\n",
            "2022-01-04 00:31:53-08:00      0.017273\n",
            "2022-01-04 00:32:06-08:00      0.018733\n",
            "2022-01-04 00:32:20-08:00      0.019295\n",
            "...                                 ...\n",
            "2022-01-26 23:58:52-08:00      0.024386\n",
            "2022-01-26 23:59:06-08:00      0.008550\n",
            "2022-01-26 23:59:19-08:00      0.008552\n",
            "2022-01-26 23:59:33-08:00      0.008208\n",
            "2022-01-26 23:59:47-08:00      0.007138\n",
            "\n",
            "[145448 rows x 1 columns]\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/type1B_5min_quant50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "206/206 [==============================] - 1s 2ms/step\n",
            "45/45 [==============================] - 0s 2ms/step\n",
            "Oracle activations: 6675\n",
            "Runtime total_E activations: 5936.226619506383\n",
            "Runtime total_E_pred activations: 5939.910872704137\n",
            "Naive total_E activations: 5936.226619506383\n",
            "Naive total_E_pred activations: 12477.678423857222\n",
            "Dataset, train set, and test set size: 44076 30848 6618\n",
            "Timeframe: 5min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 2.9484893511639427\n",
            "Maximum possible activations: 6675\n",
            "Predicted activations: 6005\n",
            "Successful activations: 5953, 99.134%\n",
            "Failed activations: 52, 0.866%\n",
            "Missed activations: 722, 10.816%\n",
            "Successful activations compared to max: 5953, 89.184%\n",
            "Naive predicted activations (usual actual energy average): 7733\n",
            "Naive successful activations (usual actual energy average): 2019, 26.109%\n",
            "Naive failed activations (usual actual energy average): 5714, 73.891%\n",
            "Naive missed activations (usual actual energy average): 4656, 69.753%\n",
            "Naive successful activations (compared to max): 2019, 30.247%\n",
            "Voltage overestimation rate: 49.894%\n",
            "Test MAPE power: 0.088336\n",
            "Test MAPE voltage: 0.009956\n",
            "Test MAPE current: 0.054174\n",
            "Predicted vs. Actual Total Energy Percent Difference: -1.847%\n",
            "Test MAPE power:\n",
            " 0.08833623911712706\n",
            "Predicted vs. Actual Total Voltage Percent Difference: 0.091%\n",
            "Test MAPE voltage:\n",
            " 0.009956331641675704\n",
            "Voltage overestimation rate: 49.894%\n",
            "Test MAPE power: 0.088336\n",
            "Test MAPE voltage: 0.009956\n",
            "Test MAPE current: 0.054174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2022-01-04 00:31:25-08:00      0.019359\n",
            "2022-01-04 00:31:39-08:00      0.017503\n",
            "2022-01-04 00:31:53-08:00      0.017273\n",
            "2022-01-04 00:32:06-08:00      0.018733\n",
            "2022-01-04 00:32:20-08:00      0.019295\n",
            "...                                 ...\n",
            "2022-01-26 23:58:52-08:00      0.024386\n",
            "2022-01-26 23:59:06-08:00      0.008550\n",
            "2022-01-26 23:59:19-08:00      0.008552\n",
            "2022-01-26 23:59:33-08:00      0.008208\n",
            "2022-01-26 23:59:47-08:00      0.007138\n",
            "\n",
            "[145448 rows x 1 columns]\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/type1B_15min_quant50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "206/206 [==============================] - 1s 2ms/step\n",
            "45/45 [==============================] - 0s 2ms/step\n",
            "Oracle activations: 6675\n",
            "Runtime total_E activations: 5926.920569910748\n",
            "Runtime total_E_pred activations: 5996.145057428384\n",
            "Naive total_E activations: 5926.920569910748\n",
            "Naive total_E_pred activations: 12455.35655836742\n",
            "Dataset, train set, and test set size: 14695 10285 2206\n",
            "Timeframe: 15min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 15.542105263157895\n",
            "Maximum possible activations: 6675\n",
            "Predicted activations: 6000\n",
            "Successful activations: 5906, 98.433%\n",
            "Failed activations: 94, 1.567%\n",
            "Missed activations: 769, 11.521%\n",
            "Successful activations compared to max: 5906, 88.479%\n",
            "Naive predicted activations (usual actual energy average): 11053\n",
            "Naive successful activations (usual actual energy average): 380, 3.438%\n",
            "Naive failed activations (usual actual energy average): 10673, 96.562%\n",
            "Naive missed activations (usual actual energy average): 6295, 94.307%\n",
            "Naive successful activations (compared to max): 380, 5.693%\n",
            "Voltage overestimation rate: 94.288%\n",
            "Test MAPE power: 0.062232\n",
            "Test MAPE voltage: 0.007299\n",
            "Test MAPE current: 0.020252\n",
            "Predicted vs. Actual Total Energy Percent Difference: 4.056%\n",
            "Test MAPE power:\n",
            " 0.06223241275789936\n",
            "Predicted vs. Actual Total Voltage Percent Difference: 0.634%\n",
            "Test MAPE voltage:\n",
            " 0.007299457707331139\n",
            "Voltage overestimation rate: 94.288%\n",
            "Test MAPE power: 0.062232\n",
            "Test MAPE voltage: 0.007299\n",
            "Test MAPE current: 0.020252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2022-01-04 00:31:25-08:00      0.019359\n",
            "2022-01-04 00:31:39-08:00      0.017503\n",
            "2022-01-04 00:31:53-08:00      0.017273\n",
            "2022-01-04 00:32:06-08:00      0.018733\n",
            "2022-01-04 00:32:20-08:00      0.019295\n",
            "...                                 ...\n",
            "2022-01-26 23:58:52-08:00      0.024386\n",
            "2022-01-26 23:59:06-08:00      0.008550\n",
            "2022-01-26 23:59:19-08:00      0.008552\n",
            "2022-01-26 23:59:33-08:00      0.008208\n",
            "2022-01-26 23:59:47-08:00      0.007138\n",
            "\n",
            "[145448 rows x 1 columns]\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/type1B_30min_quant50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258/258 [==============================] - 1s 2ms/step\n",
            "56/56 [==============================] - 0s 2ms/step\n",
            "Oracle activations: 6675\n",
            "Runtime total_E activations: 5917.1816081766065\n",
            "Runtime total_E_pred activations: 5858.583452663554\n",
            "Naive total_E activations: 5917.1816081766065\n",
            "Naive total_E_pred activations: 12439.674575895217\n",
            "Dataset, train set, and test set size: 7354 5148 1103\n",
            "Timeframe: 30min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 29.736318407960198\n",
            "Maximum possible activations: 6675\n",
            "Predicted activations: 5988\n",
            "Successful activations: 5977, 99.816%\n",
            "Failed activations: 11, 0.184%\n",
            "Missed activations: 698, 10.457%\n",
            "Successful activations compared to max: 5977, 89.543%\n",
            "Naive predicted activations (usual actual energy average): 12109\n",
            "Naive successful activations (usual actual energy average): 201, 1.660%\n",
            "Naive failed activations (usual actual energy average): 11908, 98.340%\n",
            "Naive missed activations (usual actual energy average): 6474, 96.989%\n",
            "Naive successful activations (compared to max): 201, 3.011%\n",
            "Voltage overestimation rate: 7.253%\n",
            "Test MAPE power: 0.039302\n",
            "Test MAPE voltage: 0.006371\n",
            "Test MAPE current: 0.010848\n",
            "Predicted vs. Actual Total Energy Percent Difference: 2.029%\n",
            "Test MAPE power:\n",
            " 0.03930196414263733\n",
            "Predicted vs. Actual Total Voltage Percent Difference: -0.503%\n",
            "Test MAPE voltage:\n",
            " 0.006370876292101077\n",
            "Voltage overestimation rate: 7.253%\n",
            "Test MAPE power: 0.039302\n",
            "Test MAPE voltage: 0.006371\n",
            "Test MAPE current: 0.010848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Voltage (mV)\n",
            "timestamp                              \n",
            "2022-01-04 00:31:25-08:00      0.019359\n",
            "2022-01-04 00:31:39-08:00      0.017503\n",
            "2022-01-04 00:31:53-08:00      0.017273\n",
            "2022-01-04 00:32:06-08:00      0.018733\n",
            "2022-01-04 00:32:20-08:00      0.019295\n",
            "...                                 ...\n",
            "2022-01-26 23:58:52-08:00      0.024386\n",
            "2022-01-26 23:59:06-08:00      0.008550\n",
            "2022-01-26 23:59:19-08:00      0.008552\n",
            "2022-01-26 23:59:33-08:00      0.008208\n",
            "2022-01-26 23:59:47-08:00      0.007138\n",
            "\n",
            "[145448 rows x 1 columns]\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/type1B_60min_quant50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "322/322 [==============================] - 1s 2ms/step\n",
            "69/69 [==============================] - 0s 3ms/step\n",
            "Oracle activations: 6675\n",
            "Runtime total_E activations: 5910.5568313543445\n",
            "Runtime total_E_pred activations: 5853.1653703265765\n",
            "Naive total_E activations: 5910.5568313543445\n",
            "Naive total_E_pred activations: 12416.708048531726\n",
            "Dataset, train set, and test set size: 3680 2576 552\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 30.989583333333332\n",
            "Maximum possible activations: 6675\n",
            "Predicted activations: 5984\n",
            "Successful activations: 5950, 99.432%\n",
            "Failed activations: 34, 0.568%\n",
            "Missed activations: 725, 10.861%\n",
            "Successful activations compared to max: 5950, 89.139%\n",
            "Naive predicted activations (usual actual energy average): 12088\n",
            "Naive successful activations (usual actual energy average): 192, 1.588%\n",
            "Naive failed activations (usual actual energy average): 11896, 98.412%\n",
            "Naive missed activations (usual actual energy average): 6483, 97.124%\n",
            "Naive successful activations (compared to max): 192, 2.876%\n",
            "Voltage overestimation rate: 3.623%\n",
            "Test MAPE power: 0.033288\n",
            "Test MAPE voltage: 0.007430\n",
            "Test MAPE current: 0.005871\n",
            "Predicted vs. Actual Total Energy Percent Difference: 2.739%\n",
            "Test MAPE power:\n",
            " 0.03328816349462515\n",
            "Predicted vs. Actual Total Voltage Percent Difference: -0.497%\n",
            "Test MAPE voltage:\n",
            " 0.007430420562831291\n",
            "Voltage overestimation rate: 3.623%\n",
            "Test MAPE power: 0.033288\n",
            "Test MAPE voltage: 0.007430\n",
            "Test MAPE current: 0.005871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJa0_HQqdpUn",
        "outputId": "00dc70d5-f163-4e2e-ac84-436dd57b60ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "115/115 [==============================] - 1s 3ms/step\n",
            "115/115 [==============================] - 1s 4ms/step\n",
            "115/115 [==============================] - 1s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "#Select pretrained models to graph\n",
        "\n",
        "def quantile_loss(y_true, y_pred, quantile = 0.5):\n",
        "    error = y_true - y_pred\n",
        "    return K.mean(K.maximum(quantile * error, (quantile - 1) * error), axis=-1)\n",
        "\n",
        "model = load_model(\"drive/MyDrive/jLab Shared Docs/MFC Modeling/type1A_60min_quant50\", custom_objects={'quantile_loss': quantile_loss})\n",
        "predictions = model.predict(np.concatenate((X_train, X_valid, X_test)))\n",
        "mv1 = df\n",
        "mv1[\"power_pred_med\"] = predictions[:, 0]\n",
        "mv1[\"voltage_pred_med\"] = predictions[:, 1]\n",
        "mv1[\"current_pred_med\"] = predictions[:, 2]\n",
        "\n",
        "def quantile_loss(y_true, y_pred, quantile = 0.05):\n",
        "    error = y_true - y_pred\n",
        "    return K.mean(K.maximum(quantile * error, (quantile - 1) * error), axis=-1)\n",
        "\n",
        "model = load_model(\"drive/MyDrive/jLab Shared Docs/MFC Modeling/type1A_60min_quant5\", custom_objects={'quantile_loss': quantile_loss})\n",
        "predictions = model.predict(np.concatenate((X_train, X_valid, X_test)))\n",
        "mv1 = df\n",
        "mv1[\"power_pred_lower\"] = predictions[:, 0]\n",
        "mv1[\"voltage_pred_lower\"] = predictions[:, 1]\n",
        "mv1[\"current_pred_lower\"] = predictions[:, 2]\n",
        "\n",
        "def quantile_loss(y_true, y_pred, quantile = 0.75):\n",
        "    error = y_true - y_pred\n",
        "    return K.mean(K.maximum(quantile * error, (quantile - 1) * error), axis=-1)\n",
        "\n",
        "model = load_model(\"drive/MyDrive/jLab Shared Docs/MFC Modeling/type1A_60min_quant75\", custom_objects={'quantile_loss': quantile_loss})\n",
        "predictions = model.predict(np.concatenate((X_train, X_valid, X_test)))\n",
        "mv1 = df\n",
        "mv1[\"power_pred_upper\"] = predictions[:, 0]\n",
        "mv1[\"voltage_pred_upper\"] = predictions[:, 1]\n",
        "mv1[\"current_pred_upper\"] = predictions[:, 2]\n",
        "\n",
        "\n",
        "mv1 = mv1.loc[(mv1.index > '2021-12-12') & (mv1.index < '2021-12-14')]\n",
        "mv2 = mv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "Myp7Hcfim1Qp",
        "outputId": "6c86a706-6e86-4cbf-dfad-66425f97232b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib as mpl\n",
        "mpl.use('Agg')\n",
        "#mpl.rc('font', **font)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.dates as md\n",
        "import datetime\n",
        "import numpy as np\n",
        "from pytz import timezone\n",
        "import pandas as pd\n",
        "import arrow\n",
        "import os\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "%matplotlib inline\n",
        "\n",
        "# Limits for graphs\n",
        "VOLTAGE_LIM = 0.2\n",
        "CURRENT_LIM = 40\n",
        "POWER_LIM = 4\n",
        "\n",
        "line_width = 0.5\n",
        "plt.title(\"1 hour prediction interval\")\n",
        "\n",
        "plt.close()\n",
        "plt.xlabel(\"Time\")\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3,figsize=(4,3), sharex=False)\n",
        "fig.autofmt_xdate()\n",
        "\n",
        "volt_color= 'tab:blue'\n",
        "\n",
        "amp_color = 'tab:red'\n",
        "\n",
        "\n",
        "volt_color1= 'tab:blue'\n",
        "volt_style1 = 'dashed'\n",
        "volt_color2= 'tab:green'\n",
        "volt_style2 = 'dashed'\n",
        "volt_color3= 'tab:red'\n",
        "volt_style3 = 'dashed'\n",
        "volt_color4= 'tab:orange'\n",
        "volt_style4 = 'dashed'\n",
        "\n",
        "#amp_color1 = 'tab:red'\n",
        "#amp_style1='dashed'\n",
        "#amp_color2 = 'tab:orange'\n",
        "#amp_style2='dashdot'\n",
        "\n",
        "ax1.fmt_xdata = md.DateFormatter('%m-%d')\n",
        "ax1.xaxis.set_major_formatter(md.DateFormatter('%m-%d'))\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.fmt_xdata = md.DateFormatter('%m-%d')\n",
        "ax2.xaxis.set_major_formatter(md.DateFormatter('%m-%d'))\n",
        "ax2.grid(True)\n",
        "\n",
        "ax3.fmt_xdata = md.DateFormatter('%m-%d')\n",
        "ax3.xaxis.set_major_formatter(md.DateFormatter('%m-%d'))\n",
        "ax3.grid(True)\n",
        "\n",
        "ax1.set_ylabel(\"Power (μW)\", fontsize=7.5, labelpad = 2.5)\n",
        "ax2.set_ylabel(\"Voltage (mV)\", fontsize=7.5, labelpad = 2.5)\n",
        "ax3.set_ylabel(\"Current (μA)\", fontsize=7.5, labelpad = 2.5)\n",
        "#ax3.set_xlabel(\"Date\", fontsize=10)\n",
        "\n",
        "ax1.set_ylim(0, 1.5)\n",
        "ax2.set_ylim(0, 60)\n",
        "ax3.set_ylim(0, 40)\n",
        "\n",
        "#ax3.set_xlim([datetime.date(2021, 12, 12), datetime.date(2021, 12, 14)])\n",
        "ax1.set_xlim([mv1.index[0], mv1.index[-1]])\n",
        "ax2.set_xlim([mv1.index[0], mv1.index[-1]])\n",
        "ax3.set_xlim([mv1.index[0], mv1.index[-1]])\n",
        "\n",
        "ax1.plot(mv1.index, mv2[\"power_pred_lower\"] * 1E-3, color='tab:red', ls = volt_style3, linewidth=line_width)\n",
        "ax1.plot(mv1.index, mv2[\"power_pred_upper\"] * 1E-3, color='tab:orange', ls = volt_style4, linewidth=line_width)\n",
        "ax1.plot(mv1.index, mv2[\"power\"] * 1E-3, color='tab:blue', ls = 'solid', linewidth=line_width)\n",
        "ax1.plot(mv1.index, mv2[\"power_pred_med\"] * 1E-3, color='tab:green', ls = volt_style2, linewidth=line_width)\n",
        "ax1.fill_between(mv1.index, mv2[\"power_pred_lower\"] * 1E-3, mv2[\"power_pred_upper\"] * 1E-3, color='grey', alpha=0.5)\n",
        "#ax1.legend(['lower bound', 'upper bound', 'actual', 'median predictions'], loc='lower center', prop={'size': 5.5}, ncol=2)\n",
        "\n",
        "ax2.plot(mv1.index, mv2[\"voltage_pred_lower\"] * 1E-2, color='tab:red', ls = volt_style3, linewidth=line_width)\n",
        "ax2.plot(mv1.index, mv2[\"voltage_pred_upper\"] * 1E-2, color='tab:orange', ls = volt_style4, linewidth=line_width)\n",
        "ax2.plot(mv1.index, mv2['V1 [mV]'] * 1E-2, color='tab:blue', ls = 'solid', linewidth=line_width)\n",
        "ax2.plot(mv1.index, mv2[\"voltage_pred_med\"] * 1E-2, color='tab:green', ls = volt_style2, linewidth=line_width)\n",
        "ax2.fill_between(mv1.index, mv2[\"voltage_pred_lower\"] * 1E-2, mv2[\"voltage_pred_upper\"] * 1E-2, color='grey', alpha=0.5)\n",
        "ax2.legend(['lower bound', 'upper bound', 'ground truth', 'median prediction'], loc='lower left', prop={'size': 6.6}, ncol=2, columnspacing=0.5)\n",
        "\n",
        "ax3.plot(mv1.index, mv2[\"current_pred_lower\"] * 1E-2, color='tab:red', ls = volt_style3, linewidth=line_width)\n",
        "ax3.plot(mv1.index, mv2[\"current_pred_upper\"] * 1E-2, color='tab:orange', ls = volt_style4, linewidth=line_width)\n",
        "ax3.plot(mv1.index, mv2['I1L [μA]'] * 1E-2, color='tab:blue', ls = 'solid', linewidth=line_width)\n",
        "ax3.plot(mv1.index, mv2[\"current_pred_med\"] * 1E-2, color='tab:green', ls = volt_style2, linewidth=line_width)\n",
        "ax3.fill_between(mv1.index, mv2[\"current_pred_lower\"] * 1E-2, mv2[\"current_pred_upper\"] * 1E-2, color='grey', alpha=0.5)\n",
        "#ax3.legend(['lower bound', 'upper bound', 'actual', 'median predictions'], loc='upper right', prop={'size': 5.5}, ncol=1)\n",
        "\n",
        "\n",
        "#Plot error\n",
        "#ax3.plot(mv1['timestamp'], mv1['error']/mv1['power'], color=volt_color2, ls = volt_style2)\n",
        "#ax3.legend(['error'], loc='upper right', prop={'size': 6})\n",
        "\n",
        "ax3.tick_params(axis='x', labelsize=7.5, rotation=0, pad = 0.1)\n",
        "ax3.set_xticks(list(ax3.get_xticks()) + [ax3.get_xlim()[0], ax3.get_xlim()[1]])\n",
        "for label in ax3.get_xticklabels():\n",
        "    label.set_horizontalalignment('center')\n",
        "\n",
        "ax1.tick_params(axis='y', labelsize=7.5, rotation=0, pad = 0.1)\n",
        "ax2.tick_params(axis='y', labelsize=7.5, rotation=0, pad = 0.1)\n",
        "ax3.tick_params(axis='y', labelsize=7.5, rotation=0, pad = 0.1)\n",
        "\n",
        "plt.tight_layout(pad=0.3, w_pad=0.5, h_pad=0.1)\n",
        "#plt.subplots_adjust(hspace=0.15)\n",
        "plt.savefig('twobat.pdf')\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}