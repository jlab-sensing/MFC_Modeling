{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlab-sensing/MFC_Modeling/blob/main/SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcF-Dv23N_ON",
        "outputId": "a412105a-84d6-4d11-94bf-bac9f7bc8494",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hepml in /usr/local/lib/python3.10/dist-packages (0.0.12)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from hepml) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from hepml) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hepml) (1.25.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from hepml) (0.13.1)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from hepml) (24.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hepml) (4.66.4)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from hepml) (3.2)\n",
            "Requirement already satisfied: nbdev in /usr/local/lib/python3.10/dist-packages (from hepml) (2.3.25)\n",
            "Requirement already satisfied: sklearn-pandas in /usr/local/lib/python3.10/dist-packages (from hepml) (2.2.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from hepml) (0.20.3)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from hepml) (5.1.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from hepml) (14.0.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from hepml) (0.58.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from hepml) (3.0.10)\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.10/dist-packages (from hepml) (1.0.3)\n",
            "Requirement already satisfied: giotto-tda in /usr/local/lib/python3.10/dist-packages (from hepml) (0.6.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from hepml) (9.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (24.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->hepml) (4.12.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->hepml) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->hepml) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->hepml) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (1.4.2)\n",
            "Requirement already satisfied: giotto-ph>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (0.2.4)\n",
            "Requirement already satisfied: pyflagser>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (0.4.7)\n",
            "Requirement already satisfied: igraph>=0.9.8 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (0.11.5)\n",
            "Requirement already satisfied: plotly>=4.8.2 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (5.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.10/dist-packages (from giotto-tda->hepml) (7.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->hepml) (3.5.0)\n",
            "Requirement already satisfied: fastcore>=1.5.27 in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (1.5.43)\n",
            "Requirement already satisfied: execnb>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (0.1.6)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (1.6.3)\n",
            "Requirement already satisfied: ghapi>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (1.0.5)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (4.0.1)\n",
            "Requirement already satisfied: asttokens in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (2.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from nbdev->hepml) (6.0.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->hepml) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->hepml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hepml) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hepml) (2024.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->hepml) (3.7.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from execnb>=0.1.4->nbdev->hepml) (7.34.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from ghapi>=1.0.3->nbdev->hepml) (23.1.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from igraph>=0.9.8->giotto-tda->hepml) (1.7.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->giotto-tda->hepml) (3.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->hepml) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.8.2->giotto-tda->hepml) (8.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->hepml) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->nbdev->hepml) (0.43.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->hepml) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->hepml) (1.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->giotto-tda->hepml) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->giotto-tda->hepml) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (3.0.45)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev->hepml) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->execnb>=0.1.4->nbdev->hepml) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->execnb>=0.1.4->nbdev->hepml) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->execnb>=0.1.4->nbdev->hepml) (0.2.13)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (4.9.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.18.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->giotto-tda->hepml) (1.2.1)\n",
            "Collecting arrow\n",
            "  Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow)\n",
            "  Using cached types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow) (1.16.0)\n",
            "Installing collected packages: types-python-dateutil, arrow\n",
            "Successfully installed arrow-1.3.0 types-python-dateutil-2.9.0.20240316\n",
            "Requirement already satisfied: keras_lr_finder in /usr/local/lib/python3.10/dist-packages (0.1)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from keras_lr_finder) (2.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from keras_lr_finder) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->keras_lr_finder) (1.16.0)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade hepml\n",
        "!pip install arrow\n",
        "!pip install keras_lr_finder\n",
        "! pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJsRk5_qTljC"
      },
      "outputs": [],
      "source": [
        "# reload modules before executing user code\n",
        "#%load_ext autoreload\n",
        "# reload all modules every time before executing Python code\n",
        "#%autoreload 2\n",
        "# render plots in notebook\n",
        "\n",
        "# Misc imports\n",
        "%matplotlib inline\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from hepml.core import plot_regression_tree\n",
        "sns.set(color_codes=True)\n",
        "sns.set_palette(sns.color_palette(\"muted\"))\n",
        "import random\n",
        "import statistics\n",
        "\n",
        "# sklearn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# snnTorch imports\n",
        "import snntorch as snn\n",
        "from snntorch import functional as SF\n",
        "import snntorch.spikeplot as splt\n",
        "\n",
        "# keras imports\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7PoS8fwOS-Q"
      },
      "outputs": [],
      "source": [
        "#Load teros data\n",
        "import glob\n",
        "teros_files = glob.glob(\"rocket4/TEROSoutput*.csv\")\n",
        "X = pd.DataFrame()\n",
        "for f in teros_files:\n",
        "  try:\n",
        "    csv = pd.read_csv(f, index_col=False).dropna()\n",
        "    X = pd.concat([X, csv])\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaaHwFRvXtN4"
      },
      "outputs": [],
      "source": [
        "#Load power data\n",
        "power_files = glob.glob(\"rocket4/soil*.csv\")\n",
        "y = pd.DataFrame()\n",
        "for f in sorted(power_files, key=lambda x: int(x.split('.')[0].split('_')[-1])):\n",
        "#in power_files:\n",
        "  try:\n",
        "    csv = pd.read_csv(f, on_bad_lines='skip', skiprows=10).dropna(how='all')\n",
        "    csv = csv.rename({'Unnamed: 0': 'timestamp'}, axis='columns')\n",
        "    y = pd.concat([y,csv])\n",
        "  except:\n",
        "    continue\n",
        "y[\"timestamp\"] = y[\"timestamp\"].round(decimals = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViS0TjC4n6lk"
      },
      "outputs": [],
      "source": [
        "#Convert current to amps, voltage to volts\n",
        "y[\"I1L [10pA]\"] = np.abs(y[\"I1L [10pA]\"] * 1E-11)\n",
        "y[\"V1 [10nV]\"] = np.abs(y[\"V1 [10nV]\"] * 1E-8)\n",
        "y[\"I1H [nA]\"] = np.abs(y[\"I1H [nA]\"] * 1E-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0aqozsZpSOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a89940-ea47-4ff3-f69f-4e776601c507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-38f7a5b5fe7c>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n"
          ]
        }
      ],
      "source": [
        "#Sort data by timestamp, convert to datetime\n",
        "X = X.sort_values(['timestamp'])\n",
        "y = y.sort_values(['timestamp'])\n",
        "X['timestamp'] = pd.to_datetime(X['timestamp'], unit='s')\n",
        "y['timestamp'] = pd.to_datetime(y['timestamp'], unit='s')\n",
        "\n",
        "#Merge data by timestamp\n",
        "uncut_df = pd.merge_asof(left=X,right=y,direction='nearest',tolerance=pd.Timedelta('1 sec'), on = 'timestamp').dropna(how='all')\n",
        "\n",
        "#Isolate data from cell0\n",
        "df = uncut_df.loc[uncut_df['sensorID'] == 0]\n",
        "\n",
        "#Localize timestamp\n",
        "df.timestamp = df.timestamp.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBAfOHB61Jwc"
      },
      "outputs": [],
      "source": [
        "#Use only data from after deployment date\n",
        "#df = df.loc[(df['timestamp'] > '2021-09-24') & (df['timestamp'] < '2021-10-15')] #Future of Clean Computing Graph\n",
        "#df = df.loc[(df['timestamp'] > '2021-06-24') & (df['timestamp'] < '2021-07-02')]\n",
        "#df = df.loc[(df['timestamp'] > '2021-06-18')] #Two weeks after deployment\n",
        "df = df.loc[(df['timestamp'] > '2021-06-04')] #Deployment date\n",
        "#df = df.loc[(df['timestamp'] > '2021-06-25') & (df['timestamp'] < '2021-06-26')] #Small training set\n",
        "\n",
        "#Power drop\n",
        "#df = df.loc[(df['timestamp'] > '2021-11-01') & (df['timestamp'] < '2021-11-22')]\n",
        "\n",
        "#Drop data outages\n",
        "df = df.drop(df[(df.timestamp > '2021-11-11') & (df.timestamp < '2021-11-22 01:00:00')].index)\n",
        "df = df.drop(df[(df.timestamp > '2022-01-27')].index)\n",
        "#df = df.set_index('timestamp')\n",
        "df = df[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8fFsoLNfrFA"
      },
      "outputs": [],
      "source": [
        "df = df.set_index('timestamp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kNxDOBwk7IN"
      },
      "outputs": [],
      "source": [
        "#Get time since deployement\n",
        "df['tsd'] = (df.index - df.index[0]).days\n",
        "df['hour'] = (df.index).hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pfyv0fM3te1"
      },
      "outputs": [],
      "source": [
        "#Calculate power\n",
        "df[\"power\"] = np.abs(np.multiply(df.iloc[:, 7], df.iloc[:, 8]))\n",
        "#df[\"power\"] = np.abs(np.multiply(df[\"I1L [10pA]\"], df[\"V1 [10nV]\"]))\n",
        "\n",
        "#Convert to nW\n",
        "df['power'] = df['power']*1E9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA-WVzVh2-lf"
      },
      "outputs": [],
      "source": [
        "#Convert to 10 nanoamps, 10 microvolts\n",
        "df[\"I1L [10pA]\"] = np.abs(df[\"I1L [10pA]\"] * 1E8)\n",
        "df[\"V1 [10nV]\"] = np.abs(df[\"V1 [10nV]\"] * 1E5)\n",
        "df[\"I1H [nA]\"] = np.abs(df[\"I1H [nA]\"] * 1E8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxAlm4FXh57m"
      },
      "outputs": [],
      "source": [
        "df = df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sHtXZCVrsQJ"
      },
      "outputs": [],
      "source": [
        "#Add power time series\n",
        "df['power - 1h'] = df['power'].shift(1).dropna()\n",
        "df['power - 2h'] = df['power'].shift(2).dropna()\n",
        "df['power - 3h'] = df['power'].shift(3).dropna()\n",
        "#df['power - 2h'] = df['power'].shift(2).dropna()\n",
        "#df['previous_power - 3'] = df['power'].shift(3).dropna()\n",
        "#df['previous_power - 4'] = df['power'].shift(4).dropna()\n",
        "\n",
        "#Add teros time series\n",
        "df['EC - 1h'] = df['EC'].shift(1).dropna()\n",
        "df['EC - 2h'] = df['EC'].shift(2).dropna()\n",
        "df['EC - 3h'] = df['EC'].shift(3).dropna()\n",
        "\n",
        "df['temp - 1h'] = df['temp'].shift(1).dropna()\n",
        "df['temp - 2h'] = df['temp'].shift(2).dropna()\n",
        "df['temp - 3h'] = df['temp'].shift(3).dropna()\n",
        "\n",
        "df['raw_VWC - 1h'] = df['raw_VWC'].shift(1).dropna()\n",
        "df['raw_VWC - 2h'] = df['raw_VWC'].shift(2).dropna()\n",
        "df['raw_VWC - 3h'] = df['raw_VWC'].shift(3).dropna()\n",
        "\n",
        "#Add voltage and current time series\n",
        "df['V1 - 1h'] = df['V1 [10nV]'].shift(1).dropna()\n",
        "df['V1 - 2h'] = df['V1 [10nV]'].shift(2).dropna()\n",
        "df['V1 - 3h'] = df['V1 [10nV]'].shift(3).dropna()\n",
        "\n",
        "df['I1L - 1h'] = df['I1L [10pA]'].shift(1).dropna()\n",
        "df['I1L - 2h'] = df['I1L [10pA]'].shift(2).dropna()\n",
        "df['I1L - 3h'] = df['I1L [10pA]'].shift(3).dropna()\n",
        "\n",
        "df['I1H - 1h'] = df['I1H [nA]'].shift(1).dropna()\n",
        "df['I1H - 2h'] = df['I1H [nA]'].shift(2).dropna()\n",
        "df['I1H - 3h'] = df['I1H [nA]'].shift(3).dropna()\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4SM1_EvGS6y"
      },
      "outputs": [],
      "source": [
        "#df = df.rename(columns={'power': 'power [μW]'})\n",
        "df = df.rename(columns={'I1L [10pA]': 'Current (uA)', 'V1 [10nV]' : 'Voltage (mV)', 'power' : 'Power (uW)'})\n",
        "df = df.set_index('timestamp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "239oMsSK72B3"
      },
      "outputs": [],
      "source": [
        "#New runtime calculation\n",
        "import math\n",
        "from dateutil import parser\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def internal_R_v3(R=2000): #return internal resistance of v3 cells in ohms\n",
        "    #https://www.jstage.jst.go.jp/article/jwet/20/1/20_21-087/_pdf\n",
        "    v0_oc = 48.5e-3 #48.5 mV\n",
        "    v0_cc = 4.8e-3\n",
        "    v0_r = R*((v0_oc/v0_cc)-1)\n",
        "\n",
        "    v1_oc = 43.8e-3\n",
        "    v1_cc = 20.9e-3\n",
        "    v1_r = R*((v1_oc/v1_cc)-1)\n",
        "\n",
        "    v2_oc = 45.2e-3\n",
        "    v2_cc = 23.5e-3\n",
        "    v2_r = R*((v2_oc/v2_cc)-1)\n",
        "\n",
        "    return (v0_r+v1_r+v2_r)/3\n",
        "\n",
        "def internal_R_v0(R=2000): #return internal resistance of v0 cells in ohms\n",
        "    v3_oc = 41.7e-3 #41.7mV\n",
        "    v3_cc = 5.1e-3\n",
        "    v3_r = R*((v3_oc/v3_cc)-1)\n",
        "\n",
        "    v4_oc = 48.7e-3\n",
        "    v4_cc = 16.8e-3\n",
        "    v4_r = R*((v4_oc/v4_cc)-1)\n",
        "\n",
        "    v5_oc = 39.1e-3\n",
        "    v5_cc = 16.9e-3\n",
        "    v5_r = R*((v5_oc/v5_cc)-1)\n",
        "\n",
        "    return (v3_r+v4_r+v5_r)/3\n",
        "\n",
        "def SMFC_current(v, R):\n",
        "    return v/R\n",
        "\n",
        "#MODEL\n",
        "def cap_leakage(E_cap_tn, timestep):\n",
        "    #Spec for KEMET T491\n",
        "    return 0.01e-6 * E_cap_tn * timestep\n",
        "\n",
        "def Matrix_Power(V, R):\n",
        "    #efficiency interpolated from https://www.analog.com/media/en/technical-documentation/data-sheets/ADP5091-5092.pdf\n",
        "    #given I_in = 100 uA and SYS = 3V\n",
        "    #V is the voltage (V) of the SMFC we captured\n",
        "    #R is the resistance (ohms) of the load we used to get that voltage trace\n",
        "    #Eta = -292.25665*V**4 + 784.30311*V**3 - 770.71691*V**2 + 342.00502*V + 15.83307\n",
        "    #Eta = Eta/100\n",
        "    Eta = 0.60\n",
        "    Pmax = (V**2)/R\n",
        "    Pout = Eta*Pmax\n",
        "    #assert((Eta > 0) & (Eta < 1))\n",
        "    #assert(Pout < 12000e-6)\n",
        "    return Pout\n",
        "\n",
        "def update_capEnergy(e0, V_applied, R, C, dt):\n",
        "    # e0: initial energy stored\n",
        "    # V_applied: voltage from SMFC\n",
        "    # R: internal resistance of SMFC\n",
        "    # C: capacitance of capacitor\n",
        "    # dt: time step since last data point\n",
        "    e_cap = e0 + Matrix_Power(V_applied, R)*dt - cap_leakage(e0, dt)\n",
        "    v_cap = math.sqrt(2*e_cap/C)\n",
        "    if e_cap < 0: #Not charging if leakage is greater than energy\n",
        "        e_cap = 0\n",
        "\n",
        "    return e_cap, v_cap #output final e and v\n",
        "\n",
        "def Advanced_energy():\n",
        "    #Now representing \"Advanced\"\n",
        "    #startup time of 2500 ms\n",
        "    t = 2500e-3\n",
        "    e = 2.4 * 128e-3 * t\n",
        "    e_startup = 2.4 * 128e-3 * 5e-3\n",
        "    return e+e_startup\n",
        "\n",
        "def Minimal_energy():\n",
        "    #Now representing \"Minimal\"\n",
        "    t = 0.888e-3 #tentative time\n",
        "    e = 0.9 * 4.8e-3 * t #this uses average current\n",
        "    e_startup = 0#assume negligible, no known startup time given\n",
        "    return  e + e_startup\n",
        "\n",
        "def Analog_energy():\n",
        "    #Now representing Analog\n",
        "    t = 1e-3 #estimated operating time\n",
        "    e = 0.11 * 2.15e-6 * t\n",
        "    e_startup = 0 #analog device, no startup needed :)\n",
        "    return e + e_startup\n",
        "\n",
        "#STEP 3:\n",
        "# For each day:\n",
        "#   on_Minimal, on_Advanced, on_Analog = 0\n",
        "#   For each time step (like every 60 s given our logging freq):\n",
        "#       - Update the energy in our capacitor (put fcn in models.py) given (1) input voltage, (2) time step, (3) capacitance (prob 10 uF), this will be an integral\n",
        "#       - Check if energy is enough to turn on (1) 1 uJ load, (2) 10 uJ load, and (3) 20 uJ load (will tweak later to reflect real energy cost of each system)\n",
        "#       - If so, add to on_Minimal, on_Advanced, and on_Analog and reset capacitor energy to 0 J (might tweak this value)\n",
        "#   Append on_Minimal, on_Advanced, on_Analog to on_Minimal_list, on_Advanced_list, on_Analog_list. This will be a list of how many sensor readings we are able to take with each of these systems every day given the energy we got\n",
        "#STEP 4: Visualize the daily # of readings with 3 bar graphs, y axis is # of readings and x axis is days.\n",
        "#   - Given 3 lists of integer values, plot them on bar graphs\n",
        "\n",
        "def group_util(test_date1, test_date2, N):\n",
        "    diff = (test_date2 - test_date1) / N\n",
        "    return [test_date1 + diff * idx for idx in range(N)] + [test_date2]\n",
        "\n",
        "def oracle_simulate(v_list, C_h):\n",
        "    #Calculate maximum energy\n",
        "    total_E = 0\n",
        "    for i in range(len(v_list) - 1):\n",
        "        t = (v_list.index[i+1] - v_list.index[i]).total_seconds()\n",
        "        if t > 180:\n",
        "          print(\"Discontinuity\")\n",
        "          print(v_list.index[i+1], v_list.index[i])\n",
        "          print(v_list['V1 [mV]'][i+1], v_list['V1 [mV]'][i])\n",
        "          #total_E, ignore = update_capEnergy(total_E, V_applied=(v_list['V1 [mV]'][i+1] + v_list['V1 [mV]'][i])/2, R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "        else:\n",
        "          total_E, ignore = update_capEnergy(total_E, V_applied=max(v_list['V1 [mV]'][i], v_list['V1 [mV]'][i+1]), R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "    print(\"Oracle activations:\", math.floor(total_E/Minimal_energy()))\n",
        "    return(math.floor(total_E/Minimal_energy()))\n",
        "\n",
        "def naive_simulate(t_list, v_list, v_list_naive, v_list_fine, C_h):\n",
        "    # t_list: list of decimal time stamps in unit of days (e.g. 71.85893518518519 day), same length as v_list\n",
        "    # v_list: list of voltage values from SFMC\n",
        "    # C_h: capacitance of the capacitor being filled up by harvester\n",
        "\n",
        "    #assume capacitor is completely discharged at start\n",
        "    e_minimal_stored = 0\n",
        "    e_minimal_stored_theo = 0\n",
        "\n",
        "    #Initialize evaluation metrics\n",
        "    false_act = 0\n",
        "    max_act = 0\n",
        "    pred_act = 0\n",
        "    succ_act = 0\n",
        "\n",
        "    total_E = 0\n",
        "    total_E_naive = 0\n",
        "\n",
        "    #Calculate maximum energy\n",
        "    #for i in range(len(v_list_fine) - 1):\n",
        "    #    t = (v_list_fine.index[i+1] - v_list_fine.index[i]).total_seconds()\n",
        "    #    total_E, ignore = update_capEnergy(total_E, V_applied=v_list_fine['V1 [10nV]'][i], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "    #print(total_E/Minimal_energy())\n",
        "    v = v_list_naive.mean()\n",
        "    #for each voltage data point\n",
        "    for jj in range(len(v_list) - 1): #last data point was at 71.85893518518519 day\n",
        "        t = (v_list.index[jj+1] - v_list.index[jj]).total_seconds()\n",
        "        if t <= time_frame_seconds:\n",
        "          #Total predicted vs. actual energy stored\n",
        "          #Predict energy stored during scheduled sub-interval\n",
        "          total_E, ignore = update_capEnergy(total_E, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "          total_E_naive, ignore = update_capEnergy(total_E_naive, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "\n",
        "          E_Minimal_pred, v_minimal_pred = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = t) #set dt as length of prediction interval, in seconds\n",
        "          pred_act += math.floor(E_Minimal_pred/Minimal_energy()) #Update number of activations predicted\n",
        "          itn = 0\n",
        "          if math.floor(E_Minimal_pred/Minimal_energy()) > 0:\n",
        "              minimal_intervals = [date for date in group_util(v_list.index[jj], v_list.index[jj] + timedelta(seconds=t), math.floor(E_Minimal_pred/Minimal_energy()))]\n",
        "              #Calculate desired interval\n",
        "              int_len = time_frame_seconds /  math.floor(E_Minimal_pred/Minimal_energy())\n",
        "              for i in range(len(minimal_intervals) - 1):\n",
        "                  #Determine actual energy stored during scheduled sub-interval\n",
        "                  start = v_list_fine.index.searchsorted(minimal_intervals[i])\n",
        "                  end =  v_list_fine.index.searchsorted(minimal_intervals[i+1])\n",
        "\n",
        "                  E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list_fine.iloc[start:end]['V1 [mV]'].mean(), R=internal_R_v0(), C=C_h[0], dt = int_len)\n",
        "                  if not math.isnan(v_list_fine.iloc[start:end]['V1 [mV]'].mean()):\n",
        "                    if E_Minimal < Minimal_energy():\n",
        "                        false_act += 1\n",
        "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
        "                        itn += 1\n",
        "\n",
        "                    elif E_Minimal >= Minimal_energy():\n",
        "                        succ_act += 1\n",
        "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
        "                        itn+= 1\n",
        "\n",
        "                    else:\n",
        "                      print('Error')\n",
        "                      print(e_minimal_stored, v)\n",
        "\n",
        "                  #Unit test\n",
        "                  #else:\n",
        "                  #  print(\"?\")\n",
        "                  #  print(v_list_fine.index[start])\n",
        "                  #  print(v_list_fine.index[end])\n",
        "                  #  print(minimal_intervals[i], minimal_intervals[i+1])\n",
        "\n",
        "              #Unit test\n",
        "              #if itn != math.floor(E_Minimal_pred/Minimal_energy()):\n",
        "              #    print(\"itn not matching\")\n",
        "              #    print(itn, math.floor(E_Minimal_pred/Minimal_energy()))\n",
        "              #    continue\n",
        "\n",
        "          else:\n",
        "              e_minimal_stored, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "              #Added this\n",
        "              #start = v_list_fine.index.searchsorted(v_list.index[jj])\n",
        "              #end =  v_list_fine.index.searchsorted(v_list.index[jj+1])\n",
        "              #for h in range(start, end):\n",
        "              #    v = v_list_fine.iloc[h]['V1 [mV]']\n",
        "              #    interval_length = ((v_list_fine.index[h+1]) - (v_list_fine.index[h])).total_seconds()\n",
        "              #    E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = interval_length)\n",
        "              #    e_minimal_stored = E_Minimal\n",
        "\n",
        "\n",
        "        else:\n",
        "          print(\"It's over 9000!\", v_list.index[jj], v_list.index[jj+1])\n",
        "\n",
        "    print(\"Naive total_E activations:\", total_E/Minimal_energy())\n",
        "    print(\"Naive total_E_pred activations:\", total_E_naive/Minimal_energy())\n",
        "    return pred_act, false_act, succ_act, total_E_naive\n",
        "\n",
        "def getMax(c_list, input_list):\n",
        "    max_value = max(input_list)\n",
        "    i = [index for index, item in enumerate(input_list) if item == max_value][0]\n",
        "    return i, max_value, c_list[i]\n",
        "\n",
        "\n",
        "#SMFC\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "from scipy.signal import butter, lfilter\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "def butter_lowpass(cutoff, fs, order=5):\n",
        "        return butter(order, cutoff, fs=fs, btype='low', analog=False)\n",
        "\n",
        "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def getMFC_data(y_test, test_pred):\n",
        "    unix_time = y_test.index\n",
        "    d0 = unix_time[0]\n",
        "    days = []\n",
        "    for d in unix_time:\n",
        "        day = d\n",
        "        day_from_start = day-d0\n",
        "        decimal_day = day_from_start.total_seconds()/(24 * 3600)\n",
        "        days.append(decimal_day)\n",
        "\n",
        "    return days\n",
        "\n",
        "def simulate(t_list, v_list, v_list_pred, v_list_fine, C_h):\n",
        "    # t_list: list of decimal time stamps in unit of days (e.g. 71.85893518518519 day), same length as v_list\n",
        "    # v_list: list of voltage values from SFMC\n",
        "    # C_h: capacitance of the capacitor being filled up by harvester\n",
        "\n",
        "    #assume capacitor is completely discharged at start\n",
        "    e_minimal_stored = 0\n",
        "    e_minimal_stored_theo = 0\n",
        "\n",
        "    #Initialize evaluation metrics\n",
        "    false_act = 0\n",
        "    max_act = 0\n",
        "    pred_act = 0\n",
        "    succ_act = 0\n",
        "\n",
        "    total_E = 0\n",
        "    total_E_pred = 0\n",
        "\n",
        "    #Calculate maximum energy\n",
        "    #for i in range(len(v_list_fine) - 1):\n",
        "    #    t = (v_list_fine.index[i+1] - v_list_fine.index[i]).total_seconds()\n",
        "    #    total_E, ignore = update_capEnergy(total_E, V_applied=v_list_fine['V1 [10nV]'][i], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "    #print(total_E/Minimal_energy())\n",
        "    #for each voltage data point\n",
        "    for jj in range(len(v_list) - 1): #last data point was at 71.85893518518519 day\n",
        "        t = (v_list.index[jj+1] - v_list.index[jj]).total_seconds()\n",
        "        total_E, ignore = update_capEnergy(total_E, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "        total_E_pred, ignore = update_capEnergy(total_E_pred, V_applied=v_list_pred[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "        if t <= time_frame_seconds:\n",
        "          #Total predicted vs. actual energy stored\n",
        "          #Predict energy stored during scheduled sub-interval\n",
        "          E_Minimal_pred, v_minimal_pred = update_capEnergy(e_minimal_stored, V_applied=v_list_pred[jj], R=internal_R_v0(), C=C_h[0], dt = t) #set dt as length of prediction interval, in seconds\n",
        "          pred_act += math.floor(E_Minimal_pred/Minimal_energy()) #Update number of activations predicted\n",
        "          itn = 0\n",
        "          if math.floor(E_Minimal_pred/Minimal_energy()) > 0:\n",
        "              minimal_intervals = [date for date in group_util(v_list_pred.index[jj], v_list_pred.index[jj] + timedelta(seconds=t), math.floor(E_Minimal_pred/Minimal_energy()))]\n",
        "              #Calculate desired interval\n",
        "              int_len = time_frame_seconds /  math.floor(E_Minimal_pred/Minimal_energy())\n",
        "              for i in range(len(minimal_intervals) - 1):\n",
        "                  #Determine actual energy stored during scheduled sub-interval\n",
        "                  start = v_list_fine.index.searchsorted(minimal_intervals[i])\n",
        "                  end =  v_list_fine.index.searchsorted(minimal_intervals[i+1])\n",
        "                  v = v_list_fine.iloc[start:end]['V1 [mV]'].mean()\n",
        "\n",
        "                  #interval_length = ((v_list_fine.index[end]) - (v_list_fine.index[start])).total_seconds()\n",
        "                  #if interval_length > int_len:\n",
        "                  #  print('interval_length > int_len')\n",
        "                  #  print('interval_length, int_len:', interval_length, int_len)\n",
        "                  #  print(v_list_fine.index[start], v_list_fine.index[end])\n",
        "                  #else:\n",
        "                  #  print('interval_length <= int_len')\n",
        "                  #  print('interval_length, int_len:', interval_length, int_len)\n",
        "                  #  print(v_list_fine.index[start], v_list_fine.index[end])\n",
        "\n",
        "                  E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = int_len)\n",
        "                  if not math.isnan(v_list_fine.iloc[start:end]['V1 [mV]'].mean()):\n",
        "                    if E_Minimal < Minimal_energy():\n",
        "                        false_act += 1\n",
        "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
        "                        itn += 1\n",
        "\n",
        "                    elif E_Minimal >= Minimal_energy():\n",
        "                        succ_act += 1\n",
        "                        e_minimal_stored = max(0, E_Minimal - Minimal_energy())\n",
        "                        itn+= 1\n",
        "\n",
        "                    else:\n",
        "                      print('Error')\n",
        "                      print(e_minimal_stored, v)\n",
        "\n",
        "                  #Unit test\n",
        "                  #else:\n",
        "                  #  print(\"?\")\n",
        "                  #  print(v_list_fine.index[start])\n",
        "                  #  print(v_list_fine.index[end])\n",
        "                  #  print(minimal_intervals[i], minimal_intervals[i+1])\n",
        "\n",
        "              #Unit test\n",
        "              #if itn != math.floor(E_Minimal_pred/Minimal_energy()):\n",
        "              #    print(\"itn not matching\")\n",
        "              #    print(itn, math.floor(E_Minimal_pred/Minimal_energy()))\n",
        "              #    continue\n",
        "\n",
        "          else:\n",
        "              e_minimal_stored, ignore = update_capEnergy(e_minimal_stored, V_applied=v_list[jj], R=internal_R_v0(), C=C_h[0], dt = t)\n",
        "              #Added this\n",
        "              #start = v_list_fine.index.searchsorted(v_list.index[jj])\n",
        "              #end =  v_list_fine.index.searchsorted(v_list.index[jj+1])\n",
        "              #for h in range(start, end):\n",
        "              #    v = v_list_fine.iloc[h]['V1 [mV]']\n",
        "              #    interval_length = ((v_list_fine.index[h+1]) - (v_list_fine.index[h])).total_seconds()\n",
        "              #    E_Minimal, ignore = update_capEnergy(e_minimal_stored, V_applied=v, R=internal_R_v0(), C=C_h[0], dt = interval_length)\n",
        "              #    e_minimal_stored = E_Minimal\n",
        "\n",
        "\n",
        "        else:\n",
        "          print(\"It's over 9000!\", v_list.index[jj], v_list.index[jj+1])\n",
        "\n",
        "    print(\"Runtime total_E activations:\", total_E/Minimal_energy())\n",
        "    print(\"Runtime total_E_pred activations:\", total_E_pred/Minimal_energy())\n",
        "    return pred_act, false_act, succ_act, total_E, total_E_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSkfrkLBZ42n"
      },
      "source": [
        "## Specify Device so we can use GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8NhMTInXpyS"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX2yVusgZ_qU"
      },
      "source": [
        "## Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpNnJc0hxWjZ"
      },
      "outputs": [],
      "source": [
        "beta = 0.9\n",
        "\n",
        "# old design network\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "# model.add(Dense(3))\n",
        "# model.compile(loss=quantile_loss, metrics=['mape'], optimizer='adam')\n",
        "\n",
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_inputs, num_steps):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        num_hidden1 = 200\n",
        "\n",
        "        # layer 1\n",
        "        self.slstm1 = snn.SLSTM(num_inputs, num_hidden1, threshold = 0.25)\n",
        "\n",
        "        # layer 2\n",
        "        self.fc1 = torch.nn.Linear(in_features=num_hidden1, out_features=100)\n",
        "        self.lif1 = snn.Leaky(beta=beta, threshold = 0.5)\n",
        "\n",
        "        # randomly initialize decay rate for output neuron\n",
        "        beta_out = random.uniform(0.5, 1)\n",
        "\n",
        "        # layer 2\n",
        "        self.fc2 = torch.nn.Linear(in_features=100, out_features=3)\n",
        "        self.lif2 = snn.Leaky(beta=beta_out, learn_beta=True, reset_mechanism=\"none\")\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden states and outputs at t=0\n",
        "        syn1, mem1 = self.slstm1.reset_mem()\n",
        "        mem2 = self.lif1.reset_mem()\n",
        "        mem3 = self.lif2.reset_mem()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk1_rec = []\n",
        "        spk2_rec = []\n",
        "        spk3_rec = []\n",
        "        mem_rec = []\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            spk1, syn1, mem1 = self.slstm1(x.flatten(1), syn1, mem1)\n",
        "            spk2, mem2 = self.lif1(self.fc1(spk1), mem2)\n",
        "            spk3, mem3 = self.lif2(self.fc2(spk2), mem3)\n",
        "\n",
        "            # Append the Spike and Membrane History\n",
        "            spk1_rec.append(spk1)\n",
        "            spk2_rec.append(spk2)\n",
        "            spk3_rec.append(spk3)\n",
        "            mem_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec), torch.stack(mem_rec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMJdiecpR7VB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0CgXMuciHz2"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = train_test_split(pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1), test_size=0.3, shuffle=False)\n",
        "y_train, y_test = train_test_split(pd.concat([df['Power (uW)'], df['Voltage (mV)'], df['Current (uA)']], axis = 1), test_size=0.3, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQZbzMOZiD28"
      },
      "outputs": [],
      "source": [
        "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-jZOjtUmSME"
      },
      "outputs": [],
      "source": [
        "#All Data (For Type 1 and 2 Models)\n",
        "X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "\n",
        "#Electricity Data Omitted (For Type 1A and 2A Models)\n",
        "#X = pd.concat([df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "\n",
        "#Environmental Data Omitted (For Type 1B and 2B Models)\n",
        "#X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"]], axis = 1)\n",
        "\n",
        "y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zd1y-AXaHJR"
      },
      "source": [
        "## Train Loop (Normalized Input and Output Data Version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FadItqzcypID",
        "outputId": "876ef36b-fdbf-4e93-f820-60b1ddf414e1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30min\n",
            "Epoch 0, Iteration 0 Train Loss: 11593.10\n",
            "Epoch 0, Iteration 10 Train Loss: 449.30\n",
            "Epoch 0, Iteration 20 Train Loss: 889.48\n",
            "Epoch 0, Iteration 30 Train Loss: 812.61\n",
            "Epoch 0, Iteration 40 Train Loss: 856.81\n",
            "Epoch 0, Iteration 50 Train Loss: 626.36\n",
            "Epoch 0, Iteration 60 Train Loss: 1433.57\n",
            "Epoch 0, Iteration 70 Train Loss: 987.15\n",
            "Epoch 0, Iteration 80 Train Loss: 879.82\n",
            "Epoch 0, Iteration 90 Train Loss: 1104.29\n",
            "Epoch 0, Iteration 100 Train Loss: 738.55\n",
            "Epoch 0, Iteration 110 Train Loss: 794.04\n",
            "Epoch 0, Iteration 120 Train Loss: 1142.81\n",
            "Epoch 0, Iteration 130 Train Loss: 933.89\n",
            "Epoch 0, Iteration 140 Train Loss: 920.51\n",
            "Epoch 0, Iteration 150 Train Loss: 805.13\n",
            "Epoch 0, Iteration 160 Train Loss: 734.59\n",
            "Epoch 0, Iteration 170 Train Loss: 709.87\n",
            "Epoch 0, Iteration 180 Train Loss: 740.88\n",
            "Epoch 0, Iteration 190 Train Loss: 917.56\n",
            "Epoch 0, Iteration 200 Train Loss: 742.81\n",
            "Epoch 0, Iteration 210 Train Loss: 613.82\n",
            "Epoch 0, Iteration 220 Train Loss: 462.39\n",
            "Epoch 0, Iteration 230 Train Loss: 368.13\n",
            "Epoch 0, Iteration 240 Train Loss: 363.27\n",
            "Epoch 0, Iteration 250 Train Loss: 402.32\n",
            "New Epoch! Avg loss for the last 100 iterations: 636.5674829101563\n",
            "Epoch 1, Iteration 0 Train Loss: 11119.80\n",
            "Epoch 1, Iteration 10 Train Loss: 205.55\n",
            "Epoch 1, Iteration 20 Train Loss: 417.59\n",
            "Epoch 1, Iteration 30 Train Loss: 300.13\n",
            "Epoch 1, Iteration 40 Train Loss: 331.84\n",
            "Epoch 1, Iteration 50 Train Loss: 254.13\n",
            "Epoch 1, Iteration 60 Train Loss: 1032.66\n",
            "Epoch 1, Iteration 70 Train Loss: 491.77\n",
            "Epoch 1, Iteration 80 Train Loss: 319.59\n",
            "Epoch 1, Iteration 90 Train Loss: 500.11\n",
            "Epoch 1, Iteration 100 Train Loss: 198.07\n",
            "Epoch 1, Iteration 110 Train Loss: 248.08\n",
            "Epoch 1, Iteration 120 Train Loss: 538.90\n",
            "Epoch 1, Iteration 130 Train Loss: 311.76\n",
            "Epoch 1, Iteration 140 Train Loss: 263.04\n",
            "Epoch 1, Iteration 150 Train Loss: 217.38\n",
            "Epoch 1, Iteration 160 Train Loss: 291.93\n",
            "Epoch 1, Iteration 170 Train Loss: 156.95\n",
            "Epoch 1, Iteration 180 Train Loss: 107.17\n",
            "Epoch 1, Iteration 190 Train Loss: 250.56\n",
            "Epoch 1, Iteration 200 Train Loss: 275.97\n",
            "Epoch 1, Iteration 210 Train Loss: 387.94\n",
            "Epoch 1, Iteration 220 Train Loss: 312.55\n",
            "Epoch 1, Iteration 230 Train Loss: 218.94\n",
            "Epoch 1, Iteration 240 Train Loss: 231.43\n",
            "Epoch 1, Iteration 250 Train Loss: 178.41\n",
            "New Epoch! Avg loss for the last 100 iterations: 234.8966626739502\n",
            "Epoch 2, Iteration 0 Train Loss: 10867.05\n",
            "Epoch 2, Iteration 10 Train Loss: 305.39\n",
            "Epoch 2, Iteration 20 Train Loss: 334.14\n",
            "Epoch 2, Iteration 30 Train Loss: 114.31\n",
            "Epoch 2, Iteration 40 Train Loss: 170.91\n",
            "Epoch 2, Iteration 50 Train Loss: 282.02\n",
            "Epoch 2, Iteration 60 Train Loss: 1067.23\n",
            "Epoch 2, Iteration 70 Train Loss: 465.24\n",
            "Epoch 2, Iteration 80 Train Loss: 233.54\n",
            "Epoch 2, Iteration 90 Train Loss: 337.52\n",
            "Epoch 2, Iteration 100 Train Loss: 125.06\n",
            "Epoch 2, Iteration 110 Train Loss: 200.98\n",
            "Epoch 2, Iteration 120 Train Loss: 485.98\n",
            "Epoch 2, Iteration 130 Train Loss: 249.80\n",
            "Epoch 2, Iteration 140 Train Loss: 158.70\n",
            "Epoch 2, Iteration 150 Train Loss: 193.58\n",
            "Epoch 2, Iteration 160 Train Loss: 282.44\n",
            "Epoch 2, Iteration 170 Train Loss: 177.15\n",
            "Epoch 2, Iteration 180 Train Loss: 81.14\n",
            "Epoch 2, Iteration 190 Train Loss: 158.58\n",
            "Epoch 2, Iteration 200 Train Loss: 268.26\n",
            "Epoch 2, Iteration 210 Train Loss: 378.25\n",
            "Epoch 2, Iteration 220 Train Loss: 331.69\n",
            "Epoch 2, Iteration 230 Train Loss: 250.90\n",
            "Epoch 2, Iteration 240 Train Loss: 239.37\n",
            "Epoch 2, Iteration 250 Train Loss: 177.38\n",
            "New Epoch! Avg loss for the last 100 iterations: 213.02601585388183\n",
            "Epoch 3, Iteration 0 Train Loss: 10793.16\n",
            "Epoch 3, Iteration 10 Train Loss: 355.80\n",
            "Epoch 3, Iteration 20 Train Loss: 318.14\n",
            "Epoch 3, Iteration 30 Train Loss: 95.70\n",
            "Epoch 3, Iteration 40 Train Loss: 143.37\n",
            "Epoch 3, Iteration 50 Train Loss: 295.88\n",
            "Epoch 3, Iteration 60 Train Loss: 1066.64\n",
            "Epoch 3, Iteration 70 Train Loss: 462.94\n",
            "Epoch 3, Iteration 80 Train Loss: 230.66\n",
            "Epoch 3, Iteration 90 Train Loss: 305.34\n",
            "Epoch 3, Iteration 100 Train Loss: 148.98\n",
            "Epoch 3, Iteration 110 Train Loss: 198.63\n",
            "Epoch 3, Iteration 120 Train Loss: 479.02\n",
            "Epoch 3, Iteration 130 Train Loss: 234.30\n",
            "Epoch 3, Iteration 140 Train Loss: 138.71\n",
            "Epoch 3, Iteration 150 Train Loss: 196.89\n",
            "Epoch 3, Iteration 160 Train Loss: 286.54\n",
            "Epoch 3, Iteration 170 Train Loss: 193.44\n",
            "Epoch 3, Iteration 180 Train Loss: 90.90\n",
            "Epoch 3, Iteration 190 Train Loss: 146.66\n",
            "Epoch 3, Iteration 200 Train Loss: 270.14\n",
            "Epoch 3, Iteration 210 Train Loss: 380.04\n",
            "Epoch 3, Iteration 220 Train Loss: 341.17\n",
            "Epoch 3, Iteration 230 Train Loss: 262.75\n",
            "Epoch 3, Iteration 240 Train Loss: 246.67\n",
            "Epoch 3, Iteration 250 Train Loss: 182.29\n",
            "New Epoch! Avg loss for the last 100 iterations: 214.55554382324217\n",
            "Epoch 4, Iteration 0 Train Loss: 10776.40\n",
            "Epoch 4, Iteration 10 Train Loss: 370.35\n",
            "Epoch 4, Iteration 20 Train Loss: 311.18\n",
            "Epoch 4, Iteration 30 Train Loss: 94.67\n",
            "Epoch 4, Iteration 40 Train Loss: 135.80\n",
            "Epoch 4, Iteration 50 Train Loss: 298.43\n",
            "Epoch 4, Iteration 60 Train Loss: 1063.11\n",
            "Epoch 4, Iteration 70 Train Loss: 460.37\n",
            "Epoch 4, Iteration 80 Train Loss: 228.64\n",
            "Epoch 4, Iteration 90 Train Loss: 296.75\n",
            "Epoch 4, Iteration 100 Train Loss: 157.19\n",
            "Epoch 4, Iteration 110 Train Loss: 198.57\n",
            "Epoch 4, Iteration 120 Train Loss: 476.18\n",
            "Epoch 4, Iteration 130 Train Loss: 228.82\n",
            "Epoch 4, Iteration 140 Train Loss: 133.91\n",
            "Epoch 4, Iteration 150 Train Loss: 199.48\n",
            "Epoch 4, Iteration 160 Train Loss: 289.20\n",
            "Epoch 4, Iteration 170 Train Loss: 199.22\n",
            "Epoch 4, Iteration 180 Train Loss: 96.18\n",
            "Epoch 4, Iteration 190 Train Loss: 144.51\n",
            "Epoch 4, Iteration 200 Train Loss: 273.20\n",
            "Epoch 4, Iteration 210 Train Loss: 383.24\n",
            "Epoch 4, Iteration 220 Train Loss: 346.24\n",
            "Epoch 4, Iteration 230 Train Loss: 268.61\n",
            "Epoch 4, Iteration 240 Train Loss: 251.63\n",
            "Epoch 4, Iteration 250 Train Loss: 186.75\n",
            "New Epoch! Avg loss for the last 100 iterations: 217.03494972229004\n",
            "Epoch 5, Iteration 0 Train Loss: 10769.64\n",
            "Epoch 5, Iteration 10 Train Loss: 376.35\n",
            "Epoch 5, Iteration 20 Train Loss: 306.24\n",
            "Epoch 5, Iteration 30 Train Loss: 94.41\n",
            "Epoch 5, Iteration 40 Train Loss: 131.56\n",
            "Epoch 5, Iteration 50 Train Loss: 298.71\n",
            "Epoch 5, Iteration 60 Train Loss: 1059.91\n",
            "Epoch 5, Iteration 70 Train Loss: 457.79\n",
            "Epoch 5, Iteration 80 Train Loss: 226.61\n",
            "Epoch 5, Iteration 90 Train Loss: 292.74\n",
            "Epoch 5, Iteration 100 Train Loss: 160.82\n",
            "Epoch 5, Iteration 110 Train Loss: 198.27\n",
            "Epoch 5, Iteration 120 Train Loss: 474.17\n",
            "Epoch 5, Iteration 130 Train Loss: 225.77\n",
            "Epoch 5, Iteration 140 Train Loss: 131.79\n",
            "Epoch 5, Iteration 150 Train Loss: 201.55\n",
            "Epoch 5, Iteration 160 Train Loss: 291.20\n",
            "Epoch 5, Iteration 170 Train Loss: 202.24\n",
            "Epoch 5, Iteration 180 Train Loss: 99.02\n",
            "Epoch 5, Iteration 190 Train Loss: 143.85\n",
            "Epoch 5, Iteration 200 Train Loss: 275.49\n",
            "Epoch 5, Iteration 210 Train Loss: 385.60\n",
            "Epoch 5, Iteration 220 Train Loss: 349.07\n",
            "Epoch 5, Iteration 230 Train Loss: 271.72\n",
            "Epoch 5, Iteration 240 Train Loss: 254.69\n",
            "Epoch 5, Iteration 250 Train Loss: 189.58\n",
            "New Epoch! Avg loss for the last 100 iterations: 218.7573095703125\n",
            "Epoch 6, Iteration 0 Train Loss: 10766.28\n",
            "Epoch 6, Iteration 10 Train Loss: 379.60\n",
            "Epoch 6, Iteration 20 Train Loss: 303.19\n",
            "Epoch 6, Iteration 30 Train Loss: 94.30\n",
            "Epoch 6, Iteration 40 Train Loss: 129.31\n",
            "Epoch 6, Iteration 50 Train Loss: 298.59\n",
            "Epoch 6, Iteration 60 Train Loss: 1057.47\n",
            "Epoch 6, Iteration 70 Train Loss: 455.75\n",
            "Epoch 6, Iteration 80 Train Loss: 224.97\n",
            "Epoch 6, Iteration 90 Train Loss: 290.20\n",
            "Epoch 6, Iteration 100 Train Loss: 163.15\n",
            "Epoch 6, Iteration 110 Train Loss: 198.15\n",
            "Epoch 6, Iteration 120 Train Loss: 472.53\n",
            "Epoch 6, Iteration 130 Train Loss: 223.67\n",
            "Epoch 6, Iteration 140 Train Loss: 130.40\n",
            "Epoch 6, Iteration 150 Train Loss: 203.14\n",
            "Epoch 6, Iteration 160 Train Loss: 292.76\n",
            "Epoch 6, Iteration 170 Train Loss: 204.32\n",
            "Epoch 6, Iteration 180 Train Loss: 101.05\n",
            "Epoch 6, Iteration 190 Train Loss: 143.60\n",
            "Epoch 6, Iteration 200 Train Loss: 277.47\n",
            "Epoch 6, Iteration 210 Train Loss: 387.56\n",
            "Epoch 6, Iteration 220 Train Loss: 350.99\n",
            "Epoch 6, Iteration 230 Train Loss: 273.67\n",
            "Epoch 6, Iteration 240 Train Loss: 256.81\n",
            "Epoch 6, Iteration 250 Train Loss: 191.67\n",
            "New Epoch! Avg loss for the last 100 iterations: 220.08151512145997\n",
            "Epoch 7, Iteration 0 Train Loss: 10764.14\n",
            "Epoch 7, Iteration 10 Train Loss: 381.68\n",
            "Epoch 7, Iteration 20 Train Loss: 301.03\n",
            "Epoch 7, Iteration 30 Train Loss: 94.27\n",
            "Epoch 7, Iteration 40 Train Loss: 127.75\n",
            "Epoch 7, Iteration 50 Train Loss: 298.39\n",
            "Epoch 7, Iteration 60 Train Loss: 1055.78\n",
            "Epoch 7, Iteration 70 Train Loss: 454.30\n",
            "Epoch 7, Iteration 80 Train Loss: 223.78\n",
            "Epoch 7, Iteration 90 Train Loss: 288.63\n",
            "Epoch 7, Iteration 100 Train Loss: 164.50\n",
            "Epoch 7, Iteration 110 Train Loss: 198.00\n",
            "Epoch 7, Iteration 120 Train Loss: 471.59\n",
            "Epoch 7, Iteration 130 Train Loss: 222.39\n",
            "Epoch 7, Iteration 140 Train Loss: 129.60\n",
            "Epoch 7, Iteration 150 Train Loss: 203.93\n",
            "Epoch 7, Iteration 160 Train Loss: 293.50\n",
            "Epoch 7, Iteration 170 Train Loss: 205.34\n",
            "Epoch 7, Iteration 180 Train Loss: 101.99\n",
            "Epoch 7, Iteration 190 Train Loss: 143.53\n",
            "Epoch 7, Iteration 200 Train Loss: 278.22\n",
            "Epoch 7, Iteration 210 Train Loss: 388.34\n",
            "Epoch 7, Iteration 220 Train Loss: 351.87\n",
            "Epoch 7, Iteration 230 Train Loss: 274.64\n",
            "Epoch 7, Iteration 240 Train Loss: 257.83\n",
            "Epoch 7, Iteration 250 Train Loss: 192.60\n",
            "New Epoch! Avg loss for the last 100 iterations: 220.68374908447265\n",
            "Epoch 8, Iteration 0 Train Loss: 10763.07\n",
            "Epoch 8, Iteration 10 Train Loss: 382.71\n",
            "Epoch 8, Iteration 20 Train Loss: 299.99\n",
            "Epoch 8, Iteration 30 Train Loss: 94.26\n",
            "Epoch 8, Iteration 40 Train Loss: 127.10\n",
            "Epoch 8, Iteration 50 Train Loss: 298.30\n",
            "Epoch 8, Iteration 60 Train Loss: 1055.14\n",
            "Epoch 8, Iteration 70 Train Loss: 453.75\n",
            "Epoch 8, Iteration 80 Train Loss: 223.35\n",
            "Epoch 8, Iteration 90 Train Loss: 288.04\n",
            "Epoch 8, Iteration 100 Train Loss: 165.01\n",
            "Epoch 8, Iteration 110 Train Loss: 197.90\n",
            "Epoch 8, Iteration 120 Train Loss: 471.17\n",
            "Epoch 8, Iteration 130 Train Loss: 221.83\n",
            "Epoch 8, Iteration 140 Train Loss: 129.25\n",
            "Epoch 8, Iteration 150 Train Loss: 204.32\n",
            "Epoch 8, Iteration 160 Train Loss: 293.86\n",
            "Epoch 8, Iteration 170 Train Loss: 205.83\n",
            "Epoch 8, Iteration 180 Train Loss: 102.47\n",
            "Epoch 8, Iteration 190 Train Loss: 143.52\n",
            "Epoch 8, Iteration 200 Train Loss: 278.76\n",
            "Epoch 8, Iteration 210 Train Loss: 388.92\n",
            "Epoch 8, Iteration 220 Train Loss: 352.40\n",
            "Epoch 8, Iteration 230 Train Loss: 275.16\n",
            "Epoch 8, Iteration 240 Train Loss: 258.52\n",
            "Epoch 8, Iteration 250 Train Loss: 193.24\n",
            "New Epoch! Avg loss for the last 100 iterations: 221.07853088378906\n",
            "Epoch 9, Iteration 0 Train Loss: 10762.52\n",
            "Epoch 9, Iteration 10 Train Loss: 383.24\n",
            "Epoch 9, Iteration 20 Train Loss: 299.27\n",
            "Epoch 9, Iteration 30 Train Loss: 94.15\n",
            "Epoch 9, Iteration 40 Train Loss: 126.60\n",
            "Epoch 9, Iteration 50 Train Loss: 298.15\n",
            "Epoch 9, Iteration 60 Train Loss: 1054.60\n",
            "Epoch 9, Iteration 70 Train Loss: 453.28\n",
            "Epoch 9, Iteration 80 Train Loss: 222.97\n",
            "Epoch 9, Iteration 90 Train Loss: 287.77\n",
            "Epoch 9, Iteration 100 Train Loss: 165.22\n",
            "Epoch 9, Iteration 110 Train Loss: 197.76\n",
            "Epoch 9, Iteration 120 Train Loss: 470.93\n",
            "Epoch 9, Iteration 130 Train Loss: 221.49\n",
            "Epoch 9, Iteration 140 Train Loss: 129.05\n",
            "Epoch 9, Iteration 150 Train Loss: 204.67\n",
            "Epoch 9, Iteration 160 Train Loss: 294.16\n",
            "Epoch 9, Iteration 170 Train Loss: 206.13\n",
            "Epoch 9, Iteration 180 Train Loss: 102.76\n",
            "Epoch 9, Iteration 190 Train Loss: 143.49\n",
            "Epoch 9, Iteration 200 Train Loss: 279.09\n",
            "Epoch 9, Iteration 210 Train Loss: 389.25\n",
            "Epoch 9, Iteration 220 Train Loss: 352.69\n",
            "Epoch 9, Iteration 230 Train Loss: 275.47\n",
            "Epoch 9, Iteration 240 Train Loss: 258.91\n",
            "Epoch 9, Iteration 250 Train Loss: 193.63\n",
            "New Epoch! Avg loss for the last 100 iterations: 221.31230682373047\n",
            "Epoch 10, Iteration 0 Train Loss: 10762.17\n",
            "Epoch 10, Iteration 10 Train Loss: 383.58\n",
            "Epoch 10, Iteration 20 Train Loss: 298.82\n",
            "Epoch 10, Iteration 30 Train Loss: 94.09\n",
            "Epoch 10, Iteration 40 Train Loss: 126.27\n",
            "Epoch 10, Iteration 50 Train Loss: 298.13\n",
            "Epoch 10, Iteration 60 Train Loss: 1054.25\n",
            "Epoch 10, Iteration 70 Train Loss: 453.01\n",
            "Epoch 10, Iteration 80 Train Loss: 222.76\n",
            "Epoch 10, Iteration 90 Train Loss: 287.95\n",
            "Epoch 10, Iteration 100 Train Loss: 165.34\n",
            "Epoch 10, Iteration 110 Train Loss: 197.00\n",
            "Epoch 10, Iteration 120 Train Loss: 468.15\n",
            "Epoch 10, Iteration 130 Train Loss: 219.24\n",
            "Epoch 10, Iteration 140 Train Loss: 127.71\n",
            "Epoch 10, Iteration 150 Train Loss: 208.33\n",
            "Epoch 10, Iteration 160 Train Loss: 296.84\n",
            "Epoch 10, Iteration 170 Train Loss: 207.47\n",
            "Epoch 10, Iteration 180 Train Loss: 104.18\n",
            "Epoch 10, Iteration 190 Train Loss: 144.42\n",
            "Epoch 10, Iteration 200 Train Loss: 280.50\n",
            "Epoch 10, Iteration 210 Train Loss: 390.33\n",
            "Epoch 10, Iteration 220 Train Loss: 353.57\n",
            "Epoch 10, Iteration 230 Train Loss: 276.13\n",
            "Epoch 10, Iteration 240 Train Loss: 258.74\n",
            "Epoch 10, Iteration 250 Train Loss: 193.55\n",
            "New Epoch! Avg loss for the last 100 iterations: 222.010447845459\n",
            "Epoch 11, Iteration 0 Train Loss: 10761.89\n",
            "Epoch 11, Iteration 10 Train Loss: 383.90\n",
            "Epoch 11, Iteration 20 Train Loss: 299.53\n",
            "Epoch 11, Iteration 30 Train Loss: 94.46\n",
            "Epoch 11, Iteration 40 Train Loss: 126.56\n",
            "Epoch 11, Iteration 50 Train Loss: 298.32\n",
            "Epoch 11, Iteration 60 Train Loss: 1054.18\n",
            "Epoch 11, Iteration 70 Train Loss: 452.67\n",
            "Epoch 11, Iteration 80 Train Loss: 222.09\n",
            "Epoch 11, Iteration 90 Train Loss: 286.85\n",
            "Epoch 11, Iteration 100 Train Loss: 166.06\n",
            "Epoch 11, Iteration 110 Train Loss: 197.62\n",
            "Epoch 11, Iteration 120 Train Loss: 468.82\n",
            "Epoch 11, Iteration 130 Train Loss: 219.92\n",
            "Epoch 11, Iteration 140 Train Loss: 127.87\n",
            "Epoch 11, Iteration 150 Train Loss: 206.70\n",
            "Epoch 11, Iteration 160 Train Loss: 295.65\n",
            "Epoch 11, Iteration 170 Train Loss: 206.22\n",
            "Epoch 11, Iteration 180 Train Loss: 103.75\n",
            "Epoch 11, Iteration 190 Train Loss: 143.74\n",
            "Epoch 11, Iteration 200 Train Loss: 279.87\n",
            "Epoch 11, Iteration 210 Train Loss: 388.13\n",
            "Epoch 11, Iteration 220 Train Loss: 352.28\n",
            "Epoch 11, Iteration 230 Train Loss: 275.35\n",
            "Epoch 11, Iteration 240 Train Loss: 258.34\n",
            "Epoch 11, Iteration 250 Train Loss: 192.81\n",
            "New Epoch! Avg loss for the last 100 iterations: 221.2200947570801\n",
            "Epoch 12, Iteration 0 Train Loss: 10762.09\n",
            "Epoch 12, Iteration 10 Train Loss: 377.34\n",
            "Epoch 12, Iteration 20 Train Loss: 296.58\n",
            "Epoch 12, Iteration 30 Train Loss: 89.97\n",
            "Epoch 12, Iteration 40 Train Loss: 141.31\n",
            "Epoch 12, Iteration 50 Train Loss: 234.18\n",
            "Epoch 12, Iteration 60 Train Loss: 1039.37\n",
            "Epoch 12, Iteration 70 Train Loss: 441.73\n",
            "Epoch 12, Iteration 80 Train Loss: 195.17\n",
            "Epoch 12, Iteration 90 Train Loss: 295.35\n",
            "Epoch 12, Iteration 100 Train Loss: 146.90\n",
            "Epoch 12, Iteration 110 Train Loss: 182.96\n",
            "Epoch 12, Iteration 120 Train Loss: 443.10\n",
            "Epoch 12, Iteration 130 Train Loss: 204.85\n",
            "Epoch 12, Iteration 140 Train Loss: 117.45\n",
            "Epoch 12, Iteration 150 Train Loss: 210.32\n",
            "Epoch 12, Iteration 160 Train Loss: 285.15\n",
            "Epoch 12, Iteration 170 Train Loss: 186.43\n",
            "Epoch 12, Iteration 180 Train Loss: 104.82\n",
            "Epoch 12, Iteration 190 Train Loss: 131.13\n",
            "Epoch 12, Iteration 200 Train Loss: 259.43\n",
            "Epoch 12, Iteration 210 Train Loss: 368.47\n",
            "Epoch 12, Iteration 220 Train Loss: 352.25\n",
            "Epoch 12, Iteration 230 Train Loss: 278.16\n",
            "Epoch 12, Iteration 240 Train Loss: 227.56\n",
            "Epoch 12, Iteration 250 Train Loss: 158.84\n",
            "New Epoch! Avg loss for the last 100 iterations: 206.51063194274903\n",
            "Epoch 13, Iteration 0 Train Loss: 10753.40\n",
            "Epoch 13, Iteration 10 Train Loss: 334.48\n",
            "Epoch 13, Iteration 20 Train Loss: 322.70\n",
            "Epoch 13, Iteration 30 Train Loss: 94.86\n",
            "Epoch 13, Iteration 40 Train Loss: 126.84\n",
            "Epoch 13, Iteration 50 Train Loss: 219.38\n",
            "Epoch 13, Iteration 60 Train Loss: 1047.30\n",
            "Epoch 13, Iteration 70 Train Loss: 412.49\n",
            "Epoch 13, Iteration 80 Train Loss: 188.47\n",
            "Epoch 13, Iteration 90 Train Loss: 268.31\n",
            "Epoch 13, Iteration 100 Train Loss: 161.39\n",
            "Epoch 13, Iteration 110 Train Loss: 177.44\n",
            "Epoch 13, Iteration 120 Train Loss: 436.21\n",
            "Epoch 13, Iteration 130 Train Loss: 199.94\n",
            "Epoch 13, Iteration 140 Train Loss: 108.61\n",
            "Epoch 13, Iteration 150 Train Loss: 203.74\n",
            "Epoch 13, Iteration 160 Train Loss: 288.46\n",
            "Epoch 13, Iteration 170 Train Loss: 203.57\n",
            "Epoch 13, Iteration 180 Train Loss: 103.65\n",
            "Epoch 13, Iteration 190 Train Loss: 129.80\n",
            "Epoch 13, Iteration 200 Train Loss: 237.69\n",
            "Epoch 13, Iteration 210 Train Loss: 315.02\n",
            "Epoch 13, Iteration 220 Train Loss: 297.96\n",
            "Epoch 13, Iteration 230 Train Loss: 223.67\n",
            "Epoch 13, Iteration 240 Train Loss: 169.55\n",
            "Epoch 13, Iteration 250 Train Loss: 123.15\n",
            "New Epoch! Avg loss for the last 100 iterations: 181.87137313842774\n",
            "Epoch 14, Iteration 0 Train Loss: 10739.90\n",
            "Epoch 14, Iteration 10 Train Loss: 336.05\n",
            "Epoch 14, Iteration 20 Train Loss: 262.83\n",
            "Epoch 14, Iteration 30 Train Loss: 74.99\n",
            "Epoch 14, Iteration 40 Train Loss: 116.88\n",
            "Epoch 14, Iteration 50 Train Loss: 156.91\n",
            "Epoch 14, Iteration 60 Train Loss: 982.57\n",
            "Epoch 14, Iteration 70 Train Loss: 385.45\n",
            "Epoch 14, Iteration 80 Train Loss: 162.01\n",
            "Epoch 14, Iteration 90 Train Loss: 247.99\n",
            "Epoch 14, Iteration 100 Train Loss: 179.68\n",
            "Epoch 14, Iteration 110 Train Loss: 194.69\n",
            "Epoch 14, Iteration 120 Train Loss: 405.78\n",
            "Epoch 14, Iteration 130 Train Loss: 170.75\n",
            "Epoch 14, Iteration 140 Train Loss: 101.60\n",
            "Epoch 14, Iteration 150 Train Loss: 189.62\n",
            "Epoch 14, Iteration 160 Train Loss: 170.91\n",
            "Epoch 14, Iteration 170 Train Loss: 150.55\n",
            "Epoch 14, Iteration 180 Train Loss: 85.63\n",
            "Epoch 14, Iteration 190 Train Loss: 149.16\n",
            "Epoch 14, Iteration 200 Train Loss: 164.38\n",
            "Epoch 14, Iteration 210 Train Loss: 283.05\n",
            "Epoch 14, Iteration 220 Train Loss: 299.88\n",
            "Epoch 14, Iteration 230 Train Loss: 228.74\n",
            "Epoch 14, Iteration 240 Train Loss: 165.89\n",
            "Epoch 14, Iteration 250 Train Loss: 102.95\n",
            "New Epoch! Avg loss for the last 100 iterations: 168.7842427444458\n",
            "Epoch 15, Iteration 0 Train Loss: 10701.86\n",
            "Epoch 15, Iteration 10 Train Loss: 273.38\n",
            "Epoch 15, Iteration 20 Train Loss: 213.84\n",
            "Epoch 15, Iteration 30 Train Loss: 100.13\n",
            "Epoch 15, Iteration 40 Train Loss: 100.16\n",
            "Epoch 15, Iteration 50 Train Loss: 143.71\n",
            "Epoch 15, Iteration 60 Train Loss: 935.94\n",
            "Epoch 15, Iteration 70 Train Loss: 357.68\n",
            "Epoch 15, Iteration 80 Train Loss: 118.87\n",
            "Epoch 15, Iteration 90 Train Loss: 210.50\n",
            "Epoch 15, Iteration 100 Train Loss: 210.59\n",
            "Epoch 15, Iteration 110 Train Loss: 197.87\n",
            "Epoch 15, Iteration 120 Train Loss: 368.75\n",
            "Epoch 15, Iteration 130 Train Loss: 151.35\n",
            "Epoch 15, Iteration 140 Train Loss: 87.32\n",
            "Epoch 15, Iteration 150 Train Loss: 125.44\n",
            "Epoch 15, Iteration 160 Train Loss: 162.96\n",
            "Epoch 15, Iteration 170 Train Loss: 150.09\n",
            "Epoch 15, Iteration 180 Train Loss: 96.43\n",
            "Epoch 15, Iteration 190 Train Loss: 141.75\n",
            "Epoch 15, Iteration 200 Train Loss: 152.97\n",
            "Epoch 15, Iteration 210 Train Loss: 273.84\n",
            "Epoch 15, Iteration 220 Train Loss: 301.19\n",
            "Epoch 15, Iteration 230 Train Loss: 248.66\n",
            "Epoch 15, Iteration 240 Train Loss: 178.11\n",
            "Epoch 15, Iteration 250 Train Loss: 102.59\n",
            "New Epoch! Avg loss for the last 100 iterations: 170.70833316802978\n",
            "Epoch 16, Iteration 0 Train Loss: 10679.02\n",
            "Epoch 16, Iteration 10 Train Loss: 254.09\n",
            "Epoch 16, Iteration 20 Train Loss: 190.91\n",
            "Epoch 16, Iteration 30 Train Loss: 64.79\n",
            "Epoch 16, Iteration 40 Train Loss: 138.88\n",
            "Epoch 16, Iteration 50 Train Loss: 164.18\n",
            "Epoch 16, Iteration 60 Train Loss: 896.76\n",
            "Epoch 16, Iteration 70 Train Loss: 317.23\n",
            "Epoch 16, Iteration 80 Train Loss: 90.87\n",
            "Epoch 16, Iteration 90 Train Loss: 189.13\n",
            "Epoch 16, Iteration 100 Train Loss: 230.43\n",
            "Epoch 16, Iteration 110 Train Loss: 206.16\n",
            "Epoch 16, Iteration 120 Train Loss: 343.36\n",
            "Epoch 16, Iteration 130 Train Loss: 123.78\n",
            "Epoch 16, Iteration 140 Train Loss: 86.64\n",
            "Epoch 16, Iteration 150 Train Loss: 105.80\n",
            "Epoch 16, Iteration 160 Train Loss: 156.83\n",
            "Epoch 16, Iteration 170 Train Loss: 152.59\n",
            "Epoch 16, Iteration 180 Train Loss: 86.21\n",
            "Epoch 16, Iteration 190 Train Loss: 130.40\n",
            "Epoch 16, Iteration 200 Train Loss: 158.54\n",
            "Epoch 16, Iteration 210 Train Loss: 262.29\n",
            "Epoch 16, Iteration 220 Train Loss: 284.34\n",
            "Epoch 16, Iteration 230 Train Loss: 232.18\n",
            "Epoch 16, Iteration 240 Train Loss: 156.28\n",
            "Epoch 16, Iteration 250 Train Loss: 97.26\n",
            "New Epoch! Avg loss for the last 100 iterations: 160.2342518234253\n",
            "Epoch 17, Iteration 0 Train Loss: 10661.12\n",
            "Epoch 17, Iteration 10 Train Loss: 267.69\n",
            "Epoch 17, Iteration 20 Train Loss: 150.91\n",
            "Epoch 17, Iteration 30 Train Loss: 69.88\n",
            "Epoch 17, Iteration 40 Train Loss: 131.60\n",
            "Epoch 17, Iteration 50 Train Loss: 143.60\n",
            "Epoch 17, Iteration 60 Train Loss: 875.95\n",
            "Epoch 17, Iteration 70 Train Loss: 304.42\n",
            "Epoch 17, Iteration 80 Train Loss: 59.41\n",
            "Epoch 17, Iteration 90 Train Loss: 173.24\n",
            "Epoch 17, Iteration 100 Train Loss: 243.70\n",
            "Epoch 17, Iteration 110 Train Loss: 209.81\n",
            "Epoch 17, Iteration 120 Train Loss: 328.78\n",
            "Epoch 17, Iteration 130 Train Loss: 112.37\n",
            "Epoch 17, Iteration 140 Train Loss: 109.16\n",
            "Epoch 17, Iteration 150 Train Loss: 66.51\n",
            "Epoch 17, Iteration 160 Train Loss: 130.09\n",
            "Epoch 17, Iteration 170 Train Loss: 135.59\n",
            "Epoch 17, Iteration 180 Train Loss: 102.19\n",
            "Epoch 17, Iteration 190 Train Loss: 102.04\n",
            "Epoch 17, Iteration 200 Train Loss: 131.28\n",
            "Epoch 17, Iteration 210 Train Loss: 233.32\n",
            "Epoch 17, Iteration 220 Train Loss: 249.38\n",
            "Epoch 17, Iteration 230 Train Loss: 194.62\n",
            "Epoch 17, Iteration 240 Train Loss: 125.67\n",
            "Epoch 17, Iteration 250 Train Loss: 110.24\n",
            "New Epoch! Avg loss for the last 100 iterations: 147.2257795715332\n",
            "Epoch 18, Iteration 0 Train Loss: 10648.06\n",
            "Epoch 18, Iteration 10 Train Loss: 267.50\n",
            "Epoch 18, Iteration 20 Train Loss: 134.37\n",
            "Epoch 18, Iteration 30 Train Loss: 68.38\n",
            "Epoch 18, Iteration 40 Train Loss: 116.50\n",
            "Epoch 18, Iteration 50 Train Loss: 149.04\n",
            "Epoch 18, Iteration 60 Train Loss: 858.51\n",
            "Epoch 18, Iteration 70 Train Loss: 300.64\n",
            "Epoch 18, Iteration 80 Train Loss: 59.49\n",
            "Epoch 18, Iteration 90 Train Loss: 171.17\n",
            "Epoch 18, Iteration 100 Train Loss: 248.19\n",
            "Epoch 18, Iteration 110 Train Loss: 201.07\n",
            "Epoch 18, Iteration 120 Train Loss: 314.80\n",
            "Epoch 18, Iteration 130 Train Loss: 102.46\n",
            "Epoch 18, Iteration 140 Train Loss: 118.86\n",
            "Epoch 18, Iteration 150 Train Loss: 139.86\n",
            "Epoch 18, Iteration 160 Train Loss: 127.70\n",
            "Epoch 18, Iteration 170 Train Loss: 140.36\n",
            "Epoch 18, Iteration 180 Train Loss: 91.23\n",
            "Epoch 18, Iteration 190 Train Loss: 97.53\n",
            "Epoch 18, Iteration 200 Train Loss: 123.00\n",
            "Epoch 18, Iteration 210 Train Loss: 214.73\n",
            "Epoch 18, Iteration 220 Train Loss: 212.58\n",
            "Epoch 18, Iteration 230 Train Loss: 138.71\n",
            "Epoch 18, Iteration 240 Train Loss: 75.77\n",
            "Epoch 18, Iteration 250 Train Loss: 170.54\n",
            "New Epoch! Avg loss for the last 100 iterations: 139.53950107574462\n",
            "Epoch 19, Iteration 0 Train Loss: 10621.45\n",
            "Epoch 19, Iteration 10 Train Loss: 197.85\n",
            "Epoch 19, Iteration 20 Train Loss: 152.58\n",
            "Epoch 19, Iteration 30 Train Loss: 170.66\n",
            "Epoch 19, Iteration 40 Train Loss: 80.77\n",
            "Epoch 19, Iteration 50 Train Loss: 136.42\n",
            "Epoch 19, Iteration 60 Train Loss: 857.35\n",
            "Epoch 19, Iteration 70 Train Loss: 322.16\n",
            "Epoch 19, Iteration 80 Train Loss: 68.09\n",
            "Epoch 19, Iteration 90 Train Loss: 160.51\n",
            "Epoch 19, Iteration 100 Train Loss: 248.68\n",
            "Epoch 19, Iteration 110 Train Loss: 186.39\n",
            "Epoch 19, Iteration 120 Train Loss: 318.76\n",
            "Epoch 19, Iteration 130 Train Loss: 118.17\n",
            "Epoch 19, Iteration 140 Train Loss: 123.16\n",
            "Epoch 19, Iteration 150 Train Loss: 182.02\n",
            "Epoch 19, Iteration 160 Train Loss: 120.46\n",
            "Epoch 19, Iteration 170 Train Loss: 131.86\n",
            "Epoch 19, Iteration 180 Train Loss: 110.26\n",
            "Epoch 19, Iteration 190 Train Loss: 86.07\n",
            "Epoch 19, Iteration 200 Train Loss: 116.72\n",
            "Epoch 19, Iteration 210 Train Loss: 168.22\n",
            "Epoch 19, Iteration 220 Train Loss: 156.16\n",
            "Epoch 19, Iteration 230 Train Loss: 86.84\n",
            "Epoch 19, Iteration 240 Train Loss: 69.03\n",
            "Epoch 19, Iteration 250 Train Loss: 155.65\n",
            "New Epoch! Avg loss for the last 100 iterations: 122.4785690689087\n",
            "Epoch 20, Iteration 0 Train Loss: 10610.25\n",
            "Epoch 20, Iteration 10 Train Loss: 196.52\n",
            "Epoch 20, Iteration 20 Train Loss: 143.94\n",
            "Epoch 20, Iteration 30 Train Loss: 123.53\n",
            "Epoch 20, Iteration 40 Train Loss: 82.71\n",
            "Epoch 20, Iteration 50 Train Loss: 131.94\n",
            "Epoch 20, Iteration 60 Train Loss: 845.10\n",
            "Epoch 20, Iteration 70 Train Loss: 306.12\n",
            "Epoch 20, Iteration 80 Train Loss: 63.59\n",
            "Epoch 20, Iteration 90 Train Loss: 149.35\n",
            "Epoch 20, Iteration 100 Train Loss: 204.36\n",
            "Epoch 20, Iteration 110 Train Loss: 170.71\n",
            "Epoch 20, Iteration 120 Train Loss: 297.54\n",
            "Epoch 20, Iteration 130 Train Loss: 107.25\n",
            "Epoch 20, Iteration 140 Train Loss: 117.67\n",
            "Epoch 20, Iteration 150 Train Loss: 143.89\n",
            "Epoch 20, Iteration 160 Train Loss: 95.38\n",
            "Epoch 20, Iteration 170 Train Loss: 131.80\n",
            "Epoch 20, Iteration 180 Train Loss: 79.01\n",
            "Epoch 20, Iteration 190 Train Loss: 76.38\n",
            "Epoch 20, Iteration 200 Train Loss: 103.35\n",
            "Epoch 20, Iteration 210 Train Loss: 211.88\n",
            "Epoch 20, Iteration 220 Train Loss: 182.25\n",
            "Epoch 20, Iteration 230 Train Loss: 103.85\n",
            "Epoch 20, Iteration 240 Train Loss: 68.51\n",
            "Epoch 20, Iteration 250 Train Loss: 145.46\n",
            "New Epoch! Avg loss for the last 100 iterations: 127.367121925354\n",
            "Epoch 21, Iteration 0 Train Loss: 10591.20\n",
            "Epoch 21, Iteration 10 Train Loss: 122.00\n",
            "Epoch 21, Iteration 20 Train Loss: 150.86\n",
            "Epoch 21, Iteration 30 Train Loss: 112.42\n",
            "Epoch 21, Iteration 40 Train Loss: 81.42\n",
            "Epoch 21, Iteration 50 Train Loss: 142.93\n",
            "Epoch 21, Iteration 60 Train Loss: 825.06\n",
            "Epoch 21, Iteration 70 Train Loss: 301.64\n",
            "Epoch 21, Iteration 80 Train Loss: 60.17\n",
            "Epoch 21, Iteration 90 Train Loss: 160.95\n",
            "Epoch 21, Iteration 100 Train Loss: 211.94\n",
            "Epoch 21, Iteration 110 Train Loss: 132.16\n",
            "Epoch 21, Iteration 120 Train Loss: 283.91\n",
            "Epoch 21, Iteration 130 Train Loss: 96.32\n",
            "Epoch 21, Iteration 140 Train Loss: 107.14\n",
            "Epoch 21, Iteration 150 Train Loss: 104.14\n",
            "Epoch 21, Iteration 160 Train Loss: 94.82\n",
            "Epoch 21, Iteration 170 Train Loss: 153.21\n",
            "Epoch 21, Iteration 180 Train Loss: 74.78\n",
            "Epoch 21, Iteration 190 Train Loss: 66.31\n",
            "Epoch 21, Iteration 200 Train Loss: 110.26\n",
            "Epoch 21, Iteration 210 Train Loss: 162.85\n",
            "Epoch 21, Iteration 220 Train Loss: 185.48\n",
            "Epoch 21, Iteration 230 Train Loss: 122.12\n",
            "Epoch 21, Iteration 240 Train Loss: 68.06\n",
            "Epoch 21, Iteration 250 Train Loss: 139.24\n",
            "New Epoch! Avg loss for the last 100 iterations: 125.00520568847656\n",
            "Epoch 22, Iteration 0 Train Loss: 10575.53\n",
            "Epoch 22, Iteration 10 Train Loss: 136.08\n",
            "Epoch 22, Iteration 20 Train Loss: 147.05\n",
            "Epoch 22, Iteration 30 Train Loss: 110.77\n",
            "Epoch 22, Iteration 40 Train Loss: 71.07\n",
            "Epoch 22, Iteration 50 Train Loss: 138.90\n",
            "Epoch 22, Iteration 60 Train Loss: 810.55\n",
            "Epoch 22, Iteration 70 Train Loss: 324.64\n",
            "Epoch 22, Iteration 80 Train Loss: 59.39\n",
            "Epoch 22, Iteration 90 Train Loss: 151.89\n",
            "Epoch 22, Iteration 100 Train Loss: 165.32\n",
            "Epoch 22, Iteration 110 Train Loss: 104.99\n",
            "Epoch 22, Iteration 120 Train Loss: 262.93\n",
            "Epoch 22, Iteration 130 Train Loss: 93.92\n",
            "Epoch 22, Iteration 140 Train Loss: 120.39\n",
            "Epoch 22, Iteration 150 Train Loss: 122.95\n",
            "Epoch 22, Iteration 160 Train Loss: 99.23\n",
            "Epoch 22, Iteration 170 Train Loss: 134.35\n",
            "Epoch 22, Iteration 180 Train Loss: 82.17\n",
            "Epoch 22, Iteration 190 Train Loss: 57.90\n",
            "Epoch 22, Iteration 200 Train Loss: 89.10\n",
            "Epoch 22, Iteration 210 Train Loss: 177.64\n",
            "Epoch 22, Iteration 220 Train Loss: 191.86\n",
            "Epoch 22, Iteration 230 Train Loss: 115.45\n",
            "Epoch 22, Iteration 240 Train Loss: 69.47\n",
            "Epoch 22, Iteration 250 Train Loss: 139.70\n",
            "New Epoch! Avg loss for the last 100 iterations: 122.23139827728272\n",
            "Epoch 23, Iteration 0 Train Loss: 10560.96\n",
            "Epoch 23, Iteration 10 Train Loss: 85.62\n",
            "Epoch 23, Iteration 20 Train Loss: 167.11\n",
            "Epoch 23, Iteration 30 Train Loss: 104.00\n",
            "Epoch 23, Iteration 40 Train Loss: 67.53\n",
            "Epoch 23, Iteration 50 Train Loss: 137.38\n",
            "Epoch 23, Iteration 60 Train Loss: 793.96\n",
            "Epoch 23, Iteration 70 Train Loss: 282.46\n",
            "Epoch 23, Iteration 80 Train Loss: 48.64\n",
            "Epoch 23, Iteration 90 Train Loss: 165.11\n",
            "Epoch 23, Iteration 100 Train Loss: 157.04\n",
            "Epoch 23, Iteration 110 Train Loss: 127.00\n",
            "Epoch 23, Iteration 120 Train Loss: 240.76\n",
            "Epoch 23, Iteration 130 Train Loss: 100.81\n",
            "Epoch 23, Iteration 140 Train Loss: 88.26\n",
            "Epoch 23, Iteration 150 Train Loss: 105.49\n",
            "Epoch 23, Iteration 160 Train Loss: 97.62\n",
            "Epoch 23, Iteration 170 Train Loss: 150.29\n",
            "Epoch 23, Iteration 180 Train Loss: 70.91\n",
            "Epoch 23, Iteration 190 Train Loss: 106.88\n",
            "Epoch 23, Iteration 200 Train Loss: 126.05\n",
            "Epoch 23, Iteration 210 Train Loss: 182.78\n",
            "Epoch 23, Iteration 220 Train Loss: 198.65\n",
            "Epoch 23, Iteration 230 Train Loss: 137.46\n",
            "Epoch 23, Iteration 240 Train Loss: 72.37\n",
            "Epoch 23, Iteration 250 Train Loss: 127.32\n",
            "New Epoch! Avg loss for the last 100 iterations: 131.04021018981933\n",
            "Epoch 24, Iteration 0 Train Loss: 10536.61\n",
            "Epoch 24, Iteration 10 Train Loss: 101.96\n",
            "Epoch 24, Iteration 20 Train Loss: 151.80\n",
            "Epoch 24, Iteration 30 Train Loss: 97.42\n",
            "Epoch 24, Iteration 40 Train Loss: 67.62\n",
            "Epoch 24, Iteration 50 Train Loss: 142.93\n",
            "Epoch 24, Iteration 60 Train Loss: 772.33\n",
            "Epoch 24, Iteration 70 Train Loss: 310.78\n",
            "Epoch 24, Iteration 80 Train Loss: 52.36\n",
            "Epoch 24, Iteration 90 Train Loss: 142.70\n",
            "Epoch 24, Iteration 100 Train Loss: 157.83\n",
            "Epoch 24, Iteration 110 Train Loss: 126.82\n",
            "Epoch 24, Iteration 120 Train Loss: 223.76\n",
            "Epoch 24, Iteration 130 Train Loss: 92.44\n",
            "Epoch 24, Iteration 140 Train Loss: 93.22\n",
            "Epoch 24, Iteration 150 Train Loss: 103.34\n",
            "Epoch 24, Iteration 160 Train Loss: 104.33\n",
            "Epoch 24, Iteration 170 Train Loss: 153.07\n",
            "Epoch 24, Iteration 180 Train Loss: 93.41\n",
            "Epoch 24, Iteration 190 Train Loss: 93.48\n",
            "Epoch 24, Iteration 200 Train Loss: 85.33\n",
            "Epoch 24, Iteration 210 Train Loss: 175.99\n",
            "Epoch 24, Iteration 220 Train Loss: 211.67\n",
            "Epoch 24, Iteration 230 Train Loss: 149.72\n",
            "Epoch 24, Iteration 240 Train Loss: 80.73\n",
            "Epoch 24, Iteration 250 Train Loss: 119.51\n",
            "New Epoch! Avg loss for the last 100 iterations: 133.7246357345581\n"
          ]
        }
      ],
      "source": [
        "#Train new SNN model\n",
        "power_mape = []\n",
        "voltage_mape = []\n",
        "current_mape = []\n",
        "\n",
        "E_actual_list = []\n",
        "E_pred_list = []\n",
        "\n",
        "max_act_list = []\n",
        "pred_act_list = []\n",
        "succ_act_list = []\n",
        "\n",
        "pred_act_naive_list = []\n",
        "false_act_naive_list = []\n",
        "succ_act_naive_list = []\n",
        "\n",
        "#Set parameters\n",
        "batchsize_list = [300, 150, 50, 20, 8]\n",
        "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
        "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
        "n = 0\n",
        "\n",
        "for j in range(len(batchsize_list)):\n",
        "    n += 1\n",
        "    if n == 1: #Select which timescales to train for\n",
        "        #Normalize Data\n",
        "        X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
        "\n",
        "        #Split train and test sets\n",
        "        X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
        "        y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
        "\n",
        "        X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "        y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
        "\n",
        "        batchsize = batchsize_list[j]\n",
        "        time_frame = time_frame_list[j]\n",
        "        time_frame_seconds = time_frame_seconds_list[j]\n",
        "\n",
        "        print(time_frame)\n",
        "\n",
        "        #Resample data\n",
        "        X_train = X_train.resample(time_frame).mean().dropna()\n",
        "        X_valid = X_valid.resample(time_frame).mean().dropna()\n",
        "        X_test = X_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "        y_train = y_train.resample(time_frame).mean().dropna()\n",
        "        y_valid = y_valid.resample(time_frame).mean().dropna()\n",
        "        y_test = y_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "        #Reshape data\n",
        "        X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "        X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
        "        X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "        # convert to tensor\n",
        "        X_train = torch.tensor(X_train)\n",
        "        y_train = torch.tensor(y_train.values)\n",
        "        X_test = torch.tensor(X_test)\n",
        "        y_test = torch.tensor(y_test.values)\n",
        "\n",
        "        # make datasets\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "        # Define the number of time steps for the spiking\n",
        "        num_steps = 50\n",
        "        num_inputs = X_train.shape[2]\n",
        "\n",
        "        # create new inctance of the SNN Class\n",
        "        model = Net(num_inputs, num_steps).to(device)\n",
        "\n",
        "        # define optimizer\n",
        "        optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
        "\n",
        "        # define loss function\n",
        "        def quantile_loss(y_true, y_pred, quantile=0.5):\n",
        "            error = y_true - y_pred\n",
        "            loss = torch.mean(torch.max(quantile * error, (quantile - 1) * error))\n",
        "            return loss\n",
        "        loss_fn = quantile_loss\n",
        "        #loss_fn = torch.nn.MSELoss()\n",
        "        #loss_fn = torch.nn.L1Loss()\n",
        "\n",
        "        # initialize histories\n",
        "        loss_hist = []\n",
        "        avg_loss_hist = []\n",
        "        acc_hist = []\n",
        "        mape_hist = []\n",
        "        num_epochs = 25\n",
        "\n",
        "        # put model into train mode\n",
        "        model.train()\n",
        "\n",
        "        # Train Loop\n",
        "        for epoch in range(num_epochs):\n",
        "            for i, (data, targets) in enumerate(iter(train_loader)):\n",
        "                # move to device\n",
        "                data = data.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # change to floats\n",
        "                data = data.float()\n",
        "                targets = targets.float()\n",
        "\n",
        "                # run forward pass\n",
        "                _, _, _, mem = model(data)\n",
        "                # calculate loss\n",
        "                loss_val = loss_fn(mem[-1], targets)\n",
        "\n",
        "                # calculate and store MAPE Loss\n",
        "                mem_numpy = mem.cpu().detach().numpy()\n",
        "                #mem_numpy = mem.detach().numpy()\n",
        "                targets_numpy = targets.cpu().detach().numpy()\n",
        "                #targets_numpy = targets.detach().numpy()\n",
        "                mape_hist.append(MAPE(mem_numpy[-1], targets_numpy))\n",
        "                power_mape.append(MAPE(mem_numpy[-1][:,0], targets_numpy[:,0]))\n",
        "                voltage_mape.append(MAPE(mem_numpy[-1][:,1], targets_numpy[:,1]))\n",
        "                current_mape.append(MAPE(mem_numpy[-1][:,2], targets_numpy[:,2]))\n",
        "\n",
        "                # Gradient calculation + weight update\n",
        "                optimizer.zero_grad()\n",
        "                loss_val.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Store loss history for future plotting\n",
        "                loss_hist.append(loss_val.item())\n",
        "\n",
        "                if i%10 == 0:\n",
        "                    print(f\"Epoch {epoch}, Iteration {i} Train Loss: {loss_val.item():.2f}\")\n",
        "                if len(loss_hist) > 100:\n",
        "                    avg_loss_hist.append(sum(loss_hist[-100:])/len(loss_hist[-100:]))\n",
        "                else:\n",
        "                    avg_loss_hist.append(0)\n",
        "\n",
        "            if len(loss_hist) > 100:\n",
        "                print(f'New Epoch! Avg loss for the last 100 iterations: {avg_loss_hist[-1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdiFDL0f6avz"
      },
      "outputs": [],
      "source": [
        "#Save model\n",
        "checkpoint = {'state_dict': model.state_dict(),'optimizer' :optimizer.state_dict()}\n",
        "torch.save(checkpoint, 'snn_30min_quant50.pth')\n",
        "!mv snn_30min_quant50.pth 'drive/MyDrive/jLab Shared Docs/MFC Modeling'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGI1Hcg3ti48",
        "outputId": "cb5936e8-8b78-40c8-a319-bee801706dea",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Iteration 0 Train Loss: 22023.50\n",
            "Epoch 0, Iteration 10 Train Loss: 2213.38\n",
            "Epoch 0, Iteration 20 Train Loss: 1749.62\n",
            "Epoch 0, Iteration 30 Train Loss: 1197.17\n",
            "Epoch 0, Iteration 40 Train Loss: 1414.37\n",
            "Epoch 0, Iteration 50 Train Loss: 1720.48\n",
            "Epoch 0, Iteration 60 Train Loss: 1238.34\n",
            "Epoch 0, Iteration 70 Train Loss: 2971.74\n",
            "Epoch 0, Iteration 80 Train Loss: 1844.54\n",
            "Epoch 0, Iteration 90 Train Loss: 2267.17\n",
            "Epoch 1, Iteration 0 Train Loss: 22002.41\n",
            "Epoch 1, Iteration 10 Train Loss: 2190.87\n",
            "Epoch 1, Iteration 20 Train Loss: 1726.24\n",
            "Epoch 1, Iteration 30 Train Loss: 1171.95\n",
            "Epoch 1, Iteration 40 Train Loss: 1386.33\n",
            "Epoch 1, Iteration 50 Train Loss: 1688.86\n",
            "Epoch 1, Iteration 60 Train Loss: 1202.27\n",
            "Epoch 1, Iteration 70 Train Loss: 2929.92\n",
            "Epoch 1, Iteration 80 Train Loss: 1795.16\n",
            "Epoch 1, Iteration 90 Train Loss: 2207.55\n",
            "New Epoch! Avg loss for the last 100 iterations: 2485.7793359375\n",
            "Epoch 2, Iteration 0 Train Loss: 21938.96\n",
            "Epoch 2, Iteration 10 Train Loss: 2111.23\n",
            "Epoch 2, Iteration 20 Train Loss: 1622.07\n",
            "Epoch 2, Iteration 30 Train Loss: 1026.50\n",
            "Epoch 2, Iteration 40 Train Loss: 1163.25\n",
            "Epoch 2, Iteration 50 Train Loss: 1296.03\n",
            "Epoch 2, Iteration 60 Train Loss: 612.87\n",
            "Epoch 2, Iteration 70 Train Loss: 1899.34\n",
            "Epoch 2, Iteration 80 Train Loss: 779.41\n",
            "Epoch 2, Iteration 90 Train Loss: 1299.38\n",
            "New Epoch! Avg loss for the last 100 iterations: 2050.62699508667\n",
            "Epoch 3, Iteration 0 Train Loss: 20703.04\n",
            "Epoch 3, Iteration 10 Train Loss: 972.06\n",
            "Epoch 3, Iteration 20 Train Loss: 533.74\n",
            "Epoch 3, Iteration 30 Train Loss: 164.66\n",
            "Epoch 3, Iteration 40 Train Loss: 317.05\n",
            "Epoch 3, Iteration 50 Train Loss: 406.32\n",
            "Epoch 3, Iteration 60 Train Loss: 305.21\n",
            "Epoch 3, Iteration 70 Train Loss: 1605.26\n",
            "Epoch 3, Iteration 80 Train Loss: 422.45\n",
            "Epoch 3, Iteration 90 Train Loss: 1124.06\n",
            "New Epoch! Avg loss for the last 100 iterations: 1325.8629515457153\n",
            "Epoch 4, Iteration 0 Train Loss: 20350.17\n",
            "Epoch 4, Iteration 10 Train Loss: 695.94\n",
            "Epoch 4, Iteration 20 Train Loss: 233.96\n",
            "Epoch 4, Iteration 30 Train Loss: 41.00\n",
            "Epoch 4, Iteration 40 Train Loss: 165.45\n",
            "Epoch 4, Iteration 50 Train Loss: 157.24\n",
            "Epoch 4, Iteration 60 Train Loss: 231.91\n",
            "Epoch 4, Iteration 70 Train Loss: 1456.25\n",
            "Epoch 4, Iteration 80 Train Loss: 250.93\n",
            "Epoch 4, Iteration 90 Train Loss: 1012.23\n",
            "New Epoch! Avg loss for the last 100 iterations: 1148.783561706543\n",
            "Epoch 5, Iteration 0 Train Loss: 20128.12\n",
            "Epoch 5, Iteration 10 Train Loss: 578.50\n",
            "Epoch 5, Iteration 20 Train Loss: 74.17\n",
            "Epoch 5, Iteration 30 Train Loss: 41.07\n",
            "Epoch 5, Iteration 40 Train Loss: 73.43\n",
            "Epoch 5, Iteration 50 Train Loss: 64.55\n",
            "Epoch 5, Iteration 60 Train Loss: 175.43\n",
            "Epoch 5, Iteration 70 Train Loss: 1355.34\n",
            "Epoch 5, Iteration 80 Train Loss: 154.26\n",
            "Epoch 5, Iteration 90 Train Loss: 922.90\n",
            "New Epoch! Avg loss for the last 100 iterations: 1064.035323791504\n",
            "Epoch 6, Iteration 0 Train Loss: 19976.24\n",
            "Epoch 6, Iteration 10 Train Loss: 503.21\n",
            "Epoch 6, Iteration 20 Train Loss: 30.50\n",
            "Epoch 6, Iteration 30 Train Loss: 48.51\n",
            "Epoch 6, Iteration 40 Train Loss: 37.56\n",
            "Epoch 6, Iteration 50 Train Loss: 43.21\n",
            "Epoch 6, Iteration 60 Train Loss: 139.52\n",
            "Epoch 6, Iteration 70 Train Loss: 1275.14\n",
            "Epoch 6, Iteration 80 Train Loss: 85.44\n",
            "Epoch 6, Iteration 90 Train Loss: 850.92\n",
            "New Epoch! Avg loss for the last 100 iterations: 1014.7066021919251\n",
            "Epoch 7, Iteration 0 Train Loss: 19855.38\n",
            "Epoch 7, Iteration 10 Train Loss: 450.79\n",
            "Epoch 7, Iteration 20 Train Loss: 24.73\n",
            "Epoch 7, Iteration 30 Train Loss: 54.55\n",
            "Epoch 7, Iteration 40 Train Loss: 43.25\n",
            "Epoch 7, Iteration 50 Train Loss: 36.35\n",
            "Epoch 7, Iteration 60 Train Loss: 111.04\n",
            "Epoch 7, Iteration 70 Train Loss: 1208.52\n",
            "Epoch 7, Iteration 80 Train Loss: 44.77\n",
            "Epoch 7, Iteration 90 Train Loss: 801.68\n",
            "New Epoch! Avg loss for the last 100 iterations: 980.9814919662475\n",
            "Epoch 8, Iteration 0 Train Loss: 19755.91\n",
            "Epoch 8, Iteration 10 Train Loss: 409.51\n",
            "Epoch 8, Iteration 20 Train Loss: 29.93\n",
            "Epoch 8, Iteration 30 Train Loss: 59.62\n",
            "Epoch 8, Iteration 40 Train Loss: 48.24\n",
            "Epoch 8, Iteration 50 Train Loss: 38.74\n",
            "Epoch 8, Iteration 60 Train Loss: 91.98\n",
            "Epoch 8, Iteration 70 Train Loss: 1150.10\n",
            "Epoch 8, Iteration 80 Train Loss: 26.44\n",
            "Epoch 8, Iteration 90 Train Loss: 762.19\n",
            "New Epoch! Avg loss for the last 100 iterations: 956.8345451927185\n",
            "Epoch 9, Iteration 0 Train Loss: 19668.58\n",
            "Epoch 9, Iteration 10 Train Loss: 374.92\n",
            "Epoch 9, Iteration 20 Train Loss: 34.33\n",
            "Epoch 9, Iteration 30 Train Loss: 63.87\n",
            "Epoch 9, Iteration 40 Train Loss: 52.40\n",
            "Epoch 9, Iteration 50 Train Loss: 41.10\n",
            "Epoch 9, Iteration 60 Train Loss: 77.43\n",
            "Epoch 9, Iteration 70 Train Loss: 1101.37\n",
            "Epoch 9, Iteration 80 Train Loss: 29.63\n",
            "Epoch 9, Iteration 90 Train Loss: 729.58\n",
            "New Epoch! Avg loss for the last 100 iterations: 939.2748051452637\n",
            "Epoch 10, Iteration 0 Train Loss: 19594.35\n",
            "Epoch 10, Iteration 10 Train Loss: 345.92\n",
            "Epoch 10, Iteration 20 Train Loss: 38.20\n",
            "Epoch 10, Iteration 30 Train Loss: 67.70\n",
            "Epoch 10, Iteration 40 Train Loss: 56.22\n",
            "Epoch 10, Iteration 50 Train Loss: 43.36\n",
            "Epoch 10, Iteration 60 Train Loss: 75.07\n",
            "Epoch 10, Iteration 70 Train Loss: 1056.08\n",
            "Epoch 10, Iteration 80 Train Loss: 33.39\n",
            "Epoch 10, Iteration 90 Train Loss: 702.83\n",
            "New Epoch! Avg loss for the last 100 iterations: 924.8171306610108\n",
            "Epoch 11, Iteration 0 Train Loss: 19523.49\n",
            "Epoch 11, Iteration 10 Train Loss: 318.82\n",
            "Epoch 11, Iteration 20 Train Loss: 41.90\n",
            "Epoch 11, Iteration 30 Train Loss: 71.33\n",
            "Epoch 11, Iteration 40 Train Loss: 59.81\n",
            "Epoch 11, Iteration 50 Train Loss: 45.65\n",
            "Epoch 11, Iteration 60 Train Loss: 72.73\n",
            "Epoch 11, Iteration 70 Train Loss: 1012.71\n",
            "Epoch 11, Iteration 80 Train Loss: 36.88\n",
            "Epoch 11, Iteration 90 Train Loss: 677.41\n",
            "New Epoch! Avg loss for the last 100 iterations: 912.0614921379089\n",
            "Epoch 12, Iteration 0 Train Loss: 19458.03\n",
            "Epoch 12, Iteration 10 Train Loss: 293.88\n",
            "Epoch 12, Iteration 20 Train Loss: 45.29\n",
            "Epoch 12, Iteration 30 Train Loss: 74.66\n",
            "Epoch 12, Iteration 40 Train Loss: 63.12\n",
            "Epoch 12, Iteration 50 Train Loss: 47.65\n",
            "Epoch 12, Iteration 60 Train Loss: 72.02\n",
            "Epoch 12, Iteration 70 Train Loss: 973.34\n",
            "Epoch 12, Iteration 80 Train Loss: 40.12\n",
            "Epoch 12, Iteration 90 Train Loss: 654.45\n",
            "New Epoch! Avg loss for the last 100 iterations: 901.6392954254151\n",
            "Epoch 13, Iteration 0 Train Loss: 19397.02\n",
            "Epoch 13, Iteration 10 Train Loss: 270.93\n",
            "Epoch 13, Iteration 20 Train Loss: 48.45\n",
            "Epoch 13, Iteration 30 Train Loss: 77.79\n",
            "Epoch 13, Iteration 40 Train Loss: 66.23\n",
            "Epoch 13, Iteration 50 Train Loss: 49.84\n",
            "Epoch 13, Iteration 60 Train Loss: 75.10\n",
            "Epoch 13, Iteration 70 Train Loss: 936.24\n",
            "Epoch 13, Iteration 80 Train Loss: 43.17\n",
            "Epoch 13, Iteration 90 Train Loss: 632.58\n",
            "New Epoch! Avg loss for the last 100 iterations: 892.9053162956238\n",
            "Epoch 14, Iteration 0 Train Loss: 19339.29\n",
            "Epoch 14, Iteration 10 Train Loss: 251.18\n",
            "Epoch 14, Iteration 20 Train Loss: 51.46\n",
            "Epoch 14, Iteration 30 Train Loss: 80.75\n",
            "Epoch 14, Iteration 40 Train Loss: 69.17\n",
            "Epoch 14, Iteration 50 Train Loss: 52.77\n",
            "Epoch 14, Iteration 60 Train Loss: 78.02\n",
            "Epoch 14, Iteration 70 Train Loss: 899.34\n",
            "Epoch 14, Iteration 80 Train Loss: 46.06\n",
            "Epoch 14, Iteration 90 Train Loss: 610.78\n",
            "New Epoch! Avg loss for the last 100 iterations: 884.9703805732727\n",
            "Epoch 15, Iteration 0 Train Loss: 19284.74\n",
            "Epoch 15, Iteration 10 Train Loss: 235.97\n",
            "Epoch 15, Iteration 20 Train Loss: 54.34\n",
            "Epoch 15, Iteration 30 Train Loss: 83.63\n",
            "Epoch 15, Iteration 40 Train Loss: 72.04\n",
            "Epoch 15, Iteration 50 Train Loss: 55.65\n",
            "Epoch 15, Iteration 60 Train Loss: 80.89\n",
            "Epoch 15, Iteration 70 Train Loss: 862.98\n",
            "Epoch 15, Iteration 80 Train Loss: 48.93\n",
            "Epoch 15, Iteration 90 Train Loss: 589.27\n",
            "New Epoch! Avg loss for the last 100 iterations: 877.5352279090881\n",
            "Epoch 16, Iteration 0 Train Loss: 19230.38\n",
            "Epoch 16, Iteration 10 Train Loss: 220.76\n",
            "Epoch 16, Iteration 20 Train Loss: 57.22\n",
            "Epoch 16, Iteration 30 Train Loss: 86.51\n",
            "Epoch 16, Iteration 40 Train Loss: 74.91\n",
            "Epoch 16, Iteration 50 Train Loss: 58.51\n",
            "Epoch 16, Iteration 60 Train Loss: 83.76\n",
            "Epoch 16, Iteration 70 Train Loss: 827.20\n",
            "Epoch 16, Iteration 80 Train Loss: 51.77\n",
            "Epoch 16, Iteration 90 Train Loss: 567.94\n",
            "New Epoch! Avg loss for the last 100 iterations: 870.468501625061\n",
            "Epoch 17, Iteration 0 Train Loss: 19176.70\n",
            "Epoch 17, Iteration 10 Train Loss: 205.83\n",
            "Epoch 17, Iteration 20 Train Loss: 60.05\n",
            "Epoch 17, Iteration 30 Train Loss: 89.33\n",
            "Epoch 17, Iteration 40 Train Loss: 77.73\n",
            "Epoch 17, Iteration 50 Train Loss: 61.32\n",
            "Epoch 17, Iteration 60 Train Loss: 86.55\n",
            "Epoch 17, Iteration 70 Train Loss: 793.54\n",
            "Epoch 17, Iteration 80 Train Loss: 54.53\n",
            "Epoch 17, Iteration 90 Train Loss: 546.92\n",
            "New Epoch! Avg loss for the last 100 iterations: 863.6638833618164\n",
            "Epoch 18, Iteration 0 Train Loss: 19124.45\n",
            "Epoch 18, Iteration 10 Train Loss: 191.24\n",
            "Epoch 18, Iteration 20 Train Loss: 62.80\n",
            "Epoch 18, Iteration 30 Train Loss: 92.07\n",
            "Epoch 18, Iteration 40 Train Loss: 80.46\n",
            "Epoch 18, Iteration 50 Train Loss: 64.04\n",
            "Epoch 18, Iteration 60 Train Loss: 89.26\n",
            "Epoch 18, Iteration 70 Train Loss: 763.83\n",
            "Epoch 18, Iteration 80 Train Loss: 57.21\n",
            "Epoch 18, Iteration 90 Train Loss: 526.59\n",
            "New Epoch! Avg loss for the last 100 iterations: 857.3741575622558\n",
            "Epoch 19, Iteration 0 Train Loss: 19073.93\n",
            "Epoch 19, Iteration 10 Train Loss: 177.78\n",
            "Epoch 19, Iteration 20 Train Loss: 65.45\n",
            "Epoch 19, Iteration 30 Train Loss: 94.71\n",
            "Epoch 19, Iteration 40 Train Loss: 83.10\n",
            "Epoch 19, Iteration 50 Train Loss: 66.67\n",
            "Epoch 19, Iteration 60 Train Loss: 91.88\n",
            "Epoch 19, Iteration 70 Train Loss: 734.74\n",
            "Epoch 19, Iteration 80 Train Loss: 59.83\n",
            "Epoch 19, Iteration 90 Train Loss: 506.49\n",
            "New Epoch! Avg loss for the last 100 iterations: 851.5034953689575\n",
            "Epoch 20, Iteration 0 Train Loss: 19024.24\n",
            "Epoch 20, Iteration 10 Train Loss: 164.84\n",
            "Epoch 20, Iteration 20 Train Loss: 68.08\n",
            "Epoch 20, Iteration 30 Train Loss: 97.35\n",
            "Epoch 20, Iteration 40 Train Loss: 85.74\n",
            "Epoch 20, Iteration 50 Train Loss: 69.31\n",
            "Epoch 20, Iteration 60 Train Loss: 94.52\n",
            "Epoch 20, Iteration 70 Train Loss: 706.23\n",
            "Epoch 20, Iteration 80 Train Loss: 62.46\n",
            "Epoch 20, Iteration 90 Train Loss: 486.34\n",
            "New Epoch! Avg loss for the last 100 iterations: 845.798911743164\n",
            "Epoch 21, Iteration 0 Train Loss: 18974.29\n",
            "Epoch 21, Iteration 10 Train Loss: 151.78\n",
            "Epoch 21, Iteration 20 Train Loss: 70.74\n",
            "Epoch 21, Iteration 30 Train Loss: 99.98\n",
            "Epoch 21, Iteration 40 Train Loss: 88.37\n",
            "Epoch 21, Iteration 50 Train Loss: 71.93\n",
            "Epoch 21, Iteration 60 Train Loss: 97.14\n",
            "Epoch 21, Iteration 70 Train Loss: 681.68\n",
            "Epoch 21, Iteration 80 Train Loss: 65.09\n",
            "Epoch 21, Iteration 90 Train Loss: 466.20\n",
            "New Epoch! Avg loss for the last 100 iterations: 840.2205134963989\n",
            "Epoch 22, Iteration 0 Train Loss: 18924.23\n",
            "Epoch 22, Iteration 10 Train Loss: 139.06\n",
            "Epoch 22, Iteration 20 Train Loss: 73.38\n",
            "Epoch 22, Iteration 30 Train Loss: 102.62\n",
            "Epoch 22, Iteration 40 Train Loss: 91.01\n",
            "Epoch 22, Iteration 50 Train Loss: 74.57\n",
            "Epoch 22, Iteration 60 Train Loss: 99.78\n",
            "Epoch 22, Iteration 70 Train Loss: 656.63\n",
            "Epoch 22, Iteration 80 Train Loss: 67.70\n",
            "Epoch 22, Iteration 90 Train Loss: 446.39\n",
            "New Epoch! Avg loss for the last 100 iterations: 834.7696321105957\n",
            "Epoch 23, Iteration 0 Train Loss: 18875.32\n",
            "Epoch 23, Iteration 10 Train Loss: 127.33\n",
            "Epoch 23, Iteration 20 Train Loss: 75.93\n",
            "Epoch 23, Iteration 30 Train Loss: 105.16\n",
            "Epoch 23, Iteration 40 Train Loss: 93.53\n",
            "Epoch 23, Iteration 50 Train Loss: 77.09\n",
            "Epoch 23, Iteration 60 Train Loss: 102.28\n",
            "Epoch 23, Iteration 70 Train Loss: 633.21\n",
            "Epoch 23, Iteration 80 Train Loss: 70.19\n",
            "Epoch 23, Iteration 90 Train Loss: 427.26\n",
            "New Epoch! Avg loss for the last 100 iterations: 829.6558475875854\n",
            "Epoch 24, Iteration 0 Train Loss: 18827.91\n",
            "Epoch 24, Iteration 10 Train Loss: 115.92\n",
            "Epoch 24, Iteration 20 Train Loss: 78.45\n",
            "Epoch 24, Iteration 30 Train Loss: 107.68\n",
            "Epoch 24, Iteration 40 Train Loss: 96.06\n",
            "Epoch 24, Iteration 50 Train Loss: 79.61\n",
            "Epoch 24, Iteration 60 Train Loss: 104.79\n",
            "Epoch 24, Iteration 70 Train Loss: 609.84\n",
            "Epoch 24, Iteration 80 Train Loss: 72.69\n",
            "Epoch 24, Iteration 90 Train Loss: 408.18\n",
            "New Epoch! Avg loss for the last 100 iterations: 824.8303561782836\n",
            "Discontinuity\n",
            "2021-09-24 04:17:21-07:00 2021-08-17 06:37:13-07:00\n",
            "0.04926536 0.0012214300000000001\n",
            "Oracle activations: 34073\n",
            "It's over 9000! 2021-08-17 06:00:00-07:00 2021-09-24 04:00:00-07:00\n",
            "Runtime total_E activations: 90546.45301288954\n",
            "Runtime total_E_pred activations: 283883.3431728721\n",
            "It's over 9000! 2021-08-17 06:00:00-07:00 2021-09-24 04:00:00-07:00\n",
            "Naive total_E activations: 33131.984647044796\n",
            "Naive total_E_pred activations: 39451.51160935354\n",
            "Dataset, train set, and test set size: 1476 738 369\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 0.42112099759693444\n",
            "Maximum possible activations: 34073\n",
            "Predicted activations: 84085\n",
            "Successful activations: 6484, 7.711%\n",
            "Failed activations: 76232, 90.661%\n",
            "Missed activations: 27589, 80.970%\n",
            "Naive predicted activations (usual actual energy average): 46478\n",
            "Naive successful activations (usual actual energy average): 15397, 33.128%\n",
            "Naive failed activations (usual actual energy average): 29769, 64.050%\n",
            "Naive missed activations (usual actual energy average): 18676, 54.812%\n",
            "Voltage overestimation rate: 94.038%\n",
            "Test MAPE power: 2.481210\n",
            "Test MAPE voltage: 0.837154\n",
            "Test MAPE current: 0.530874\n",
            "Epoch 0, Iteration 0 Train Loss: 22024.58\n",
            "Epoch 0, Iteration 10 Train Loss: 2211.94\n",
            "Epoch 0, Iteration 20 Train Loss: 1742.96\n",
            "Epoch 0, Iteration 30 Train Loss: 1184.22\n",
            "Epoch 0, Iteration 40 Train Loss: 1395.28\n",
            "Epoch 0, Iteration 50 Train Loss: 1692.71\n",
            "Epoch 0, Iteration 60 Train Loss: 1197.45\n",
            "Epoch 0, Iteration 70 Train Loss: 2909.45\n",
            "Epoch 0, Iteration 80 Train Loss: 1744.26\n",
            "Epoch 0, Iteration 90 Train Loss: 2092.27\n",
            "Epoch 0, Iteration 100 Train Loss: 1155.21\n",
            "Epoch 0, Iteration 110 Train Loss: 1468.63\n",
            "Epoch 0, Iteration 120 Train Loss: 1589.94\n",
            "Epoch 0, Iteration 130 Train Loss: 1474.86\n",
            "Epoch 0, Iteration 140 Train Loss: 1337.92\n",
            "Epoch 0, Iteration 150 Train Loss: 1668.15\n",
            "Epoch 0, Iteration 160 Train Loss: 1764.67\n",
            "Epoch 0, Iteration 170 Train Loss: 1178.32\n",
            "Epoch 0, Iteration 180 Train Loss: 1280.94\n",
            "New Epoch! Avg loss for the last 100 iterations: 1442.189194946289\n",
            "Epoch 1, Iteration 0 Train Loss: 21371.05\n",
            "Epoch 1, Iteration 10 Train Loss: 1574.76\n",
            "Epoch 1, Iteration 20 Train Loss: 1068.61\n",
            "Epoch 1, Iteration 30 Train Loss: 596.95\n",
            "Epoch 1, Iteration 40 Train Loss: 764.18\n",
            "Epoch 1, Iteration 50 Train Loss: 981.44\n",
            "Epoch 1, Iteration 60 Train Loss: 609.39\n",
            "Epoch 1, Iteration 70 Train Loss: 2160.58\n",
            "Epoch 1, Iteration 80 Train Loss: 1020.44\n",
            "Epoch 1, Iteration 90 Train Loss: 1463.45\n",
            "Epoch 1, Iteration 100 Train Loss: 680.39\n",
            "Epoch 1, Iteration 110 Train Loss: 949.95\n",
            "Epoch 1, Iteration 120 Train Loss: 1120.49\n",
            "Epoch 1, Iteration 130 Train Loss: 1011.22\n",
            "Epoch 1, Iteration 140 Train Loss: 876.29\n",
            "Epoch 1, Iteration 150 Train Loss: 1193.14\n",
            "Epoch 1, Iteration 160 Train Loss: 1274.88\n",
            "Epoch 1, Iteration 170 Train Loss: 759.66\n",
            "Epoch 1, Iteration 180 Train Loss: 852.48\n",
            "New Epoch! Avg loss for the last 100 iterations: 964.0149401855468\n",
            "Epoch 2, Iteration 0 Train Loss: 20898.83\n",
            "Epoch 2, Iteration 10 Train Loss: 1136.74\n",
            "Epoch 2, Iteration 20 Train Loss: 662.84\n",
            "Epoch 2, Iteration 30 Train Loss: 263.15\n",
            "Epoch 2, Iteration 40 Train Loss: 427.01\n",
            "Epoch 2, Iteration 50 Train Loss: 591.66\n",
            "Epoch 2, Iteration 60 Train Loss: 374.40\n",
            "Epoch 2, Iteration 70 Train Loss: 1791.72\n",
            "Epoch 2, Iteration 80 Train Loss: 649.16\n",
            "Epoch 2, Iteration 90 Train Loss: 1242.36\n",
            "Epoch 2, Iteration 100 Train Loss: 379.25\n",
            "Epoch 2, Iteration 110 Train Loss: 592.25\n",
            "Epoch 2, Iteration 120 Train Loss: 870.29\n",
            "Epoch 2, Iteration 130 Train Loss: 696.14\n",
            "Epoch 2, Iteration 140 Train Loss: 524.16\n",
            "Epoch 2, Iteration 150 Train Loss: 849.26\n",
            "Epoch 2, Iteration 160 Train Loss: 896.50\n",
            "Epoch 2, Iteration 170 Train Loss: 407.10\n",
            "Epoch 2, Iteration 180 Train Loss: 565.42\n",
            "New Epoch! Avg loss for the last 100 iterations: 648.8467916870118\n",
            "Epoch 3, Iteration 0 Train Loss: 20516.38\n",
            "Epoch 3, Iteration 10 Train Loss: 796.80\n",
            "Epoch 3, Iteration 20 Train Loss: 317.58\n",
            "Epoch 3, Iteration 30 Train Loss: 56.34\n",
            "Epoch 3, Iteration 40 Train Loss: 198.94\n",
            "Epoch 3, Iteration 50 Train Loss: 254.79\n",
            "Epoch 3, Iteration 60 Train Loss: 251.43\n",
            "Epoch 3, Iteration 70 Train Loss: 1525.12\n",
            "Epoch 3, Iteration 80 Train Loss: 330.20\n",
            "Epoch 3, Iteration 90 Train Loss: 1063.89\n",
            "Epoch 3, Iteration 100 Train Loss: 173.11\n",
            "Epoch 3, Iteration 110 Train Loss: 271.81\n",
            "Epoch 3, Iteration 120 Train Loss: 665.95\n",
            "Epoch 3, Iteration 130 Train Loss: 473.39\n",
            "Epoch 3, Iteration 140 Train Loss: 218.61\n",
            "Epoch 3, Iteration 150 Train Loss: 646.93\n",
            "Epoch 3, Iteration 160 Train Loss: 613.59\n",
            "Epoch 3, Iteration 170 Train Loss: 183.69\n",
            "Epoch 3, Iteration 180 Train Loss: 433.80\n",
            "New Epoch! Avg loss for the last 100 iterations: 421.80860271453855\n",
            "Epoch 4, Iteration 0 Train Loss: 20213.34\n",
            "Epoch 4, Iteration 10 Train Loss: 617.13\n",
            "Epoch 4, Iteration 20 Train Loss: 69.50\n",
            "Epoch 4, Iteration 30 Train Loss: 34.95\n",
            "Epoch 4, Iteration 40 Train Loss: 62.67\n",
            "Epoch 4, Iteration 50 Train Loss: 69.78\n",
            "Epoch 4, Iteration 60 Train Loss: 168.38\n",
            "Epoch 4, Iteration 70 Train Loss: 1358.46\n",
            "Epoch 4, Iteration 80 Train Loss: 155.52\n",
            "Epoch 4, Iteration 90 Train Loss: 928.10\n",
            "Epoch 4, Iteration 100 Train Loss: 86.44\n",
            "Epoch 4, Iteration 110 Train Loss: 73.83\n",
            "Epoch 4, Iteration 120 Train Loss: 527.92\n",
            "Epoch 4, Iteration 130 Train Loss: 343.29\n",
            "Epoch 4, Iteration 140 Train Loss: 58.31\n",
            "Epoch 4, Iteration 150 Train Loss: 510.06\n",
            "Epoch 4, Iteration 160 Train Loss: 445.35\n",
            "Epoch 4, Iteration 170 Train Loss: 123.48\n",
            "Epoch 4, Iteration 180 Train Loss: 352.89\n",
            "New Epoch! Avg loss for the last 100 iterations: 305.52564520835875\n",
            "Epoch 5, Iteration 0 Train Loss: 20014.37\n",
            "Epoch 5, Iteration 10 Train Loss: 510.05\n",
            "Epoch 5, Iteration 20 Train Loss: 18.52\n",
            "Epoch 5, Iteration 30 Train Loss: 45.25\n",
            "Epoch 5, Iteration 40 Train Loss: 34.00\n",
            "Epoch 5, Iteration 50 Train Loss: 34.45\n",
            "Epoch 5, Iteration 60 Train Loss: 114.75\n",
            "Epoch 5, Iteration 70 Train Loss: 1234.80\n",
            "Epoch 5, Iteration 80 Train Loss: 56.98\n",
            "Epoch 5, Iteration 90 Train Loss: 820.67\n",
            "Epoch 5, Iteration 100 Train Loss: 42.76\n",
            "Epoch 5, Iteration 110 Train Loss: 33.47\n",
            "Epoch 5, Iteration 120 Train Loss: 419.79\n",
            "Epoch 5, Iteration 130 Train Loss: 252.70\n",
            "Epoch 5, Iteration 140 Train Loss: 17.05\n",
            "Epoch 5, Iteration 150 Train Loss: 404.93\n",
            "Epoch 5, Iteration 160 Train Loss: 333.22\n",
            "Epoch 5, Iteration 170 Train Loss: 91.99\n",
            "Epoch 5, Iteration 180 Train Loss: 290.55\n",
            "New Epoch! Avg loss for the last 100 iterations: 243.3011855316162\n",
            "Epoch 6, Iteration 0 Train Loss: 19868.66\n",
            "Epoch 6, Iteration 10 Train Loss: 442.94\n",
            "Epoch 6, Iteration 20 Train Loss: 23.08\n",
            "Epoch 6, Iteration 30 Train Loss: 52.62\n",
            "Epoch 6, Iteration 40 Train Loss: 41.23\n",
            "Epoch 6, Iteration 50 Train Loss: 33.70\n",
            "Epoch 6, Iteration 60 Train Loss: 84.05\n",
            "Epoch 6, Iteration 70 Train Loss: 1143.55\n",
            "Epoch 6, Iteration 80 Train Loss: 19.58\n",
            "Epoch 6, Iteration 90 Train Loss: 757.75\n",
            "Epoch 6, Iteration 100 Train Loss: 38.42\n",
            "Epoch 6, Iteration 110 Train Loss: 29.60\n",
            "Epoch 6, Iteration 120 Train Loss: 342.88\n",
            "Epoch 6, Iteration 130 Train Loss: 199.47\n",
            "Epoch 6, Iteration 140 Train Loss: 20.55\n",
            "Epoch 6, Iteration 150 Train Loss: 324.62\n",
            "Epoch 6, Iteration 160 Train Loss: 253.59\n",
            "Epoch 6, Iteration 170 Train Loss: 77.15\n",
            "Epoch 6, Iteration 180 Train Loss: 241.79\n",
            "New Epoch! Avg loss for the last 100 iterations: 209.24891975402832\n",
            "Epoch 7, Iteration 0 Train Loss: 19762.55\n",
            "Epoch 7, Iteration 10 Train Loss: 396.69\n",
            "Epoch 7, Iteration 20 Train Loss: 28.58\n",
            "Epoch 7, Iteration 30 Train Loss: 58.04\n",
            "Epoch 7, Iteration 40 Train Loss: 46.58\n",
            "Epoch 7, Iteration 50 Train Loss: 36.76\n",
            "Epoch 7, Iteration 60 Train Loss: 68.17\n",
            "Epoch 7, Iteration 70 Train Loss: 1068.99\n",
            "Epoch 7, Iteration 80 Train Loss: 23.89\n",
            "Epoch 7, Iteration 90 Train Loss: 708.95\n",
            "Epoch 7, Iteration 100 Train Loss: 43.52\n",
            "Epoch 7, Iteration 110 Train Loss: 30.34\n",
            "Epoch 7, Iteration 120 Train Loss: 291.01\n",
            "Epoch 7, Iteration 130 Train Loss: 161.64\n",
            "Epoch 7, Iteration 140 Train Loss: 25.47\n",
            "Epoch 7, Iteration 150 Train Loss: 258.83\n",
            "Epoch 7, Iteration 160 Train Loss: 190.00\n",
            "Epoch 7, Iteration 170 Train Loss: 70.00\n",
            "Epoch 7, Iteration 180 Train Loss: 205.10\n",
            "New Epoch! Avg loss for the last 100 iterations: 186.61670736312865\n",
            "Epoch 8, Iteration 0 Train Loss: 19675.21\n",
            "Epoch 8, Iteration 10 Train Loss: 358.03\n",
            "Epoch 8, Iteration 20 Train Loss: 33.19\n",
            "Epoch 8, Iteration 30 Train Loss: 62.61\n",
            "Epoch 8, Iteration 40 Train Loss: 51.12\n",
            "Epoch 8, Iteration 50 Train Loss: 39.57\n",
            "Epoch 8, Iteration 60 Train Loss: 64.10\n",
            "Epoch 8, Iteration 70 Train Loss: 1004.50\n",
            "Epoch 8, Iteration 80 Train Loss: 28.32\n",
            "Epoch 8, Iteration 90 Train Loss: 671.01\n",
            "Epoch 8, Iteration 100 Train Loss: 47.88\n",
            "Epoch 8, Iteration 110 Train Loss: 31.92\n",
            "Epoch 8, Iteration 120 Train Loss: 253.14\n",
            "Epoch 8, Iteration 130 Train Loss: 132.63\n",
            "Epoch 8, Iteration 140 Train Loss: 29.75\n",
            "Epoch 8, Iteration 150 Train Loss: 218.10\n",
            "Epoch 8, Iteration 160 Train Loss: 143.21\n",
            "Epoch 8, Iteration 170 Train Loss: 67.40\n",
            "Epoch 8, Iteration 180 Train Loss: 176.28\n",
            "New Epoch! Avg loss for the last 100 iterations: 170.96595542907716\n",
            "Epoch 9, Iteration 0 Train Loss: 19597.50\n",
            "Epoch 9, Iteration 10 Train Loss: 324.06\n",
            "Epoch 9, Iteration 20 Train Loss: 37.33\n",
            "Epoch 9, Iteration 30 Train Loss: 66.74\n",
            "Epoch 9, Iteration 40 Train Loss: 55.24\n",
            "Epoch 9, Iteration 50 Train Loss: 42.40\n",
            "Epoch 9, Iteration 60 Train Loss: 64.28\n",
            "Epoch 9, Iteration 70 Train Loss: 944.64\n",
            "Epoch 9, Iteration 80 Train Loss: 32.38\n",
            "Epoch 9, Iteration 90 Train Loss: 635.33\n",
            "Epoch 9, Iteration 100 Train Loss: 51.89\n",
            "Epoch 9, Iteration 110 Train Loss: 34.77\n",
            "Epoch 9, Iteration 120 Train Loss: 217.83\n",
            "Epoch 9, Iteration 130 Train Loss: 111.54\n",
            "Epoch 9, Iteration 140 Train Loss: 33.67\n",
            "Epoch 9, Iteration 150 Train Loss: 183.64\n",
            "Epoch 9, Iteration 160 Train Loss: 107.89\n",
            "Epoch 9, Iteration 170 Train Loss: 66.80\n",
            "Epoch 9, Iteration 180 Train Loss: 149.84\n",
            "New Epoch! Avg loss for the last 100 iterations: 158.4934814453125\n",
            "Epoch 10, Iteration 0 Train Loss: 19524.95\n",
            "Epoch 10, Iteration 10 Train Loss: 294.59\n",
            "Epoch 10, Iteration 20 Train Loss: 41.22\n",
            "Epoch 10, Iteration 30 Train Loss: 70.60\n",
            "Epoch 10, Iteration 40 Train Loss: 59.08\n",
            "Epoch 10, Iteration 50 Train Loss: 45.17\n",
            "Epoch 10, Iteration 60 Train Loss: 68.08\n",
            "Epoch 10, Iteration 70 Train Loss: 888.33\n",
            "Epoch 10, Iteration 80 Train Loss: 36.15\n",
            "Epoch 10, Iteration 90 Train Loss: 601.64\n",
            "Epoch 10, Iteration 100 Train Loss: 55.63\n",
            "Epoch 10, Iteration 110 Train Loss: 37.52\n",
            "Epoch 10, Iteration 120 Train Loss: 190.77\n",
            "Epoch 10, Iteration 130 Train Loss: 92.91\n",
            "Epoch 10, Iteration 140 Train Loss: 37.36\n",
            "Epoch 10, Iteration 150 Train Loss: 157.20\n",
            "Epoch 10, Iteration 160 Train Loss: 81.07\n",
            "Epoch 10, Iteration 170 Train Loss: 66.89\n",
            "Epoch 10, Iteration 180 Train Loss: 124.02\n",
            "New Epoch! Avg loss for the last 100 iterations: 148.12611860275268\n",
            "Epoch 11, Iteration 0 Train Loss: 19460.73\n",
            "Epoch 11, Iteration 10 Train Loss: 272.65\n",
            "Epoch 11, Iteration 20 Train Loss: 44.76\n",
            "Epoch 11, Iteration 30 Train Loss: 74.14\n",
            "Epoch 11, Iteration 40 Train Loss: 62.62\n",
            "Epoch 11, Iteration 50 Train Loss: 47.73\n",
            "Epoch 11, Iteration 60 Train Loss: 71.59\n",
            "Epoch 11, Iteration 70 Train Loss: 833.71\n",
            "Epoch 11, Iteration 80 Train Loss: 39.65\n",
            "Epoch 11, Iteration 90 Train Loss: 568.84\n",
            "Epoch 11, Iteration 100 Train Loss: 59.10\n",
            "Epoch 11, Iteration 110 Train Loss: 40.07\n",
            "Epoch 11, Iteration 120 Train Loss: 165.44\n",
            "Epoch 11, Iteration 130 Train Loss: 74.81\n",
            "Epoch 11, Iteration 140 Train Loss: 40.72\n",
            "Epoch 11, Iteration 150 Train Loss: 132.18\n",
            "Epoch 11, Iteration 160 Train Loss: 61.87\n",
            "Epoch 11, Iteration 170 Train Loss: 67.80\n",
            "Epoch 11, Iteration 180 Train Loss: 104.33\n",
            "New Epoch! Avg loss for the last 100 iterations: 139.21373498916626\n",
            "Epoch 12, Iteration 0 Train Loss: 19398.15\n",
            "Epoch 12, Iteration 10 Train Loss: 252.19\n",
            "Epoch 12, Iteration 20 Train Loss: 48.04\n",
            "Epoch 12, Iteration 30 Train Loss: 77.42\n",
            "Epoch 12, Iteration 40 Train Loss: 65.89\n",
            "Epoch 12, Iteration 50 Train Loss: 50.30\n",
            "Epoch 12, Iteration 60 Train Loss: 74.84\n",
            "Epoch 12, Iteration 70 Train Loss: 782.80\n",
            "Epoch 12, Iteration 80 Train Loss: 42.90\n",
            "Epoch 12, Iteration 90 Train Loss: 537.09\n",
            "Epoch 12, Iteration 100 Train Loss: 62.33\n",
            "Epoch 12, Iteration 110 Train Loss: 42.90\n",
            "Epoch 12, Iteration 120 Train Loss: 140.63\n",
            "Epoch 12, Iteration 130 Train Loss: 63.90\n",
            "Epoch 12, Iteration 140 Train Loss: 43.88\n",
            "Epoch 12, Iteration 150 Train Loss: 107.83\n",
            "Epoch 12, Iteration 160 Train Loss: 46.32\n",
            "Epoch 12, Iteration 170 Train Loss: 69.62\n",
            "Epoch 12, Iteration 180 Train Loss: 86.70\n",
            "New Epoch! Avg loss for the last 100 iterations: 131.53197481155397\n",
            "Epoch 13, Iteration 0 Train Loss: 19340.43\n",
            "Epoch 13, Iteration 10 Train Loss: 232.17\n",
            "Epoch 13, Iteration 20 Train Loss: 51.27\n",
            "Epoch 13, Iteration 30 Train Loss: 80.66\n",
            "Epoch 13, Iteration 40 Train Loss: 69.11\n",
            "Epoch 13, Iteration 50 Train Loss: 52.92\n",
            "Epoch 13, Iteration 60 Train Loss: 78.03\n",
            "Epoch 13, Iteration 70 Train Loss: 736.91\n",
            "Epoch 13, Iteration 80 Train Loss: 46.10\n",
            "Epoch 13, Iteration 90 Train Loss: 504.85\n",
            "Epoch 13, Iteration 100 Train Loss: 65.52\n",
            "Epoch 13, Iteration 110 Train Loss: 46.08\n",
            "Epoch 13, Iteration 120 Train Loss: 122.39\n",
            "Epoch 13, Iteration 130 Train Loss: 52.73\n",
            "Epoch 13, Iteration 140 Train Loss: 47.01\n",
            "Epoch 13, Iteration 150 Train Loss: 89.38\n",
            "Epoch 13, Iteration 160 Train Loss: 35.18\n",
            "Epoch 13, Iteration 170 Train Loss: 71.62\n",
            "Epoch 13, Iteration 180 Train Loss: 75.68\n",
            "New Epoch! Avg loss for the last 100 iterations: 124.58302995681763\n",
            "Epoch 14, Iteration 0 Train Loss: 19281.36\n",
            "Epoch 14, Iteration 10 Train Loss: 212.23\n",
            "Epoch 14, Iteration 20 Train Loss: 54.39\n",
            "Epoch 14, Iteration 30 Train Loss: 83.78\n",
            "Epoch 14, Iteration 40 Train Loss: 72.23\n",
            "Epoch 14, Iteration 50 Train Loss: 55.84\n",
            "Epoch 14, Iteration 60 Train Loss: 81.12\n",
            "Epoch 14, Iteration 70 Train Loss: 691.87\n",
            "Epoch 14, Iteration 80 Train Loss: 49.17\n",
            "Epoch 14, Iteration 90 Train Loss: 472.85\n",
            "Epoch 14, Iteration 100 Train Loss: 68.57\n",
            "Epoch 14, Iteration 110 Train Loss: 49.09\n",
            "Epoch 14, Iteration 120 Train Loss: 103.98\n",
            "Epoch 14, Iteration 130 Train Loss: 46.37\n",
            "Epoch 14, Iteration 140 Train Loss: 49.89\n",
            "Epoch 14, Iteration 150 Train Loss: 70.43\n",
            "Epoch 14, Iteration 160 Train Loss: 28.99\n",
            "Epoch 14, Iteration 170 Train Loss: 74.66\n",
            "Epoch 14, Iteration 180 Train Loss: 64.04\n",
            "New Epoch! Avg loss for the last 100 iterations: 118.54011779785156\n",
            "Epoch 15, Iteration 0 Train Loss: 19228.02\n",
            "Epoch 15, Iteration 10 Train Loss: 192.90\n",
            "Epoch 15, Iteration 20 Train Loss: 57.52\n",
            "Epoch 15, Iteration 30 Train Loss: 86.95\n",
            "Epoch 15, Iteration 40 Train Loss: 75.40\n",
            "Epoch 15, Iteration 50 Train Loss: 58.98\n",
            "Epoch 15, Iteration 60 Train Loss: 84.26\n",
            "Epoch 15, Iteration 70 Train Loss: 650.03\n",
            "Epoch 15, Iteration 80 Train Loss: 52.33\n",
            "Epoch 15, Iteration 90 Train Loss: 438.04\n",
            "Epoch 15, Iteration 100 Train Loss: 71.72\n",
            "Epoch 15, Iteration 110 Train Loss: 52.23\n",
            "Epoch 15, Iteration 120 Train Loss: 88.22\n",
            "Epoch 15, Iteration 130 Train Loss: 47.73\n",
            "Epoch 15, Iteration 140 Train Loss: 52.92\n",
            "Epoch 15, Iteration 150 Train Loss: 55.89\n",
            "Epoch 15, Iteration 160 Train Loss: 26.87\n",
            "Epoch 15, Iteration 170 Train Loss: 76.74\n",
            "Epoch 15, Iteration 180 Train Loss: 58.66\n",
            "New Epoch! Avg loss for the last 100 iterations: 113.35459394454956\n",
            "Epoch 16, Iteration 0 Train Loss: 19173.25\n",
            "Epoch 16, Iteration 10 Train Loss: 174.46\n",
            "Epoch 16, Iteration 20 Train Loss: 60.37\n",
            "Epoch 16, Iteration 30 Train Loss: 89.78\n",
            "Epoch 16, Iteration 40 Train Loss: 78.22\n",
            "Epoch 16, Iteration 50 Train Loss: 61.79\n",
            "Epoch 16, Iteration 60 Train Loss: 87.06\n",
            "Epoch 16, Iteration 70 Train Loss: 614.29\n",
            "Epoch 16, Iteration 80 Train Loss: 55.10\n",
            "Epoch 16, Iteration 90 Train Loss: 409.04\n",
            "Epoch 16, Iteration 100 Train Loss: 74.47\n",
            "Epoch 16, Iteration 110 Train Loss: 54.99\n",
            "Epoch 16, Iteration 120 Train Loss: 78.25\n",
            "Epoch 16, Iteration 130 Train Loss: 50.49\n",
            "Epoch 16, Iteration 140 Train Loss: 55.68\n",
            "Epoch 16, Iteration 150 Train Loss: 45.94\n",
            "Epoch 16, Iteration 160 Train Loss: 29.58\n",
            "Epoch 16, Iteration 170 Train Loss: 79.09\n",
            "Epoch 16, Iteration 180 Train Loss: 55.10\n",
            "New Epoch! Avg loss for the last 100 iterations: 109.71029392242431\n",
            "Epoch 17, Iteration 0 Train Loss: 19122.65\n",
            "Epoch 17, Iteration 10 Train Loss: 159.29\n",
            "Epoch 17, Iteration 20 Train Loss: 63.06\n",
            "Epoch 17, Iteration 30 Train Loss: 92.48\n",
            "Epoch 17, Iteration 40 Train Loss: 80.91\n",
            "Epoch 17, Iteration 50 Train Loss: 64.48\n",
            "Epoch 17, Iteration 60 Train Loss: 89.74\n",
            "Epoch 17, Iteration 70 Train Loss: 582.50\n",
            "Epoch 17, Iteration 80 Train Loss: 57.76\n",
            "Epoch 17, Iteration 90 Train Loss: 380.84\n",
            "Epoch 17, Iteration 100 Train Loss: 77.12\n",
            "Epoch 17, Iteration 110 Train Loss: 57.61\n",
            "Epoch 17, Iteration 120 Train Loss: 68.73\n",
            "Epoch 17, Iteration 130 Train Loss: 53.09\n",
            "Epoch 17, Iteration 140 Train Loss: 58.26\n",
            "Epoch 17, Iteration 150 Train Loss: 40.98\n",
            "Epoch 17, Iteration 160 Train Loss: 32.11\n",
            "Epoch 17, Iteration 170 Train Loss: 81.25\n",
            "Epoch 17, Iteration 180 Train Loss: 55.81\n",
            "New Epoch! Avg loss for the last 100 iterations: 106.64930027008057\n",
            "Epoch 18, Iteration 0 Train Loss: 19075.40\n",
            "Epoch 18, Iteration 10 Train Loss: 149.00\n",
            "Epoch 18, Iteration 20 Train Loss: 65.58\n",
            "Epoch 18, Iteration 30 Train Loss: 94.98\n",
            "Epoch 18, Iteration 40 Train Loss: 83.40\n",
            "Epoch 18, Iteration 50 Train Loss: 66.96\n",
            "Epoch 18, Iteration 60 Train Loss: 92.20\n",
            "Epoch 18, Iteration 70 Train Loss: 555.86\n",
            "Epoch 18, Iteration 80 Train Loss: 60.23\n",
            "Epoch 18, Iteration 90 Train Loss: 354.08\n",
            "Epoch 18, Iteration 100 Train Loss: 79.58\n",
            "Epoch 18, Iteration 110 Train Loss: 60.06\n",
            "Epoch 18, Iteration 120 Train Loss: 60.30\n",
            "Epoch 18, Iteration 130 Train Loss: 55.50\n",
            "Epoch 18, Iteration 140 Train Loss: 60.65\n",
            "Epoch 18, Iteration 150 Train Loss: 41.37\n",
            "Epoch 18, Iteration 160 Train Loss: 34.46\n",
            "Epoch 18, Iteration 170 Train Loss: 83.22\n",
            "Epoch 18, Iteration 180 Train Loss: 58.12\n",
            "New Epoch! Avg loss for the last 100 iterations: 104.15832103729248\n",
            "Epoch 19, Iteration 0 Train Loss: 19031.47\n",
            "Epoch 19, Iteration 10 Train Loss: 139.46\n",
            "Epoch 19, Iteration 20 Train Loss: 67.95\n",
            "Epoch 19, Iteration 30 Train Loss: 97.31\n",
            "Epoch 19, Iteration 40 Train Loss: 85.71\n",
            "Epoch 19, Iteration 50 Train Loss: 69.26\n",
            "Epoch 19, Iteration 60 Train Loss: 94.50\n",
            "Epoch 19, Iteration 70 Train Loss: 530.73\n",
            "Epoch 19, Iteration 80 Train Loss: 62.54\n",
            "Epoch 19, Iteration 90 Train Loss: 329.00\n",
            "Epoch 19, Iteration 100 Train Loss: 81.86\n",
            "Epoch 19, Iteration 110 Train Loss: 62.34\n",
            "Epoch 19, Iteration 120 Train Loss: 57.11\n",
            "Epoch 19, Iteration 130 Train Loss: 57.78\n",
            "Epoch 19, Iteration 140 Train Loss: 62.92\n",
            "Epoch 19, Iteration 150 Train Loss: 43.63\n",
            "Epoch 19, Iteration 160 Train Loss: 36.71\n",
            "Epoch 19, Iteration 170 Train Loss: 85.31\n",
            "Epoch 19, Iteration 180 Train Loss: 60.34\n",
            "New Epoch! Avg loss for the last 100 iterations: 102.22005466461182\n",
            "Epoch 20, Iteration 0 Train Loss: 18989.54\n",
            "Epoch 20, Iteration 10 Train Loss: 130.36\n",
            "Epoch 20, Iteration 20 Train Loss: 70.20\n",
            "Epoch 20, Iteration 30 Train Loss: 99.57\n",
            "Epoch 20, Iteration 40 Train Loss: 87.97\n",
            "Epoch 20, Iteration 50 Train Loss: 71.52\n",
            "Epoch 20, Iteration 60 Train Loss: 96.76\n",
            "Epoch 20, Iteration 70 Train Loss: 506.10\n",
            "Epoch 20, Iteration 80 Train Loss: 64.80\n",
            "Epoch 20, Iteration 90 Train Loss: 304.35\n",
            "Epoch 20, Iteration 100 Train Loss: 84.11\n",
            "Epoch 20, Iteration 110 Train Loss: 64.58\n",
            "Epoch 20, Iteration 120 Train Loss: 55.99\n",
            "Epoch 20, Iteration 130 Train Loss: 59.96\n",
            "Epoch 20, Iteration 140 Train Loss: 65.08\n",
            "Epoch 20, Iteration 150 Train Loss: 45.77\n",
            "Epoch 20, Iteration 160 Train Loss: 38.84\n",
            "Epoch 20, Iteration 170 Train Loss: 87.28\n",
            "Epoch 20, Iteration 180 Train Loss: 62.45\n",
            "New Epoch! Avg loss for the last 100 iterations: 100.6434704208374\n",
            "Epoch 21, Iteration 0 Train Loss: 18949.46\n",
            "Epoch 21, Iteration 10 Train Loss: 122.35\n",
            "Epoch 21, Iteration 20 Train Loss: 72.34\n",
            "Epoch 21, Iteration 30 Train Loss: 101.68\n",
            "Epoch 21, Iteration 40 Train Loss: 90.07\n",
            "Epoch 21, Iteration 50 Train Loss: 73.61\n",
            "Epoch 21, Iteration 60 Train Loss: 98.84\n",
            "Epoch 21, Iteration 70 Train Loss: 483.18\n",
            "Epoch 21, Iteration 80 Train Loss: 66.86\n",
            "Epoch 21, Iteration 90 Train Loss: 281.52\n",
            "Epoch 21, Iteration 100 Train Loss: 86.17\n",
            "Epoch 21, Iteration 110 Train Loss: 66.64\n",
            "Epoch 21, Iteration 120 Train Loss: 58.01\n",
            "Epoch 21, Iteration 130 Train Loss: 61.93\n",
            "Epoch 21, Iteration 140 Train Loss: 67.01\n",
            "Epoch 21, Iteration 150 Train Loss: 47.69\n",
            "Epoch 21, Iteration 160 Train Loss: 40.75\n",
            "Epoch 21, Iteration 170 Train Loss: 89.53\n",
            "Epoch 21, Iteration 180 Train Loss: 64.35\n",
            "New Epoch! Avg loss for the last 100 iterations: 99.38526546478272\n",
            "Epoch 22, Iteration 0 Train Loss: 18913.30\n",
            "Epoch 22, Iteration 10 Train Loss: 119.37\n",
            "Epoch 22, Iteration 20 Train Loss: 74.29\n",
            "Epoch 22, Iteration 30 Train Loss: 103.64\n",
            "Epoch 22, Iteration 40 Train Loss: 92.03\n",
            "Epoch 22, Iteration 50 Train Loss: 75.56\n",
            "Epoch 22, Iteration 60 Train Loss: 100.79\n",
            "Epoch 22, Iteration 70 Train Loss: 463.14\n",
            "Epoch 22, Iteration 80 Train Loss: 68.81\n",
            "Epoch 22, Iteration 90 Train Loss: 259.14\n",
            "Epoch 22, Iteration 100 Train Loss: 88.11\n",
            "Epoch 22, Iteration 110 Train Loss: 68.57\n",
            "Epoch 22, Iteration 120 Train Loss: 59.98\n",
            "Epoch 22, Iteration 130 Train Loss: 63.92\n",
            "Epoch 22, Iteration 140 Train Loss: 69.00\n",
            "Epoch 22, Iteration 150 Train Loss: 49.66\n",
            "Epoch 22, Iteration 160 Train Loss: 42.72\n",
            "Epoch 22, Iteration 170 Train Loss: 91.31\n",
            "Epoch 22, Iteration 180 Train Loss: 66.28\n",
            "New Epoch! Avg loss for the last 100 iterations: 98.32835865020752\n",
            "Epoch 23, Iteration 0 Train Loss: 18876.70\n",
            "Epoch 23, Iteration 10 Train Loss: 116.21\n",
            "Epoch 23, Iteration 20 Train Loss: 76.25\n",
            "Epoch 23, Iteration 30 Train Loss: 105.60\n",
            "Epoch 23, Iteration 40 Train Loss: 93.98\n",
            "Epoch 23, Iteration 50 Train Loss: 77.51\n",
            "Epoch 23, Iteration 60 Train Loss: 102.74\n",
            "Epoch 23, Iteration 70 Train Loss: 446.31\n",
            "Epoch 23, Iteration 80 Train Loss: 70.76\n",
            "Epoch 23, Iteration 90 Train Loss: 240.14\n",
            "Epoch 23, Iteration 100 Train Loss: 90.04\n",
            "Epoch 23, Iteration 110 Train Loss: 70.48\n",
            "Epoch 23, Iteration 120 Train Loss: 61.84\n",
            "Epoch 23, Iteration 130 Train Loss: 65.73\n",
            "Epoch 23, Iteration 140 Train Loss: 70.78\n",
            "Epoch 23, Iteration 150 Train Loss: 51.43\n",
            "Epoch 23, Iteration 160 Train Loss: 44.48\n",
            "Epoch 23, Iteration 170 Train Loss: 93.41\n",
            "Epoch 23, Iteration 180 Train Loss: 68.03\n",
            "New Epoch! Avg loss for the last 100 iterations: 97.54416561126709\n",
            "Epoch 24, Iteration 0 Train Loss: 18843.47\n",
            "Epoch 24, Iteration 10 Train Loss: 116.88\n",
            "Epoch 24, Iteration 20 Train Loss: 78.03\n",
            "Epoch 24, Iteration 30 Train Loss: 107.36\n",
            "Epoch 24, Iteration 40 Train Loss: 95.73\n",
            "Epoch 24, Iteration 50 Train Loss: 79.25\n",
            "Epoch 24, Iteration 60 Train Loss: 104.49\n",
            "Epoch 24, Iteration 70 Train Loss: 430.59\n",
            "Epoch 24, Iteration 80 Train Loss: 72.52\n",
            "Epoch 24, Iteration 90 Train Loss: 224.29\n",
            "Epoch 24, Iteration 100 Train Loss: 91.80\n",
            "Epoch 24, Iteration 110 Train Loss: 72.25\n",
            "Epoch 24, Iteration 120 Train Loss: 63.65\n",
            "Epoch 24, Iteration 130 Train Loss: 67.56\n",
            "Epoch 24, Iteration 140 Train Loss: 72.61\n",
            "Epoch 24, Iteration 150 Train Loss: 53.25\n",
            "Epoch 24, Iteration 160 Train Loss: 46.29\n",
            "Epoch 24, Iteration 170 Train Loss: 95.03\n",
            "Epoch 24, Iteration 180 Train Loss: 69.84\n",
            "New Epoch! Avg loss for the last 100 iterations: 97.02215244293212\n",
            "Discontinuity\n",
            "2021-11-22 01:00:02-08:00 2021-11-10 23:59:53-08:00\n",
            "0.0012214300000000001 0.02535399\n",
            "Oracle activations: 10996\n",
            "It's over 9000! 2021-11-10 23:00:00-08:00 2021-11-22 01:00:00-08:00\n",
            "Runtime total_E activations: 23962.27847877203\n",
            "Runtime total_E_pred activations: 194666.00187660928\n",
            "It's over 9000! 2021-11-10 23:00:00-08:00 2021-11-22 01:00:00-08:00\n",
            "Naive total_E activations: 10347.00107440601\n",
            "Naive total_E_pred activations: 14082.006131178232\n",
            "Dataset, train set, and test set size: 2211 1474 367\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 0.000171939477303989\n",
            "Maximum possible activations: 10996\n",
            "Predicted activations: 113256\n",
            "Successful activations: 1, 0.001%\n",
            "Failed activations: 95777, 84.567%\n",
            "Missed activations: 10995, 99.991%\n",
            "Naive predicted activations (usual actual energy average): 15217\n",
            "Naive successful activations (usual actual energy average): 5816, 38.220%\n",
            "Naive failed activations (usual actual energy average): 9401, 61.780%\n",
            "Naive missed activations (usual actual energy average): 5180, 47.108%\n",
            "Voltage overestimation rate: 100.000%\n",
            "Test MAPE power: 1.932139\n",
            "Test MAPE voltage: 2.918926\n",
            "Test MAPE current: 0.117654\n",
            "Epoch 0, Iteration 0 Train Loss: 22023.59\n",
            "Epoch 0, Iteration 10 Train Loss: 2213.93\n",
            "Epoch 0, Iteration 20 Train Loss: 1749.86\n",
            "Epoch 0, Iteration 30 Train Loss: 1197.09\n",
            "Epoch 0, Iteration 40 Train Loss: 1414.02\n",
            "Epoch 0, Iteration 50 Train Loss: 1719.71\n",
            "Epoch 0, Iteration 60 Train Loss: 1237.02\n",
            "Epoch 0, Iteration 70 Train Loss: 2969.75\n",
            "Epoch 0, Iteration 80 Train Loss: 1841.68\n",
            "Epoch 0, Iteration 90 Train Loss: 2263.14\n",
            "Epoch 0, Iteration 100 Train Loss: 1466.04\n",
            "Epoch 0, Iteration 110 Train Loss: 1830.08\n",
            "Epoch 0, Iteration 120 Train Loss: 1986.31\n",
            "Epoch 0, Iteration 130 Train Loss: 1900.50\n",
            "Epoch 0, Iteration 140 Train Loss: 1788.43\n",
            "Epoch 0, Iteration 150 Train Loss: 2137.20\n",
            "Epoch 0, Iteration 160 Train Loss: 2246.98\n",
            "Epoch 0, Iteration 170 Train Loss: 1658.06\n",
            "Epoch 0, Iteration 180 Train Loss: 1714.98\n",
            "Epoch 0, Iteration 190 Train Loss: 1383.24\n",
            "Epoch 0, Iteration 200 Train Loss: 1042.88\n",
            "Epoch 0, Iteration 210 Train Loss: 793.62\n",
            "Epoch 0, Iteration 220 Train Loss: 1132.48\n",
            "Epoch 0, Iteration 230 Train Loss: 842.33\n",
            "Epoch 0, Iteration 240 Train Loss: 789.75\n",
            "Epoch 0, Iteration 250 Train Loss: 578.39\n",
            "Epoch 0, Iteration 260 Train Loss: 701.87\n",
            "Epoch 0, Iteration 270 Train Loss: 232.47\n",
            "New Epoch! Avg loss for the last 100 iterations: 867.1392517471313\n",
            "Epoch 1, Iteration 0 Train Loss: 20818.59\n",
            "Epoch 1, Iteration 10 Train Loss: 1050.13\n",
            "Epoch 1, Iteration 20 Train Loss: 578.62\n",
            "Epoch 1, Iteration 30 Train Loss: 211.09\n",
            "Epoch 1, Iteration 40 Train Loss: 365.34\n",
            "Epoch 1, Iteration 50 Train Loss: 471.03\n",
            "Epoch 1, Iteration 60 Train Loss: 339.37\n",
            "Epoch 1, Iteration 70 Train Loss: 1672.60\n",
            "Epoch 1, Iteration 80 Train Loss: 498.71\n",
            "Epoch 1, Iteration 90 Train Loss: 1175.81\n",
            "Epoch 1, Iteration 100 Train Loss: 279.37\n",
            "Epoch 1, Iteration 110 Train Loss: 419.88\n",
            "Epoch 1, Iteration 120 Train Loss: 766.20\n",
            "Epoch 1, Iteration 130 Train Loss: 568.06\n",
            "Epoch 1, Iteration 140 Train Loss: 337.26\n",
            "Epoch 1, Iteration 150 Train Loss: 727.53\n",
            "Epoch 1, Iteration 160 Train Loss: 702.76\n",
            "Epoch 1, Iteration 170 Train Loss: 226.13\n",
            "Epoch 1, Iteration 180 Train Loss: 473.03\n",
            "Epoch 1, Iteration 190 Train Loss: 123.76\n",
            "Epoch 1, Iteration 200 Train Loss: 238.42\n",
            "Epoch 1, Iteration 210 Train Loss: 94.03\n",
            "Epoch 1, Iteration 220 Train Loss: 331.16\n",
            "Epoch 1, Iteration 230 Train Loss: 125.80\n",
            "Epoch 1, Iteration 240 Train Loss: 89.16\n",
            "Epoch 1, Iteration 250 Train Loss: 143.59\n",
            "Epoch 1, Iteration 260 Train Loss: 156.19\n",
            "Epoch 1, Iteration 270 Train Loss: 39.74\n",
            "New Epoch! Avg loss for the last 100 iterations: 153.65031848907472\n",
            "Epoch 2, Iteration 0 Train Loss: 20158.87\n",
            "Epoch 2, Iteration 10 Train Loss: 576.38\n",
            "Epoch 2, Iteration 20 Train Loss: 65.95\n",
            "Epoch 2, Iteration 30 Train Loss: 38.84\n",
            "Epoch 2, Iteration 40 Train Loss: 80.72\n",
            "Epoch 2, Iteration 50 Train Loss: 52.88\n",
            "Epoch 2, Iteration 60 Train Loss: 181.26\n",
            "Epoch 2, Iteration 70 Train Loss: 1368.16\n",
            "Epoch 2, Iteration 80 Train Loss: 165.68\n",
            "Epoch 2, Iteration 90 Train Loss: 934.82\n",
            "Epoch 2, Iteration 100 Train Loss: 89.71\n",
            "Epoch 2, Iteration 110 Train Loss: 36.28\n",
            "Epoch 2, Iteration 120 Train Loss: 522.45\n",
            "Epoch 2, Iteration 130 Train Loss: 334.80\n",
            "Epoch 2, Iteration 140 Train Loss: 31.74\n",
            "Epoch 2, Iteration 150 Train Loss: 494.29\n",
            "Epoch 2, Iteration 160 Train Loss: 418.48\n",
            "Epoch 2, Iteration 170 Train Loss: 59.49\n",
            "Epoch 2, Iteration 180 Train Loss: 334.27\n",
            "Epoch 2, Iteration 190 Train Loss: 23.33\n",
            "Epoch 2, Iteration 200 Train Loss: 118.61\n",
            "Epoch 2, Iteration 210 Train Loss: 33.88\n",
            "Epoch 2, Iteration 220 Train Loss: 82.00\n",
            "Epoch 2, Iteration 230 Train Loss: 41.19\n",
            "Epoch 2, Iteration 240 Train Loss: 30.60\n",
            "Epoch 2, Iteration 250 Train Loss: 73.95\n",
            "Epoch 2, Iteration 260 Train Loss: 65.05\n",
            "Epoch 2, Iteration 270 Train Loss: 54.69\n",
            "New Epoch! Avg loss for the last 100 iterations: 69.62031213760376\n",
            "Epoch 3, Iteration 0 Train Loss: 19877.54\n",
            "Epoch 3, Iteration 10 Train Loss: 467.15\n",
            "Epoch 3, Iteration 20 Train Loss: 23.31\n",
            "Epoch 3, Iteration 30 Train Loss: 53.11\n",
            "Epoch 3, Iteration 40 Train Loss: 41.78\n",
            "Epoch 3, Iteration 50 Train Loss: 26.21\n",
            "Epoch 3, Iteration 60 Train Loss: 111.47\n",
            "Epoch 3, Iteration 70 Train Loss: 1217.09\n",
            "Epoch 3, Iteration 80 Train Loss: 46.37\n",
            "Epoch 3, Iteration 90 Train Loss: 804.54\n",
            "Epoch 3, Iteration 100 Train Loss: 42.09\n",
            "Epoch 3, Iteration 110 Train Loss: 20.70\n",
            "Epoch 3, Iteration 120 Train Loss: 388.56\n",
            "Epoch 3, Iteration 130 Train Loss: 230.53\n",
            "Epoch 3, Iteration 140 Train Loss: 22.28\n",
            "Epoch 3, Iteration 150 Train Loss: 368.19\n",
            "Epoch 3, Iteration 160 Train Loss: 294.78\n",
            "Epoch 3, Iteration 170 Train Loss: 31.84\n",
            "Epoch 3, Iteration 180 Train Loss: 263.03\n",
            "Epoch 3, Iteration 190 Train Loss: 33.77\n",
            "Epoch 3, Iteration 200 Train Loss: 63.55\n",
            "Epoch 3, Iteration 210 Train Loss: 39.00\n",
            "Epoch 3, Iteration 220 Train Loss: 33.08\n",
            "Epoch 3, Iteration 230 Train Loss: 29.48\n",
            "Epoch 3, Iteration 240 Train Loss: 28.45\n",
            "Epoch 3, Iteration 250 Train Loss: 52.47\n",
            "Epoch 3, Iteration 260 Train Loss: 43.34\n",
            "Epoch 3, Iteration 270 Train Loss: 63.35\n",
            "New Epoch! Avg loss for the last 100 iterations: 53.8312692451477\n",
            "Epoch 4, Iteration 0 Train Loss: 19714.54\n",
            "Epoch 4, Iteration 10 Train Loss: 403.85\n",
            "Epoch 4, Iteration 20 Train Loss: 31.88\n",
            "Epoch 4, Iteration 30 Train Loss: 61.55\n",
            "Epoch 4, Iteration 40 Train Loss: 50.12\n",
            "Epoch 4, Iteration 50 Train Loss: 33.81\n",
            "Epoch 4, Iteration 60 Train Loss: 76.52\n",
            "Epoch 4, Iteration 70 Train Loss: 1109.45\n",
            "Epoch 4, Iteration 80 Train Loss: 27.75\n",
            "Epoch 4, Iteration 90 Train Loss: 730.54\n",
            "Epoch 4, Iteration 100 Train Loss: 47.61\n",
            "Epoch 4, Iteration 110 Train Loss: 28.38\n",
            "Epoch 4, Iteration 120 Train Loss: 309.48\n",
            "Epoch 4, Iteration 130 Train Loss: 174.02\n",
            "Epoch 4, Iteration 140 Train Loss: 29.80\n",
            "Epoch 4, Iteration 150 Train Loss: 273.31\n",
            "Epoch 4, Iteration 160 Train Loss: 203.36\n",
            "Epoch 4, Iteration 170 Train Loss: 34.18\n",
            "Epoch 4, Iteration 180 Train Loss: 212.81\n",
            "Epoch 4, Iteration 190 Train Loss: 40.73\n",
            "Epoch 4, Iteration 200 Train Loss: 58.64\n",
            "Epoch 4, Iteration 210 Train Loss: 45.77\n",
            "Epoch 4, Iteration 220 Train Loss: 26.14\n",
            "Epoch 4, Iteration 230 Train Loss: 34.99\n",
            "Epoch 4, Iteration 240 Train Loss: 34.92\n",
            "Epoch 4, Iteration 250 Train Loss: 50.93\n",
            "Epoch 4, Iteration 260 Train Loss: 41.53\n",
            "Epoch 4, Iteration 270 Train Loss: 69.40\n",
            "New Epoch! Avg loss for the last 100 iterations: 52.86552942276001\n",
            "Epoch 5, Iteration 0 Train Loss: 19601.56\n",
            "Epoch 5, Iteration 10 Train Loss: 359.10\n",
            "Epoch 5, Iteration 20 Train Loss: 37.85\n",
            "Epoch 5, Iteration 30 Train Loss: 67.48\n",
            "Epoch 5, Iteration 40 Train Loss: 56.02\n",
            "Epoch 5, Iteration 50 Train Loss: 39.68\n",
            "Epoch 5, Iteration 60 Train Loss: 70.22\n",
            "Epoch 5, Iteration 70 Train Loss: 1022.29\n",
            "Epoch 5, Iteration 80 Train Loss: 33.58\n",
            "Epoch 5, Iteration 90 Train Loss: 678.62\n",
            "Epoch 5, Iteration 100 Train Loss: 53.33\n",
            "Epoch 5, Iteration 110 Train Loss: 34.03\n",
            "Epoch 5, Iteration 120 Train Loss: 256.24\n",
            "Epoch 5, Iteration 130 Train Loss: 135.20\n",
            "Epoch 5, Iteration 140 Train Loss: 35.33\n",
            "Epoch 5, Iteration 150 Train Loss: 217.85\n",
            "Epoch 5, Iteration 160 Train Loss: 141.07\n",
            "Epoch 5, Iteration 170 Train Loss: 39.54\n",
            "Epoch 5, Iteration 180 Train Loss: 175.57\n",
            "Epoch 5, Iteration 190 Train Loss: 46.01\n",
            "Epoch 5, Iteration 200 Train Loss: 60.71\n",
            "Epoch 5, Iteration 210 Train Loss: 50.97\n",
            "Epoch 5, Iteration 220 Train Loss: 27.71\n",
            "Epoch 5, Iteration 230 Train Loss: 40.08\n",
            "Epoch 5, Iteration 240 Train Loss: 39.94\n",
            "Epoch 5, Iteration 250 Train Loss: 52.34\n",
            "Epoch 5, Iteration 260 Train Loss: 43.51\n",
            "Epoch 5, Iteration 270 Train Loss: 74.21\n",
            "New Epoch! Avg loss for the last 100 iterations: 55.274745025634765\n",
            "Epoch 6, Iteration 0 Train Loss: 19510.65\n",
            "Epoch 6, Iteration 10 Train Loss: 321.67\n",
            "Epoch 6, Iteration 20 Train Loss: 42.72\n",
            "Epoch 6, Iteration 30 Train Loss: 72.31\n",
            "Epoch 6, Iteration 40 Train Loss: 60.82\n",
            "Epoch 6, Iteration 50 Train Loss: 44.46\n",
            "Epoch 6, Iteration 60 Train Loss: 69.87\n",
            "Epoch 6, Iteration 70 Train Loss: 945.62\n",
            "Epoch 6, Iteration 80 Train Loss: 38.29\n",
            "Epoch 6, Iteration 90 Train Loss: 632.84\n",
            "Epoch 6, Iteration 100 Train Loss: 58.00\n",
            "Epoch 6, Iteration 110 Train Loss: 38.68\n",
            "Epoch 6, Iteration 120 Train Loss: 212.07\n",
            "Epoch 6, Iteration 130 Train Loss: 108.39\n",
            "Epoch 6, Iteration 140 Train Loss: 39.72\n",
            "Epoch 6, Iteration 150 Train Loss: 174.79\n",
            "Epoch 6, Iteration 160 Train Loss: 98.05\n",
            "Epoch 6, Iteration 170 Train Loss: 43.71\n",
            "Epoch 6, Iteration 180 Train Loss: 139.88\n",
            "Epoch 6, Iteration 190 Train Loss: 50.04\n",
            "Epoch 6, Iteration 200 Train Loss: 64.38\n",
            "Epoch 6, Iteration 210 Train Loss: 54.91\n",
            "Epoch 6, Iteration 220 Train Loss: 31.35\n",
            "Epoch 6, Iteration 230 Train Loss: 44.01\n",
            "Epoch 6, Iteration 240 Train Loss: 43.85\n",
            "Epoch 6, Iteration 250 Train Loss: 55.55\n",
            "Epoch 6, Iteration 260 Train Loss: 46.99\n",
            "Epoch 6, Iteration 270 Train Loss: 78.13\n",
            "New Epoch! Avg loss for the last 100 iterations: 58.46780820846558\n",
            "Epoch 7, Iteration 0 Train Loss: 19436.46\n",
            "Epoch 7, Iteration 10 Train Loss: 292.80\n",
            "Epoch 7, Iteration 20 Train Loss: 46.84\n",
            "Epoch 7, Iteration 30 Train Loss: 76.45\n",
            "Epoch 7, Iteration 40 Train Loss: 64.95\n",
            "Epoch 7, Iteration 50 Train Loss: 48.56\n",
            "Epoch 7, Iteration 60 Train Loss: 73.97\n",
            "Epoch 7, Iteration 70 Train Loss: 871.79\n",
            "Epoch 7, Iteration 80 Train Loss: 42.39\n",
            "Epoch 7, Iteration 90 Train Loss: 587.76\n",
            "Epoch 7, Iteration 100 Train Loss: 62.08\n",
            "Epoch 7, Iteration 110 Train Loss: 42.74\n",
            "Epoch 7, Iteration 120 Train Loss: 177.06\n",
            "Epoch 7, Iteration 130 Train Loss: 83.43\n",
            "Epoch 7, Iteration 140 Train Loss: 43.90\n",
            "Epoch 7, Iteration 150 Train Loss: 141.03\n",
            "Epoch 7, Iteration 160 Train Loss: 69.03\n",
            "Epoch 7, Iteration 170 Train Loss: 47.88\n",
            "Epoch 7, Iteration 180 Train Loss: 110.83\n",
            "Epoch 7, Iteration 190 Train Loss: 54.21\n",
            "Epoch 7, Iteration 200 Train Loss: 67.47\n",
            "Epoch 7, Iteration 210 Train Loss: 59.05\n",
            "Epoch 7, Iteration 220 Train Loss: 34.50\n",
            "Epoch 7, Iteration 230 Train Loss: 48.12\n",
            "Epoch 7, Iteration 240 Train Loss: 47.93\n",
            "Epoch 7, Iteration 250 Train Loss: 58.99\n",
            "Epoch 7, Iteration 260 Train Loss: 50.32\n",
            "Epoch 7, Iteration 270 Train Loss: 82.13\n",
            "New Epoch! Avg loss for the last 100 iterations: 61.626121292114256\n",
            "Epoch 8, Iteration 0 Train Loss: 19360.57\n",
            "Epoch 8, Iteration 10 Train Loss: 268.49\n",
            "Epoch 8, Iteration 20 Train Loss: 50.91\n",
            "Epoch 8, Iteration 30 Train Loss: 80.52\n",
            "Epoch 8, Iteration 40 Train Loss: 69.00\n",
            "Epoch 8, Iteration 50 Train Loss: 52.60\n",
            "Epoch 8, Iteration 60 Train Loss: 77.99\n",
            "Epoch 8, Iteration 70 Train Loss: 802.81\n",
            "Epoch 8, Iteration 80 Train Loss: 46.38\n",
            "Epoch 8, Iteration 90 Train Loss: 546.50\n",
            "Epoch 8, Iteration 100 Train Loss: 66.02\n",
            "Epoch 8, Iteration 110 Train Loss: 46.65\n",
            "Epoch 8, Iteration 120 Train Loss: 145.34\n",
            "Epoch 8, Iteration 130 Train Loss: 67.65\n",
            "Epoch 8, Iteration 140 Train Loss: 47.72\n",
            "Epoch 8, Iteration 150 Train Loss: 110.51\n",
            "Epoch 8, Iteration 160 Train Loss: 49.42\n",
            "Epoch 8, Iteration 170 Train Loss: 51.62\n",
            "Epoch 8, Iteration 180 Train Loss: 89.42\n",
            "Epoch 8, Iteration 190 Train Loss: 57.90\n",
            "Epoch 8, Iteration 200 Train Loss: 70.70\n",
            "Epoch 8, Iteration 210 Train Loss: 62.71\n",
            "Epoch 8, Iteration 220 Train Loss: 37.74\n",
            "Epoch 8, Iteration 230 Train Loss: 51.75\n",
            "Epoch 8, Iteration 240 Train Loss: 51.56\n",
            "Epoch 8, Iteration 250 Train Loss: 62.60\n",
            "Epoch 8, Iteration 260 Train Loss: 53.59\n",
            "Epoch 8, Iteration 270 Train Loss: 85.73\n",
            "New Epoch! Avg loss for the last 100 iterations: 64.7683084487915\n",
            "Epoch 9, Iteration 0 Train Loss: 19292.37\n",
            "Epoch 9, Iteration 10 Train Loss: 245.86\n",
            "Epoch 9, Iteration 20 Train Loss: 54.58\n",
            "Epoch 9, Iteration 30 Train Loss: 84.17\n",
            "Epoch 9, Iteration 40 Train Loss: 72.65\n",
            "Epoch 9, Iteration 50 Train Loss: 56.23\n",
            "Epoch 9, Iteration 60 Train Loss: 81.60\n",
            "Epoch 9, Iteration 70 Train Loss: 747.58\n",
            "Epoch 9, Iteration 80 Train Loss: 49.98\n",
            "Epoch 9, Iteration 90 Train Loss: 508.29\n",
            "Epoch 9, Iteration 100 Train Loss: 69.57\n",
            "Epoch 9, Iteration 110 Train Loss: 50.18\n",
            "Epoch 9, Iteration 120 Train Loss: 123.94\n",
            "Epoch 9, Iteration 130 Train Loss: 54.74\n",
            "Epoch 9, Iteration 140 Train Loss: 51.17\n",
            "Epoch 9, Iteration 150 Train Loss: 89.62\n",
            "Epoch 9, Iteration 160 Train Loss: 36.71\n",
            "Epoch 9, Iteration 170 Train Loss: 54.95\n",
            "Epoch 9, Iteration 180 Train Loss: 76.59\n",
            "Epoch 9, Iteration 190 Train Loss: 60.96\n",
            "Epoch 9, Iteration 200 Train Loss: 74.05\n",
            "Epoch 9, Iteration 210 Train Loss: 65.54\n",
            "Epoch 9, Iteration 220 Train Loss: 40.98\n",
            "Epoch 9, Iteration 230 Train Loss: 54.51\n",
            "Epoch 9, Iteration 240 Train Loss: 54.27\n",
            "Epoch 9, Iteration 250 Train Loss: 65.30\n",
            "Epoch 9, Iteration 260 Train Loss: 56.40\n",
            "Epoch 9, Iteration 270 Train Loss: 88.49\n",
            "New Epoch! Avg loss for the last 100 iterations: 67.63190021514893\n",
            "Epoch 10, Iteration 0 Train Loss: 19240.03\n",
            "Epoch 10, Iteration 10 Train Loss: 228.52\n",
            "Epoch 10, Iteration 20 Train Loss: 57.59\n",
            "Epoch 10, Iteration 30 Train Loss: 87.23\n",
            "Epoch 10, Iteration 40 Train Loss: 75.71\n",
            "Epoch 10, Iteration 50 Train Loss: 59.26\n",
            "Epoch 10, Iteration 60 Train Loss: 84.63\n",
            "Epoch 10, Iteration 70 Train Loss: 698.70\n",
            "Epoch 10, Iteration 80 Train Loss: 53.03\n",
            "Epoch 10, Iteration 90 Train Loss: 472.41\n",
            "Epoch 10, Iteration 100 Train Loss: 72.64\n",
            "Epoch 10, Iteration 110 Train Loss: 53.24\n",
            "Epoch 10, Iteration 120 Train Loss: 103.48\n",
            "Epoch 10, Iteration 130 Train Loss: 48.93\n",
            "Epoch 10, Iteration 140 Train Loss: 54.13\n",
            "Epoch 10, Iteration 150 Train Loss: 69.55\n",
            "Epoch 10, Iteration 160 Train Loss: 31.69\n",
            "Epoch 10, Iteration 170 Train Loss: 57.87\n",
            "Epoch 10, Iteration 180 Train Loss: 64.84\n",
            "Epoch 10, Iteration 190 Train Loss: 64.01\n",
            "Epoch 10, Iteration 200 Train Loss: 76.65\n",
            "Epoch 10, Iteration 210 Train Loss: 68.74\n",
            "Epoch 10, Iteration 220 Train Loss: 43.68\n",
            "Epoch 10, Iteration 230 Train Loss: 57.75\n",
            "Epoch 10, Iteration 240 Train Loss: 57.53\n",
            "Epoch 10, Iteration 250 Train Loss: 68.57\n",
            "Epoch 10, Iteration 260 Train Loss: 59.32\n",
            "Epoch 10, Iteration 270 Train Loss: 91.78\n",
            "New Epoch! Avg loss for the last 100 iterations: 70.39181007385254\n",
            "Epoch 11, Iteration 0 Train Loss: 19177.17\n",
            "Epoch 11, Iteration 10 Train Loss: 207.81\n",
            "Epoch 11, Iteration 20 Train Loss: 61.38\n",
            "Epoch 11, Iteration 30 Train Loss: 90.99\n",
            "Epoch 11, Iteration 40 Train Loss: 79.45\n",
            "Epoch 11, Iteration 50 Train Loss: 62.99\n",
            "Epoch 11, Iteration 60 Train Loss: 88.36\n",
            "Epoch 11, Iteration 70 Train Loss: 655.57\n",
            "Epoch 11, Iteration 80 Train Loss: 56.68\n",
            "Epoch 11, Iteration 90 Train Loss: 438.37\n",
            "Epoch 11, Iteration 100 Train Loss: 76.20\n",
            "Epoch 11, Iteration 110 Train Loss: 56.74\n",
            "Epoch 11, Iteration 120 Train Loss: 89.85\n",
            "Epoch 11, Iteration 130 Train Loss: 52.27\n",
            "Epoch 11, Iteration 140 Train Loss: 57.45\n",
            "Epoch 11, Iteration 150 Train Loss: 56.97\n",
            "Epoch 11, Iteration 160 Train Loss: 31.61\n",
            "Epoch 11, Iteration 170 Train Loss: 61.07\n",
            "Epoch 11, Iteration 180 Train Loss: 61.39\n",
            "Epoch 11, Iteration 190 Train Loss: 67.17\n",
            "Epoch 11, Iteration 200 Train Loss: 79.12\n",
            "Epoch 11, Iteration 210 Train Loss: 71.89\n",
            "Epoch 11, Iteration 220 Train Loss: 46.16\n",
            "Epoch 11, Iteration 230 Train Loss: 60.88\n",
            "Epoch 11, Iteration 240 Train Loss: 60.65\n",
            "Epoch 11, Iteration 250 Train Loss: 71.69\n",
            "Epoch 11, Iteration 260 Train Loss: 61.94\n",
            "Epoch 11, Iteration 270 Train Loss: 94.82\n",
            "New Epoch! Avg loss for the last 100 iterations: 73.08273029327393\n",
            "Epoch 12, Iteration 0 Train Loss: 19119.72\n",
            "Epoch 12, Iteration 10 Train Loss: 188.57\n",
            "Epoch 12, Iteration 20 Train Loss: 63.97\n",
            "Epoch 12, Iteration 30 Train Loss: 93.56\n",
            "Epoch 12, Iteration 40 Train Loss: 82.01\n",
            "Epoch 12, Iteration 50 Train Loss: 65.54\n",
            "Epoch 12, Iteration 60 Train Loss: 90.89\n",
            "Epoch 12, Iteration 70 Train Loss: 619.86\n",
            "Epoch 12, Iteration 80 Train Loss: 59.18\n",
            "Epoch 12, Iteration 90 Train Loss: 408.86\n",
            "Epoch 12, Iteration 100 Train Loss: 78.72\n",
            "Epoch 12, Iteration 110 Train Loss: 59.28\n",
            "Epoch 12, Iteration 120 Train Loss: 79.43\n",
            "Epoch 12, Iteration 130 Train Loss: 54.83\n",
            "Epoch 12, Iteration 140 Train Loss: 59.99\n",
            "Epoch 12, Iteration 150 Train Loss: 46.92\n",
            "Epoch 12, Iteration 160 Train Loss: 34.06\n",
            "Epoch 12, Iteration 170 Train Loss: 63.47\n",
            "Epoch 12, Iteration 180 Train Loss: 57.58\n",
            "Epoch 12, Iteration 190 Train Loss: 69.52\n",
            "Epoch 12, Iteration 200 Train Loss: 81.70\n",
            "Epoch 12, Iteration 210 Train Loss: 74.20\n",
            "Epoch 12, Iteration 220 Train Loss: 48.73\n",
            "Epoch 12, Iteration 230 Train Loss: 63.18\n",
            "Epoch 12, Iteration 240 Train Loss: 62.94\n",
            "Epoch 12, Iteration 250 Train Loss: 73.99\n",
            "Epoch 12, Iteration 260 Train Loss: 64.41\n",
            "Epoch 12, Iteration 270 Train Loss: 97.17\n",
            "New Epoch! Avg loss for the last 100 iterations: 75.47287353515625\n",
            "Epoch 13, Iteration 0 Train Loss: 19074.95\n",
            "Epoch 13, Iteration 10 Train Loss: 174.90\n",
            "Epoch 13, Iteration 20 Train Loss: 66.40\n",
            "Epoch 13, Iteration 30 Train Loss: 96.01\n",
            "Epoch 13, Iteration 40 Train Loss: 84.45\n",
            "Epoch 13, Iteration 50 Train Loss: 67.98\n",
            "Epoch 13, Iteration 60 Train Loss: 93.31\n",
            "Epoch 13, Iteration 70 Train Loss: 588.85\n",
            "Epoch 13, Iteration 80 Train Loss: 61.62\n",
            "Epoch 13, Iteration 90 Train Loss: 380.85\n",
            "Epoch 13, Iteration 100 Train Loss: 81.15\n",
            "Epoch 13, Iteration 110 Train Loss: 61.69\n",
            "Epoch 13, Iteration 120 Train Loss: 69.81\n",
            "Epoch 13, Iteration 130 Train Loss: 57.22\n",
            "Epoch 13, Iteration 140 Train Loss: 62.34\n",
            "Epoch 13, Iteration 150 Train Loss: 43.37\n",
            "Epoch 13, Iteration 160 Train Loss: 36.38\n",
            "Epoch 13, Iteration 170 Train Loss: 65.77\n",
            "Epoch 13, Iteration 180 Train Loss: 59.87\n",
            "Epoch 13, Iteration 190 Train Loss: 71.79\n",
            "Epoch 13, Iteration 200 Train Loss: 84.10\n",
            "Epoch 13, Iteration 210 Train Loss: 76.45\n",
            "Epoch 13, Iteration 220 Train Loss: 51.13\n",
            "Epoch 13, Iteration 230 Train Loss: 65.42\n",
            "Epoch 13, Iteration 240 Train Loss: 65.18\n",
            "Epoch 13, Iteration 250 Train Loss: 76.23\n",
            "Epoch 13, Iteration 260 Train Loss: 66.73\n",
            "Epoch 13, Iteration 270 Train Loss: 99.46\n",
            "New Epoch! Avg loss for the last 100 iterations: 77.80094291687011\n",
            "Epoch 14, Iteration 0 Train Loss: 19031.43\n",
            "Epoch 14, Iteration 10 Train Loss: 165.72\n",
            "Epoch 14, Iteration 20 Train Loss: 68.78\n",
            "Epoch 14, Iteration 30 Train Loss: 98.38\n",
            "Epoch 14, Iteration 40 Train Loss: 86.82\n",
            "Epoch 14, Iteration 50 Train Loss: 70.33\n",
            "Epoch 14, Iteration 60 Train Loss: 95.65\n",
            "Epoch 14, Iteration 70 Train Loss: 562.71\n",
            "Epoch 14, Iteration 80 Train Loss: 63.96\n",
            "Epoch 14, Iteration 90 Train Loss: 354.46\n",
            "Epoch 14, Iteration 100 Train Loss: 83.50\n",
            "Epoch 14, Iteration 110 Train Loss: 64.03\n",
            "Epoch 14, Iteration 120 Train Loss: 62.85\n",
            "Epoch 14, Iteration 130 Train Loss: 59.52\n",
            "Epoch 14, Iteration 140 Train Loss: 64.63\n",
            "Epoch 14, Iteration 150 Train Loss: 45.38\n",
            "Epoch 14, Iteration 160 Train Loss: 38.63\n",
            "Epoch 14, Iteration 170 Train Loss: 67.97\n",
            "Epoch 14, Iteration 180 Train Loss: 62.03\n",
            "Epoch 14, Iteration 190 Train Loss: 73.93\n",
            "Epoch 14, Iteration 200 Train Loss: 86.27\n",
            "Epoch 14, Iteration 210 Train Loss: 78.57\n",
            "Epoch 14, Iteration 220 Train Loss: 53.29\n",
            "Epoch 14, Iteration 230 Train Loss: 67.53\n",
            "Epoch 14, Iteration 240 Train Loss: 67.28\n",
            "Epoch 14, Iteration 250 Train Loss: 78.33\n",
            "Epoch 14, Iteration 260 Train Loss: 68.83\n",
            "Epoch 14, Iteration 270 Train Loss: 101.58\n",
            "New Epoch! Avg loss for the last 100 iterations: 79.93511013031006\n",
            "Epoch 15, Iteration 0 Train Loss: 18991.19\n",
            "Epoch 15, Iteration 10 Train Loss: 157.24\n",
            "Epoch 15, Iteration 20 Train Loss: 70.97\n",
            "Epoch 15, Iteration 30 Train Loss: 100.55\n",
            "Epoch 15, Iteration 40 Train Loss: 88.97\n",
            "Epoch 15, Iteration 50 Train Loss: 72.47\n",
            "Epoch 15, Iteration 60 Train Loss: 97.78\n",
            "Epoch 15, Iteration 70 Train Loss: 538.71\n",
            "Epoch 15, Iteration 80 Train Loss: 66.11\n",
            "Epoch 15, Iteration 90 Train Loss: 330.52\n",
            "Epoch 15, Iteration 100 Train Loss: 85.63\n",
            "Epoch 15, Iteration 110 Train Loss: 66.15\n",
            "Epoch 15, Iteration 120 Train Loss: 59.75\n",
            "Epoch 15, Iteration 130 Train Loss: 61.63\n",
            "Epoch 15, Iteration 140 Train Loss: 66.74\n",
            "Epoch 15, Iteration 150 Train Loss: 47.46\n",
            "Epoch 15, Iteration 160 Train Loss: 40.68\n",
            "Epoch 15, Iteration 170 Train Loss: 70.01\n",
            "Epoch 15, Iteration 180 Train Loss: 64.04\n",
            "Epoch 15, Iteration 190 Train Loss: 75.93\n",
            "Epoch 15, Iteration 200 Train Loss: 88.28\n",
            "Epoch 15, Iteration 210 Train Loss: 80.55\n",
            "Epoch 15, Iteration 220 Train Loss: 55.29\n",
            "Epoch 15, Iteration 230 Train Loss: 69.50\n",
            "Epoch 15, Iteration 240 Train Loss: 69.24\n",
            "Epoch 15, Iteration 250 Train Loss: 80.29\n",
            "Epoch 15, Iteration 260 Train Loss: 70.79\n",
            "Epoch 15, Iteration 270 Train Loss: 103.55\n",
            "New Epoch! Avg loss for the last 100 iterations: 81.9186141204834\n",
            "Epoch 16, Iteration 0 Train Loss: 18953.73\n",
            "Epoch 16, Iteration 10 Train Loss: 149.34\n",
            "Epoch 16, Iteration 20 Train Loss: 73.00\n",
            "Epoch 16, Iteration 30 Train Loss: 102.59\n",
            "Epoch 16, Iteration 40 Train Loss: 91.01\n",
            "Epoch 16, Iteration 50 Train Loss: 74.50\n",
            "Epoch 16, Iteration 60 Train Loss: 99.82\n",
            "Epoch 16, Iteration 70 Train Loss: 515.79\n",
            "Epoch 16, Iteration 80 Train Loss: 68.15\n",
            "Epoch 16, Iteration 90 Train Loss: 307.59\n",
            "Epoch 16, Iteration 100 Train Loss: 87.65\n",
            "Epoch 16, Iteration 110 Train Loss: 68.16\n",
            "Epoch 16, Iteration 120 Train Loss: 59.57\n",
            "Epoch 16, Iteration 130 Train Loss: 63.54\n",
            "Epoch 16, Iteration 140 Train Loss: 68.62\n",
            "Epoch 16, Iteration 150 Train Loss: 49.33\n",
            "Epoch 16, Iteration 160 Train Loss: 42.57\n",
            "Epoch 16, Iteration 170 Train Loss: 71.88\n",
            "Epoch 16, Iteration 180 Train Loss: 65.91\n",
            "Epoch 16, Iteration 190 Train Loss: 77.79\n",
            "Epoch 16, Iteration 200 Train Loss: 90.13\n",
            "Epoch 16, Iteration 210 Train Loss: 82.39\n",
            "Epoch 16, Iteration 220 Train Loss: 57.14\n",
            "Epoch 16, Iteration 230 Train Loss: 71.36\n",
            "Epoch 16, Iteration 240 Train Loss: 71.11\n",
            "Epoch 16, Iteration 250 Train Loss: 82.15\n",
            "Epoch 16, Iteration 260 Train Loss: 72.54\n",
            "Epoch 16, Iteration 270 Train Loss: 105.42\n",
            "New Epoch! Avg loss for the last 100 iterations: 83.72430191040038\n",
            "Epoch 17, Iteration 0 Train Loss: 18918.17\n",
            "Epoch 17, Iteration 10 Train Loss: 142.05\n",
            "Epoch 17, Iteration 20 Train Loss: 74.92\n",
            "Epoch 17, Iteration 30 Train Loss: 104.50\n",
            "Epoch 17, Iteration 40 Train Loss: 92.91\n",
            "Epoch 17, Iteration 50 Train Loss: 76.40\n",
            "Epoch 17, Iteration 60 Train Loss: 101.71\n",
            "Epoch 17, Iteration 70 Train Loss: 495.22\n",
            "Epoch 17, Iteration 80 Train Loss: 69.99\n",
            "Epoch 17, Iteration 90 Train Loss: 287.14\n",
            "Epoch 17, Iteration 100 Train Loss: 89.48\n",
            "Epoch 17, Iteration 110 Train Loss: 70.00\n",
            "Epoch 17, Iteration 120 Train Loss: 61.41\n",
            "Epoch 17, Iteration 130 Train Loss: 65.35\n",
            "Epoch 17, Iteration 140 Train Loss: 70.39\n",
            "Epoch 17, Iteration 150 Train Loss: 51.09\n",
            "Epoch 17, Iteration 160 Train Loss: 44.32\n",
            "Epoch 17, Iteration 170 Train Loss: 73.63\n",
            "Epoch 17, Iteration 180 Train Loss: 67.65\n",
            "Epoch 17, Iteration 190 Train Loss: 79.52\n",
            "Epoch 17, Iteration 200 Train Loss: 91.75\n",
            "Epoch 17, Iteration 210 Train Loss: 84.11\n",
            "Epoch 17, Iteration 220 Train Loss: 58.75\n",
            "Epoch 17, Iteration 230 Train Loss: 73.04\n",
            "Epoch 17, Iteration 240 Train Loss: 72.78\n",
            "Epoch 17, Iteration 250 Train Loss: 83.82\n",
            "Epoch 17, Iteration 260 Train Loss: 74.19\n",
            "Epoch 17, Iteration 270 Train Loss: 107.09\n",
            "New Epoch! Avg loss for the last 100 iterations: 85.39991432189942\n",
            "Epoch 18, Iteration 0 Train Loss: 18886.51\n",
            "Epoch 18, Iteration 10 Train Loss: 138.59\n",
            "Epoch 18, Iteration 20 Train Loss: 76.62\n",
            "Epoch 18, Iteration 30 Train Loss: 106.20\n",
            "Epoch 18, Iteration 40 Train Loss: 94.60\n",
            "Epoch 18, Iteration 50 Train Loss: 78.08\n",
            "Epoch 18, Iteration 60 Train Loss: 103.39\n",
            "Epoch 18, Iteration 70 Train Loss: 476.89\n",
            "Epoch 18, Iteration 80 Train Loss: 71.67\n",
            "Epoch 18, Iteration 90 Train Loss: 268.31\n",
            "Epoch 18, Iteration 100 Train Loss: 91.14\n",
            "Epoch 18, Iteration 110 Train Loss: 71.66\n",
            "Epoch 18, Iteration 120 Train Loss: 63.08\n",
            "Epoch 18, Iteration 130 Train Loss: 67.02\n",
            "Epoch 18, Iteration 140 Train Loss: 72.05\n",
            "Epoch 18, Iteration 150 Train Loss: 52.72\n",
            "Epoch 18, Iteration 160 Train Loss: 45.92\n",
            "Epoch 18, Iteration 170 Train Loss: 75.20\n",
            "Epoch 18, Iteration 180 Train Loss: 69.20\n",
            "Epoch 18, Iteration 190 Train Loss: 81.06\n",
            "Epoch 18, Iteration 200 Train Loss: 93.27\n",
            "Epoch 18, Iteration 210 Train Loss: 85.64\n",
            "Epoch 18, Iteration 220 Train Loss: 60.27\n",
            "Epoch 18, Iteration 230 Train Loss: 74.56\n",
            "Epoch 18, Iteration 240 Train Loss: 74.29\n",
            "Epoch 18, Iteration 250 Train Loss: 85.33\n",
            "Epoch 18, Iteration 260 Train Loss: 75.67\n",
            "Epoch 18, Iteration 270 Train Loss: 108.60\n",
            "New Epoch! Avg loss for the last 100 iterations: 86.91656005859375\n",
            "Epoch 19, Iteration 0 Train Loss: 18857.78\n",
            "Epoch 19, Iteration 10 Train Loss: 136.59\n",
            "Epoch 19, Iteration 20 Train Loss: 78.17\n",
            "Epoch 19, Iteration 30 Train Loss: 107.75\n",
            "Epoch 19, Iteration 40 Train Loss: 96.16\n",
            "Epoch 19, Iteration 50 Train Loss: 79.63\n",
            "Epoch 19, Iteration 60 Train Loss: 104.95\n",
            "Epoch 19, Iteration 70 Train Loss: 462.90\n",
            "Epoch 19, Iteration 80 Train Loss: 73.26\n",
            "Epoch 19, Iteration 90 Train Loss: 250.82\n",
            "Epoch 19, Iteration 100 Train Loss: 92.73\n",
            "Epoch 19, Iteration 110 Train Loss: 73.23\n",
            "Epoch 19, Iteration 120 Train Loss: 64.65\n",
            "Epoch 19, Iteration 130 Train Loss: 68.57\n",
            "Epoch 19, Iteration 140 Train Loss: 73.56\n",
            "Epoch 19, Iteration 150 Train Loss: 54.22\n",
            "Epoch 19, Iteration 160 Train Loss: 47.42\n",
            "Epoch 19, Iteration 170 Train Loss: 76.67\n",
            "Epoch 19, Iteration 180 Train Loss: 70.65\n",
            "Epoch 19, Iteration 190 Train Loss: 82.51\n",
            "Epoch 19, Iteration 200 Train Loss: 94.70\n",
            "Epoch 19, Iteration 210 Train Loss: 87.07\n",
            "Epoch 19, Iteration 220 Train Loss: 61.69\n",
            "Epoch 19, Iteration 230 Train Loss: 75.99\n",
            "Epoch 19, Iteration 240 Train Loss: 75.72\n",
            "Epoch 19, Iteration 250 Train Loss: 86.75\n",
            "Epoch 19, Iteration 260 Train Loss: 77.07\n",
            "Epoch 19, Iteration 270 Train Loss: 110.01\n",
            "New Epoch! Avg loss for the last 100 iterations: 88.33934368133545\n",
            "Epoch 20, Iteration 0 Train Loss: 18831.30\n",
            "Epoch 20, Iteration 10 Train Loss: 134.39\n",
            "Epoch 20, Iteration 20 Train Loss: 79.58\n",
            "Epoch 20, Iteration 30 Train Loss: 109.15\n",
            "Epoch 20, Iteration 40 Train Loss: 97.55\n",
            "Epoch 20, Iteration 50 Train Loss: 81.02\n",
            "Epoch 20, Iteration 60 Train Loss: 106.33\n",
            "Epoch 20, Iteration 70 Train Loss: 450.29\n",
            "Epoch 20, Iteration 80 Train Loss: 74.65\n",
            "Epoch 20, Iteration 90 Train Loss: 238.09\n",
            "Epoch 20, Iteration 100 Train Loss: 94.12\n",
            "Epoch 20, Iteration 110 Train Loss: 74.61\n",
            "Epoch 20, Iteration 120 Train Loss: 66.02\n",
            "Epoch 20, Iteration 130 Train Loss: 69.92\n",
            "Epoch 20, Iteration 140 Train Loss: 74.91\n",
            "Epoch 20, Iteration 150 Train Loss: 55.56\n",
            "Epoch 20, Iteration 160 Train Loss: 48.73\n",
            "Epoch 20, Iteration 170 Train Loss: 77.98\n",
            "Epoch 20, Iteration 180 Train Loss: 71.95\n",
            "Epoch 20, Iteration 190 Train Loss: 83.80\n",
            "Epoch 20, Iteration 200 Train Loss: 96.08\n",
            "Epoch 20, Iteration 210 Train Loss: 88.36\n",
            "Epoch 20, Iteration 220 Train Loss: 63.07\n",
            "Epoch 20, Iteration 230 Train Loss: 77.27\n",
            "Epoch 20, Iteration 240 Train Loss: 76.99\n",
            "Epoch 20, Iteration 250 Train Loss: 88.03\n",
            "Epoch 20, Iteration 260 Train Loss: 78.43\n",
            "Epoch 20, Iteration 270 Train Loss: 111.30\n",
            "New Epoch! Avg loss for the last 100 iterations: 89.67808940887451\n",
            "Epoch 21, Iteration 0 Train Loss: 18806.51\n",
            "Epoch 21, Iteration 10 Train Loss: 134.92\n",
            "Epoch 21, Iteration 20 Train Loss: 80.90\n",
            "Epoch 21, Iteration 30 Train Loss: 110.45\n",
            "Epoch 21, Iteration 40 Train Loss: 98.83\n",
            "Epoch 21, Iteration 50 Train Loss: 82.30\n",
            "Epoch 21, Iteration 60 Train Loss: 107.61\n",
            "Epoch 21, Iteration 70 Train Loss: 439.04\n",
            "Epoch 21, Iteration 80 Train Loss: 75.94\n",
            "Epoch 21, Iteration 90 Train Loss: 226.74\n",
            "Epoch 21, Iteration 100 Train Loss: 95.42\n",
            "Epoch 21, Iteration 110 Train Loss: 75.90\n",
            "Epoch 21, Iteration 120 Train Loss: 67.30\n",
            "Epoch 21, Iteration 130 Train Loss: 71.19\n",
            "Epoch 21, Iteration 140 Train Loss: 76.18\n",
            "Epoch 21, Iteration 150 Train Loss: 56.83\n",
            "Epoch 21, Iteration 160 Train Loss: 50.02\n",
            "Epoch 21, Iteration 170 Train Loss: 79.27\n",
            "Epoch 21, Iteration 180 Train Loss: 73.25\n",
            "Epoch 21, Iteration 190 Train Loss: 85.10\n",
            "Epoch 21, Iteration 200 Train Loss: 97.35\n",
            "Epoch 21, Iteration 210 Train Loss: 89.65\n",
            "Epoch 21, Iteration 220 Train Loss: 64.33\n",
            "Epoch 21, Iteration 230 Train Loss: 78.55\n",
            "Epoch 21, Iteration 240 Train Loss: 78.27\n",
            "Epoch 21, Iteration 250 Train Loss: 89.30\n",
            "Epoch 21, Iteration 260 Train Loss: 79.68\n",
            "Epoch 21, Iteration 270 Train Loss: 112.58\n",
            "New Epoch! Avg loss for the last 100 iterations: 90.94488876342774\n",
            "Epoch 22, Iteration 0 Train Loss: 18782.26\n",
            "Epoch 22, Iteration 10 Train Loss: 136.13\n",
            "Epoch 22, Iteration 20 Train Loss: 82.20\n",
            "Epoch 22, Iteration 30 Train Loss: 111.75\n",
            "Epoch 22, Iteration 40 Train Loss: 100.13\n",
            "Epoch 22, Iteration 50 Train Loss: 83.60\n",
            "Epoch 22, Iteration 60 Train Loss: 108.91\n",
            "Epoch 22, Iteration 70 Train Loss: 427.59\n",
            "Epoch 22, Iteration 80 Train Loss: 77.25\n",
            "Epoch 22, Iteration 90 Train Loss: 215.21\n",
            "Epoch 22, Iteration 100 Train Loss: 96.74\n",
            "Epoch 22, Iteration 110 Train Loss: 77.22\n",
            "Epoch 22, Iteration 120 Train Loss: 68.62\n",
            "Epoch 22, Iteration 130 Train Loss: 72.52\n",
            "Epoch 22, Iteration 140 Train Loss: 77.50\n",
            "Epoch 22, Iteration 150 Train Loss: 58.15\n",
            "Epoch 22, Iteration 160 Train Loss: 51.34\n",
            "Epoch 22, Iteration 170 Train Loss: 80.59\n",
            "Epoch 22, Iteration 180 Train Loss: 74.56\n",
            "Epoch 22, Iteration 190 Train Loss: 86.41\n",
            "Epoch 22, Iteration 200 Train Loss: 98.63\n",
            "Epoch 22, Iteration 210 Train Loss: 90.95\n",
            "Epoch 22, Iteration 220 Train Loss: 65.61\n",
            "Epoch 22, Iteration 230 Train Loss: 79.86\n",
            "Epoch 22, Iteration 240 Train Loss: 79.57\n",
            "Epoch 22, Iteration 250 Train Loss: 90.60\n",
            "Epoch 22, Iteration 260 Train Loss: 80.94\n",
            "Epoch 22, Iteration 270 Train Loss: 113.86\n",
            "New Epoch! Avg loss for the last 100 iterations: 92.23028152465821\n",
            "Epoch 23, Iteration 0 Train Loss: 18758.19\n",
            "Epoch 23, Iteration 10 Train Loss: 137.32\n",
            "Epoch 23, Iteration 20 Train Loss: 83.47\n",
            "Epoch 23, Iteration 30 Train Loss: 113.02\n",
            "Epoch 23, Iteration 40 Train Loss: 101.40\n",
            "Epoch 23, Iteration 50 Train Loss: 84.87\n",
            "Epoch 23, Iteration 60 Train Loss: 110.18\n",
            "Epoch 23, Iteration 70 Train Loss: 415.95\n",
            "Epoch 23, Iteration 80 Train Loss: 78.48\n",
            "Epoch 23, Iteration 90 Train Loss: 203.88\n",
            "Epoch 23, Iteration 100 Train Loss: 97.92\n",
            "Epoch 23, Iteration 110 Train Loss: 78.41\n",
            "Epoch 23, Iteration 120 Train Loss: 69.76\n",
            "Epoch 23, Iteration 130 Train Loss: 73.64\n",
            "Epoch 23, Iteration 140 Train Loss: 78.62\n",
            "Epoch 23, Iteration 150 Train Loss: 59.26\n",
            "Epoch 23, Iteration 160 Train Loss: 52.43\n",
            "Epoch 23, Iteration 170 Train Loss: 81.66\n",
            "Epoch 23, Iteration 180 Train Loss: 75.63\n",
            "Epoch 23, Iteration 190 Train Loss: 87.47\n",
            "Epoch 23, Iteration 200 Train Loss: 99.77\n",
            "Epoch 23, Iteration 210 Train Loss: 92.01\n",
            "Epoch 23, Iteration 220 Train Loss: 66.74\n",
            "Epoch 23, Iteration 230 Train Loss: 80.93\n",
            "Epoch 23, Iteration 240 Train Loss: 80.66\n",
            "Epoch 23, Iteration 250 Train Loss: 91.69\n",
            "Epoch 23, Iteration 260 Train Loss: 82.00\n",
            "Epoch 23, Iteration 270 Train Loss: 114.95\n",
            "New Epoch! Avg loss for the last 100 iterations: 93.30413520812988\n",
            "Epoch 24, Iteration 0 Train Loss: 18737.55\n",
            "Epoch 24, Iteration 10 Train Loss: 138.75\n",
            "Epoch 24, Iteration 20 Train Loss: 84.57\n",
            "Epoch 24, Iteration 30 Train Loss: 114.13\n",
            "Epoch 24, Iteration 40 Train Loss: 102.51\n",
            "Epoch 24, Iteration 50 Train Loss: 85.97\n",
            "Epoch 24, Iteration 60 Train Loss: 111.26\n",
            "Epoch 24, Iteration 70 Train Loss: 406.19\n",
            "Epoch 24, Iteration 80 Train Loss: 79.58\n",
            "Epoch 24, Iteration 90 Train Loss: 194.02\n",
            "Epoch 24, Iteration 100 Train Loss: 99.04\n",
            "Epoch 24, Iteration 110 Train Loss: 79.54\n",
            "Epoch 24, Iteration 120 Train Loss: 70.89\n",
            "Epoch 24, Iteration 130 Train Loss: 74.77\n",
            "Epoch 24, Iteration 140 Train Loss: 79.75\n",
            "Epoch 24, Iteration 150 Train Loss: 60.38\n",
            "Epoch 24, Iteration 160 Train Loss: 53.55\n",
            "Epoch 24, Iteration 170 Train Loss: 82.79\n",
            "Epoch 24, Iteration 180 Train Loss: 76.76\n",
            "Epoch 24, Iteration 190 Train Loss: 88.59\n",
            "Epoch 24, Iteration 200 Train Loss: 100.86\n",
            "Epoch 24, Iteration 210 Train Loss: 93.13\n",
            "Epoch 24, Iteration 220 Train Loss: 67.83\n",
            "Epoch 24, Iteration 230 Train Loss: 82.02\n",
            "Epoch 24, Iteration 240 Train Loss: 81.73\n",
            "Epoch 24, Iteration 250 Train Loss: 92.76\n",
            "Epoch 24, Iteration 260 Train Loss: 83.14\n",
            "Epoch 24, Iteration 270 Train Loss: 116.03\n",
            "New Epoch! Avg loss for the last 100 iterations: 94.43074188232421\n",
            "Oracle activations: 13933\n",
            "Runtime total_E activations: 13178.756812865347\n",
            "Runtime total_E_pred activations: 118650.44013800309\n",
            "Naive total_E activations: 13178.756812865347\n",
            "Naive total_E_pred activations: 5153.631019665272\n",
            "Dataset, train set, and test set size: 2945 2209 368\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 0.0\n",
            "Maximum possible activations: 13933\n",
            "Predicted activations: 119355\n",
            "Successful activations: 0, 0.000%\n",
            "Failed activations: 96315, 80.696%\n",
            "Missed activations: 13933, 100.000%\n",
            "Naive predicted activations (usual actual energy average): 13729\n",
            "Naive successful activations (usual actual energy average): 12324, 89.766%\n",
            "Naive failed activations (usual actual energy average): 1405, 10.234%\n",
            "Naive missed activations (usual actual energy average): 1609, 11.548%\n",
            "Voltage overestimation rate: 100.000%\n",
            "Test MAPE power: 1.602852\n",
            "Test MAPE voltage: 3.040679\n",
            "Test MAPE current: 0.341850\n",
            "Epoch 0, Iteration 0 Train Loss: 22024.08\n",
            "Epoch 0, Iteration 10 Train Loss: 2211.14\n",
            "Epoch 0, Iteration 20 Train Loss: 1742.48\n",
            "Epoch 0, Iteration 30 Train Loss: 1185.24\n",
            "Epoch 0, Iteration 40 Train Loss: 1396.58\n",
            "Epoch 0, Iteration 50 Train Loss: 1694.60\n",
            "Epoch 0, Iteration 60 Train Loss: 1200.09\n",
            "Epoch 0, Iteration 70 Train Loss: 2913.30\n",
            "Epoch 0, Iteration 80 Train Loss: 1750.86\n",
            "Epoch 0, Iteration 90 Train Loss: 2106.54\n",
            "Epoch 0, Iteration 100 Train Loss: 1180.15\n",
            "Epoch 0, Iteration 110 Train Loss: 1415.75\n",
            "Epoch 0, Iteration 120 Train Loss: 1527.51\n",
            "Epoch 0, Iteration 130 Train Loss: 1401.37\n",
            "Epoch 0, Iteration 140 Train Loss: 1255.43\n",
            "Epoch 0, Iteration 150 Train Loss: 1581.50\n",
            "Epoch 0, Iteration 160 Train Loss: 1670.04\n",
            "Epoch 0, Iteration 170 Train Loss: 1097.49\n",
            "Epoch 0, Iteration 180 Train Loss: 1192.50\n",
            "Epoch 0, Iteration 190 Train Loss: 941.68\n",
            "Epoch 0, Iteration 200 Train Loss: 795.47\n",
            "Epoch 0, Iteration 210 Train Loss: 806.25\n",
            "Epoch 0, Iteration 220 Train Loss: 1204.48\n",
            "Epoch 0, Iteration 230 Train Loss: 913.83\n",
            "Epoch 0, Iteration 240 Train Loss: 886.45\n",
            "Epoch 0, Iteration 250 Train Loss: 697.89\n",
            "Epoch 0, Iteration 260 Train Loss: 841.16\n",
            "Epoch 0, Iteration 270 Train Loss: 303.87\n",
            "Epoch 0, Iteration 280 Train Loss: 534.38\n",
            "Epoch 0, Iteration 290 Train Loss: 414.95\n",
            "Epoch 0, Iteration 300 Train Loss: 338.90\n",
            "Epoch 0, Iteration 310 Train Loss: 476.48\n",
            "Epoch 0, Iteration 320 Train Loss: 531.02\n",
            "Epoch 0, Iteration 330 Train Loss: 266.28\n",
            "Epoch 0, Iteration 340 Train Loss: 437.68\n",
            "Epoch 0, Iteration 350 Train Loss: 715.75\n",
            "Epoch 0, Iteration 360 Train Loss: 589.79\n",
            "New Epoch! Avg loss for the last 100 iterations: 470.7643646240234\n",
            "Epoch 1, Iteration 0 Train Loss: 20849.75\n",
            "Epoch 1, Iteration 10 Train Loss: 1066.46\n",
            "Epoch 1, Iteration 20 Train Loss: 574.05\n",
            "Epoch 1, Iteration 30 Train Loss: 224.00\n",
            "Epoch 1, Iteration 40 Train Loss: 385.86\n",
            "Epoch 1, Iteration 50 Train Loss: 496.72\n",
            "Epoch 1, Iteration 60 Train Loss: 362.66\n",
            "Epoch 1, Iteration 70 Train Loss: 1757.44\n",
            "Epoch 1, Iteration 80 Train Loss: 549.07\n",
            "Epoch 1, Iteration 90 Train Loss: 1221.92\n",
            "Epoch 1, Iteration 100 Train Loss: 333.79\n",
            "Epoch 1, Iteration 110 Train Loss: 492.26\n",
            "Epoch 1, Iteration 120 Train Loss: 829.40\n",
            "Epoch 1, Iteration 130 Train Loss: 631.87\n",
            "Epoch 1, Iteration 140 Train Loss: 425.07\n",
            "Epoch 1, Iteration 150 Train Loss: 803.05\n",
            "Epoch 1, Iteration 160 Train Loss: 807.60\n",
            "Epoch 1, Iteration 170 Train Loss: 312.32\n",
            "Epoch 1, Iteration 180 Train Loss: 526.60\n",
            "Epoch 1, Iteration 190 Train Loss: 202.97\n",
            "Epoch 1, Iteration 200 Train Loss: 272.30\n",
            "Epoch 1, Iteration 210 Train Loss: 148.41\n",
            "Epoch 1, Iteration 220 Train Loss: 441.87\n",
            "Epoch 1, Iteration 230 Train Loss: 188.09\n",
            "Epoch 1, Iteration 240 Train Loss: 171.07\n",
            "Epoch 1, Iteration 250 Train Loss: 182.20\n",
            "Epoch 1, Iteration 260 Train Loss: 204.51\n",
            "Epoch 1, Iteration 270 Train Loss: 39.06\n",
            "Epoch 1, Iteration 280 Train Loss: 201.30\n",
            "Epoch 1, Iteration 290 Train Loss: 121.91\n",
            "Epoch 1, Iteration 300 Train Loss: 43.25\n",
            "Epoch 1, Iteration 310 Train Loss: 29.26\n",
            "Epoch 1, Iteration 320 Train Loss: 97.67\n",
            "Epoch 1, Iteration 330 Train Loss: 29.11\n",
            "Epoch 1, Iteration 340 Train Loss: 129.39\n",
            "Epoch 1, Iteration 350 Train Loss: 159.22\n",
            "Epoch 1, Iteration 360 Train Loss: 122.18\n",
            "New Epoch! Avg loss for the last 100 iterations: 117.50340503692627\n",
            "Epoch 2, Iteration 0 Train Loss: 20215.32\n",
            "Epoch 2, Iteration 10 Train Loss: 609.51\n",
            "Epoch 2, Iteration 20 Train Loss: 107.90\n",
            "Epoch 2, Iteration 30 Train Loss: 34.72\n",
            "Epoch 2, Iteration 40 Train Loss: 123.32\n",
            "Epoch 2, Iteration 50 Train Loss: 75.10\n",
            "Epoch 2, Iteration 60 Train Loss: 210.11\n",
            "Epoch 2, Iteration 70 Train Loss: 1430.20\n",
            "Epoch 2, Iteration 80 Train Loss: 214.42\n",
            "Epoch 2, Iteration 90 Train Loss: 984.80\n",
            "Epoch 2, Iteration 100 Train Loss: 119.32\n",
            "Epoch 2, Iteration 110 Train Loss: 77.28\n",
            "Epoch 2, Iteration 120 Train Loss: 575.17\n",
            "Epoch 2, Iteration 130 Train Loss: 387.93\n",
            "Epoch 2, Iteration 140 Train Loss: 53.34\n",
            "Epoch 2, Iteration 150 Train Loss: 549.12\n",
            "Epoch 2, Iteration 160 Train Loss: 477.02\n",
            "Epoch 2, Iteration 170 Train Loss: 54.00\n",
            "Epoch 2, Iteration 180 Train Loss: 364.40\n",
            "Epoch 2, Iteration 190 Train Loss: 19.56\n",
            "Epoch 2, Iteration 200 Train Loss: 113.56\n",
            "Epoch 2, Iteration 210 Train Loss: 30.36\n",
            "Epoch 2, Iteration 220 Train Loss: 108.65\n",
            "Epoch 2, Iteration 230 Train Loss: 38.82\n",
            "Epoch 2, Iteration 240 Train Loss: 28.48\n",
            "Epoch 2, Iteration 250 Train Loss: 74.05\n",
            "Epoch 2, Iteration 260 Train Loss: 68.01\n",
            "Epoch 2, Iteration 270 Train Loss: 51.11\n",
            "Epoch 2, Iteration 280 Train Loss: 70.17\n",
            "Epoch 2, Iteration 290 Train Loss: 69.78\n",
            "Epoch 2, Iteration 300 Train Loss: 44.99\n",
            "Epoch 2, Iteration 310 Train Loss: 34.77\n",
            "Epoch 2, Iteration 320 Train Loss: 30.03\n",
            "Epoch 2, Iteration 330 Train Loss: 45.55\n",
            "Epoch 2, Iteration 340 Train Loss: 57.75\n",
            "Epoch 2, Iteration 350 Train Loss: 14.34\n",
            "Epoch 2, Iteration 360 Train Loss: 21.35\n",
            "New Epoch! Avg loss for the last 100 iterations: 68.20753191947937\n",
            "Epoch 3, Iteration 0 Train Loss: 19927.43\n",
            "Epoch 3, Iteration 10 Train Loss: 496.93\n",
            "Epoch 3, Iteration 20 Train Loss: 24.36\n",
            "Epoch 3, Iteration 30 Train Loss: 49.72\n",
            "Epoch 3, Iteration 40 Train Loss: 38.44\n",
            "Epoch 3, Iteration 50 Train Loss: 28.65\n",
            "Epoch 3, Iteration 60 Train Loss: 128.81\n",
            "Epoch 3, Iteration 70 Train Loss: 1265.41\n",
            "Epoch 3, Iteration 80 Train Loss: 71.19\n",
            "Epoch 3, Iteration 90 Train Loss: 834.68\n",
            "Epoch 3, Iteration 100 Train Loss: 51.08\n",
            "Epoch 3, Iteration 110 Train Loss: 18.00\n",
            "Epoch 3, Iteration 120 Train Loss: 427.87\n",
            "Epoch 3, Iteration 130 Train Loss: 258.43\n",
            "Epoch 3, Iteration 140 Train Loss: 20.02\n",
            "Epoch 3, Iteration 150 Train Loss: 406.82\n",
            "Epoch 3, Iteration 160 Train Loss: 334.52\n",
            "Epoch 3, Iteration 170 Train Loss: 28.12\n",
            "Epoch 3, Iteration 180 Train Loss: 285.39\n",
            "Epoch 3, Iteration 190 Train Loss: 31.60\n",
            "Epoch 3, Iteration 200 Train Loss: 58.59\n",
            "Epoch 3, Iteration 210 Train Loss: 36.93\n",
            "Epoch 3, Iteration 220 Train Loss: 29.96\n",
            "Epoch 3, Iteration 230 Train Loss: 26.96\n",
            "Epoch 3, Iteration 240 Train Loss: 26.47\n",
            "Epoch 3, Iteration 250 Train Loss: 49.85\n",
            "Epoch 3, Iteration 260 Train Loss: 41.31\n",
            "Epoch 3, Iteration 270 Train Loss: 61.28\n",
            "Epoch 3, Iteration 280 Train Loss: 49.39\n",
            "Epoch 3, Iteration 290 Train Loss: 67.01\n",
            "Epoch 3, Iteration 300 Train Loss: 54.72\n",
            "Epoch 3, Iteration 310 Train Loss: 44.41\n",
            "Epoch 3, Iteration 320 Train Loss: 39.54\n",
            "Epoch 3, Iteration 330 Train Loss: 54.96\n",
            "Epoch 3, Iteration 340 Train Loss: 60.72\n",
            "Epoch 3, Iteration 350 Train Loss: 23.54\n",
            "Epoch 3, Iteration 360 Train Loss: 30.43\n",
            "New Epoch! Avg loss for the last 100 iterations: 64.84590562820435\n",
            "Epoch 4, Iteration 0 Train Loss: 19755.93\n",
            "Epoch 4, Iteration 10 Train Loss: 426.35\n",
            "Epoch 4, Iteration 20 Train Loss: 29.12\n",
            "Epoch 4, Iteration 30 Train Loss: 58.79\n",
            "Epoch 4, Iteration 40 Train Loss: 47.41\n",
            "Epoch 4, Iteration 50 Train Loss: 31.15\n",
            "Epoch 4, Iteration 60 Train Loss: 86.25\n",
            "Epoch 4, Iteration 70 Train Loss: 1143.83\n",
            "Epoch 4, Iteration 80 Train Loss: 25.23\n",
            "Epoch 4, Iteration 90 Train Loss: 752.27\n",
            "Epoch 4, Iteration 100 Train Loss: 45.23\n",
            "Epoch 4, Iteration 110 Train Loss: 26.07\n",
            "Epoch 4, Iteration 120 Train Loss: 331.63\n",
            "Epoch 4, Iteration 130 Train Loss: 190.05\n",
            "Epoch 4, Iteration 140 Train Loss: 27.51\n",
            "Epoch 4, Iteration 150 Train Loss: 299.95\n",
            "Epoch 4, Iteration 160 Train Loss: 227.69\n",
            "Epoch 4, Iteration 170 Train Loss: 32.02\n",
            "Epoch 4, Iteration 180 Train Loss: 224.07\n",
            "Epoch 4, Iteration 190 Train Loss: 38.66\n",
            "Epoch 4, Iteration 200 Train Loss: 58.46\n",
            "Epoch 4, Iteration 210 Train Loss: 43.76\n",
            "Epoch 4, Iteration 220 Train Loss: 26.29\n",
            "Epoch 4, Iteration 230 Train Loss: 33.08\n",
            "Epoch 4, Iteration 240 Train Loss: 33.08\n",
            "Epoch 4, Iteration 250 Train Loss: 50.24\n",
            "Epoch 4, Iteration 260 Train Loss: 40.79\n",
            "Epoch 4, Iteration 270 Train Loss: 67.74\n",
            "Epoch 4, Iteration 280 Train Loss: 54.69\n",
            "Epoch 4, Iteration 290 Train Loss: 68.43\n",
            "Epoch 4, Iteration 300 Train Loss: 60.91\n",
            "Epoch 4, Iteration 310 Train Loss: 50.53\n",
            "Epoch 4, Iteration 320 Train Loss: 45.62\n",
            "Epoch 4, Iteration 330 Train Loss: 61.08\n",
            "Epoch 4, Iteration 340 Train Loss: 64.50\n",
            "Epoch 4, Iteration 350 Train Loss: 29.61\n",
            "Epoch 4, Iteration 360 Train Loss: 36.44\n",
            "New Epoch! Avg loss for the last 100 iterations: 65.66939130783081\n",
            "Epoch 5, Iteration 0 Train Loss: 19642.65\n",
            "Epoch 5, Iteration 10 Train Loss: 378.27\n",
            "Epoch 5, Iteration 20 Train Loss: 35.35\n",
            "Epoch 5, Iteration 30 Train Loss: 65.01\n",
            "Epoch 5, Iteration 40 Train Loss: 53.58\n",
            "Epoch 5, Iteration 50 Train Loss: 37.29\n",
            "Epoch 5, Iteration 60 Train Loss: 71.26\n",
            "Epoch 5, Iteration 70 Train Loss: 1049.27\n",
            "Epoch 5, Iteration 80 Train Loss: 31.32\n",
            "Epoch 5, Iteration 90 Train Loss: 692.82\n",
            "Epoch 5, Iteration 100 Train Loss: 51.21\n",
            "Epoch 5, Iteration 110 Train Loss: 31.97\n",
            "Epoch 5, Iteration 120 Train Loss: 269.64\n",
            "Epoch 5, Iteration 130 Train Loss: 145.40\n",
            "Epoch 5, Iteration 140 Train Loss: 33.38\n",
            "Epoch 5, Iteration 150 Train Loss: 230.85\n",
            "Epoch 5, Iteration 160 Train Loss: 157.00\n",
            "Epoch 5, Iteration 170 Train Loss: 37.65\n",
            "Epoch 5, Iteration 180 Train Loss: 185.43\n",
            "Epoch 5, Iteration 190 Train Loss: 44.18\n",
            "Epoch 5, Iteration 200 Train Loss: 59.74\n",
            "Epoch 5, Iteration 210 Train Loss: 49.20\n",
            "Epoch 5, Iteration 220 Train Loss: 26.77\n",
            "Epoch 5, Iteration 230 Train Loss: 38.37\n",
            "Epoch 5, Iteration 240 Train Loss: 38.28\n",
            "Epoch 5, Iteration 250 Train Loss: 51.85\n",
            "Epoch 5, Iteration 260 Train Loss: 42.43\n",
            "Epoch 5, Iteration 270 Train Loss: 72.79\n",
            "Epoch 5, Iteration 280 Train Loss: 59.71\n",
            "Epoch 5, Iteration 290 Train Loss: 71.91\n",
            "Epoch 5, Iteration 300 Train Loss: 65.88\n",
            "Epoch 5, Iteration 310 Train Loss: 55.47\n",
            "Epoch 5, Iteration 320 Train Loss: 50.55\n",
            "Epoch 5, Iteration 330 Train Loss: 66.00\n",
            "Epoch 5, Iteration 340 Train Loss: 68.08\n",
            "Epoch 5, Iteration 350 Train Loss: 34.45\n",
            "Epoch 5, Iteration 360 Train Loss: 41.25\n",
            "New Epoch! Avg loss for the last 100 iterations: 67.18043872833252\n",
            "Epoch 6, Iteration 0 Train Loss: 19551.73\n",
            "Epoch 6, Iteration 10 Train Loss: 338.61\n",
            "Epoch 6, Iteration 20 Train Loss: 40.29\n",
            "Epoch 6, Iteration 30 Train Loss: 69.91\n",
            "Epoch 6, Iteration 40 Train Loss: 58.45\n",
            "Epoch 6, Iteration 50 Train Loss: 42.11\n",
            "Epoch 6, Iteration 60 Train Loss: 67.55\n",
            "Epoch 6, Iteration 70 Train Loss: 970.67\n",
            "Epoch 6, Iteration 80 Train Loss: 36.04\n",
            "Epoch 6, Iteration 90 Train Loss: 646.25\n",
            "Epoch 6, Iteration 100 Train Loss: 55.84\n",
            "Epoch 6, Iteration 110 Train Loss: 36.56\n",
            "Epoch 6, Iteration 120 Train Loss: 223.72\n",
            "Epoch 6, Iteration 130 Train Loss: 115.87\n",
            "Epoch 6, Iteration 140 Train Loss: 37.88\n",
            "Epoch 6, Iteration 150 Train Loss: 185.92\n",
            "Epoch 6, Iteration 160 Train Loss: 109.39\n",
            "Epoch 6, Iteration 170 Train Loss: 42.02\n",
            "Epoch 6, Iteration 180 Train Loss: 150.78\n",
            "Epoch 6, Iteration 190 Train Loss: 48.48\n",
            "Epoch 6, Iteration 200 Train Loss: 63.20\n",
            "Epoch 6, Iteration 210 Train Loss: 53.45\n",
            "Epoch 6, Iteration 220 Train Loss: 30.23\n",
            "Epoch 6, Iteration 230 Train Loss: 42.59\n",
            "Epoch 6, Iteration 240 Train Loss: 42.46\n",
            "Epoch 6, Iteration 250 Train Loss: 54.80\n",
            "Epoch 6, Iteration 260 Train Loss: 45.96\n",
            "Epoch 6, Iteration 270 Train Loss: 76.87\n",
            "Epoch 6, Iteration 280 Train Loss: 63.74\n",
            "Epoch 6, Iteration 290 Train Loss: 75.52\n",
            "Epoch 6, Iteration 300 Train Loss: 69.85\n",
            "Epoch 6, Iteration 310 Train Loss: 59.41\n",
            "Epoch 6, Iteration 320 Train Loss: 54.46\n",
            "Epoch 6, Iteration 330 Train Loss: 69.92\n",
            "Epoch 6, Iteration 340 Train Loss: 71.79\n",
            "Epoch 6, Iteration 350 Train Loss: 38.35\n",
            "Epoch 6, Iteration 360 Train Loss: 45.12\n",
            "New Epoch! Avg loss for the last 100 iterations: 68.53338096618653\n",
            "Epoch 7, Iteration 0 Train Loss: 19478.43\n",
            "Epoch 7, Iteration 10 Train Loss: 305.28\n",
            "Epoch 7, Iteration 20 Train Loss: 44.32\n",
            "Epoch 7, Iteration 30 Train Loss: 73.96\n",
            "Epoch 7, Iteration 40 Train Loss: 62.49\n",
            "Epoch 7, Iteration 50 Train Loss: 46.13\n",
            "Epoch 7, Iteration 60 Train Loss: 71.56\n",
            "Epoch 7, Iteration 70 Train Loss: 898.36\n",
            "Epoch 7, Iteration 80 Train Loss: 40.04\n",
            "Epoch 7, Iteration 90 Train Loss: 602.74\n",
            "Epoch 7, Iteration 100 Train Loss: 59.82\n",
            "Epoch 7, Iteration 110 Train Loss: 40.51\n",
            "Epoch 7, Iteration 120 Train Loss: 188.46\n",
            "Epoch 7, Iteration 130 Train Loss: 91.37\n",
            "Epoch 7, Iteration 140 Train Loss: 41.78\n",
            "Epoch 7, Iteration 150 Train Loss: 152.07\n",
            "Epoch 7, Iteration 160 Train Loss: 76.75\n",
            "Epoch 7, Iteration 170 Train Loss: 45.78\n",
            "Epoch 7, Iteration 180 Train Loss: 118.36\n",
            "Epoch 7, Iteration 190 Train Loss: 52.17\n",
            "Epoch 7, Iteration 200 Train Loss: 66.96\n",
            "Epoch 7, Iteration 210 Train Loss: 57.10\n",
            "Epoch 7, Iteration 220 Train Loss: 33.98\n",
            "Epoch 7, Iteration 230 Train Loss: 46.23\n",
            "Epoch 7, Iteration 240 Train Loss: 46.09\n",
            "Epoch 7, Iteration 250 Train Loss: 58.51\n",
            "Epoch 7, Iteration 260 Train Loss: 49.54\n",
            "Epoch 7, Iteration 270 Train Loss: 80.58\n",
            "Epoch 7, Iteration 280 Train Loss: 67.47\n",
            "Epoch 7, Iteration 290 Train Loss: 78.96\n",
            "Epoch 7, Iteration 300 Train Loss: 73.55\n",
            "Epoch 7, Iteration 310 Train Loss: 63.10\n",
            "Epoch 7, Iteration 320 Train Loss: 58.12\n",
            "Epoch 7, Iteration 330 Train Loss: 73.55\n",
            "Epoch 7, Iteration 340 Train Loss: 75.24\n",
            "Epoch 7, Iteration 350 Train Loss: 41.93\n",
            "Epoch 7, Iteration 360 Train Loss: 48.68\n",
            "New Epoch! Avg loss for the last 100 iterations: 69.9481734085083\n",
            "Epoch 8, Iteration 0 Train Loss: 19411.17\n",
            "Epoch 8, Iteration 10 Train Loss: 282.18\n",
            "Epoch 8, Iteration 20 Train Loss: 48.02\n",
            "Epoch 8, Iteration 30 Train Loss: 77.67\n",
            "Epoch 8, Iteration 40 Train Loss: 66.19\n",
            "Epoch 8, Iteration 50 Train Loss: 49.81\n",
            "Epoch 8, Iteration 60 Train Loss: 75.23\n",
            "Epoch 8, Iteration 70 Train Loss: 831.24\n",
            "Epoch 8, Iteration 80 Train Loss: 43.70\n",
            "Epoch 8, Iteration 90 Train Loss: 562.03\n",
            "Epoch 8, Iteration 100 Train Loss: 63.44\n",
            "Epoch 8, Iteration 110 Train Loss: 44.11\n",
            "Epoch 8, Iteration 120 Train Loss: 156.72\n",
            "Epoch 8, Iteration 130 Train Loss: 72.02\n",
            "Epoch 8, Iteration 140 Train Loss: 45.29\n",
            "Epoch 8, Iteration 150 Train Loss: 120.80\n",
            "Epoch 8, Iteration 160 Train Loss: 54.18\n",
            "Epoch 8, Iteration 170 Train Loss: 49.25\n",
            "Epoch 8, Iteration 180 Train Loss: 95.86\n",
            "Epoch 8, Iteration 190 Train Loss: 55.58\n",
            "Epoch 8, Iteration 200 Train Loss: 70.51\n",
            "Epoch 8, Iteration 210 Train Loss: 60.48\n",
            "Epoch 8, Iteration 220 Train Loss: 37.48\n",
            "Epoch 8, Iteration 230 Train Loss: 49.63\n",
            "Epoch 8, Iteration 240 Train Loss: 49.48\n",
            "Epoch 8, Iteration 250 Train Loss: 61.92\n",
            "Epoch 8, Iteration 260 Train Loss: 52.87\n",
            "Epoch 8, Iteration 270 Train Loss: 84.06\n",
            "Epoch 8, Iteration 280 Train Loss: 70.95\n",
            "Epoch 8, Iteration 290 Train Loss: 82.16\n",
            "Epoch 8, Iteration 300 Train Loss: 77.02\n",
            "Epoch 8, Iteration 310 Train Loss: 66.55\n",
            "Epoch 8, Iteration 320 Train Loss: 61.54\n",
            "Epoch 8, Iteration 330 Train Loss: 76.86\n",
            "Epoch 8, Iteration 340 Train Loss: 78.33\n",
            "Epoch 8, Iteration 350 Train Loss: 45.12\n",
            "Epoch 8, Iteration 360 Train Loss: 51.84\n",
            "New Epoch! Avg loss for the last 100 iterations: 71.8531103515625\n",
            "Epoch 9, Iteration 0 Train Loss: 19351.45\n",
            "Epoch 9, Iteration 10 Train Loss: 261.73\n",
            "Epoch 9, Iteration 20 Train Loss: 51.33\n",
            "Epoch 9, Iteration 30 Train Loss: 81.01\n",
            "Epoch 9, Iteration 40 Train Loss: 69.53\n",
            "Epoch 9, Iteration 50 Train Loss: 53.13\n",
            "Epoch 9, Iteration 60 Train Loss: 78.54\n",
            "Epoch 9, Iteration 70 Train Loss: 774.78\n",
            "Epoch 9, Iteration 80 Train Loss: 47.00\n",
            "Epoch 9, Iteration 90 Train Loss: 525.24\n",
            "Epoch 9, Iteration 100 Train Loss: 66.72\n",
            "Epoch 9, Iteration 110 Train Loss: 47.37\n",
            "Epoch 9, Iteration 120 Train Loss: 132.25\n",
            "Epoch 9, Iteration 130 Train Loss: 59.24\n",
            "Epoch 9, Iteration 140 Train Loss: 48.52\n",
            "Epoch 9, Iteration 150 Train Loss: 97.27\n",
            "Epoch 9, Iteration 160 Train Loss: 40.87\n",
            "Epoch 9, Iteration 170 Train Loss: 52.38\n",
            "Epoch 9, Iteration 180 Train Loss: 80.94\n",
            "Epoch 9, Iteration 190 Train Loss: 58.63\n",
            "Epoch 9, Iteration 200 Train Loss: 73.66\n",
            "Epoch 9, Iteration 210 Train Loss: 63.48\n",
            "Epoch 9, Iteration 220 Train Loss: 40.61\n",
            "Epoch 9, Iteration 230 Train Loss: 52.62\n",
            "Epoch 9, Iteration 240 Train Loss: 52.49\n",
            "Epoch 9, Iteration 250 Train Loss: 64.83\n",
            "Epoch 9, Iteration 260 Train Loss: 55.78\n",
            "Epoch 9, Iteration 270 Train Loss: 87.14\n",
            "Epoch 9, Iteration 280 Train Loss: 74.03\n",
            "Epoch 9, Iteration 290 Train Loss: 85.01\n",
            "Epoch 9, Iteration 300 Train Loss: 80.07\n",
            "Epoch 9, Iteration 310 Train Loss: 69.59\n",
            "Epoch 9, Iteration 320 Train Loss: 64.56\n",
            "Epoch 9, Iteration 330 Train Loss: 79.80\n",
            "Epoch 9, Iteration 340 Train Loss: 81.07\n",
            "Epoch 9, Iteration 350 Train Loss: 47.96\n",
            "Epoch 9, Iteration 360 Train Loss: 54.66\n",
            "New Epoch! Avg loss for the last 100 iterations: 74.11038013458251\n",
            "Epoch 10, Iteration 0 Train Loss: 19298.31\n",
            "Epoch 10, Iteration 10 Train Loss: 243.57\n",
            "Epoch 10, Iteration 20 Train Loss: 54.26\n",
            "Epoch 10, Iteration 30 Train Loss: 83.94\n",
            "Epoch 10, Iteration 40 Train Loss: 72.44\n",
            "Epoch 10, Iteration 50 Train Loss: 56.02\n",
            "Epoch 10, Iteration 60 Train Loss: 81.43\n",
            "Epoch 10, Iteration 70 Train Loss: 728.61\n",
            "Epoch 10, Iteration 80 Train Loss: 49.91\n",
            "Epoch 10, Iteration 90 Train Loss: 492.69\n",
            "Epoch 10, Iteration 100 Train Loss: 69.61\n",
            "Epoch 10, Iteration 110 Train Loss: 50.24\n",
            "Epoch 10, Iteration 120 Train Loss: 114.09\n",
            "Epoch 10, Iteration 130 Train Loss: 50.47\n",
            "Epoch 10, Iteration 140 Train Loss: 51.30\n",
            "Epoch 10, Iteration 150 Train Loss: 79.49\n",
            "Epoch 10, Iteration 160 Train Loss: 33.12\n",
            "Epoch 10, Iteration 170 Train Loss: 55.09\n",
            "Epoch 10, Iteration 180 Train Loss: 70.15\n",
            "Epoch 10, Iteration 190 Train Loss: 61.28\n",
            "Epoch 10, Iteration 200 Train Loss: 76.40\n",
            "Epoch 10, Iteration 210 Train Loss: 66.10\n",
            "Epoch 10, Iteration 220 Train Loss: 43.33\n",
            "Epoch 10, Iteration 230 Train Loss: 55.23\n",
            "Epoch 10, Iteration 240 Train Loss: 55.10\n",
            "Epoch 10, Iteration 250 Train Loss: 67.52\n",
            "Epoch 10, Iteration 260 Train Loss: 58.36\n",
            "Epoch 10, Iteration 270 Train Loss: 89.81\n",
            "Epoch 10, Iteration 280 Train Loss: 76.70\n",
            "Epoch 10, Iteration 290 Train Loss: 87.50\n",
            "Epoch 10, Iteration 300 Train Loss: 82.73\n",
            "Epoch 10, Iteration 310 Train Loss: 72.23\n",
            "Epoch 10, Iteration 320 Train Loss: 67.19\n",
            "Epoch 10, Iteration 330 Train Loss: 82.36\n",
            "Epoch 10, Iteration 340 Train Loss: 83.47\n",
            "Epoch 10, Iteration 350 Train Loss: 50.46\n",
            "Epoch 10, Iteration 360 Train Loss: 57.13\n",
            "New Epoch! Avg loss for the last 100 iterations: 76.48238170623779\n",
            "Epoch 11, Iteration 0 Train Loss: 19251.64\n",
            "Epoch 11, Iteration 10 Train Loss: 227.59\n",
            "Epoch 11, Iteration 20 Train Loss: 56.82\n",
            "Epoch 11, Iteration 30 Train Loss: 86.50\n",
            "Epoch 11, Iteration 40 Train Loss: 75.00\n",
            "Epoch 11, Iteration 50 Train Loss: 58.56\n",
            "Epoch 11, Iteration 60 Train Loss: 83.96\n",
            "Epoch 11, Iteration 70 Train Loss: 690.23\n",
            "Epoch 11, Iteration 80 Train Loss: 52.45\n",
            "Epoch 11, Iteration 90 Train Loss: 463.68\n",
            "Epoch 11, Iteration 100 Train Loss: 72.18\n",
            "Epoch 11, Iteration 110 Train Loss: 52.83\n",
            "Epoch 11, Iteration 120 Train Loss: 97.41\n",
            "Epoch 11, Iteration 130 Train Loss: 48.50\n",
            "Epoch 11, Iteration 140 Train Loss: 53.72\n",
            "Epoch 11, Iteration 150 Train Loss: 63.89\n",
            "Epoch 11, Iteration 160 Train Loss: 29.50\n",
            "Epoch 11, Iteration 170 Train Loss: 57.43\n",
            "Epoch 11, Iteration 180 Train Loss: 62.63\n",
            "Epoch 11, Iteration 190 Train Loss: 63.57\n",
            "Epoch 11, Iteration 200 Train Loss: 78.84\n",
            "Epoch 11, Iteration 210 Train Loss: 68.35\n",
            "Epoch 11, Iteration 220 Train Loss: 45.75\n",
            "Epoch 11, Iteration 230 Train Loss: 57.48\n",
            "Epoch 11, Iteration 240 Train Loss: 57.35\n",
            "Epoch 11, Iteration 250 Train Loss: 69.70\n",
            "Epoch 11, Iteration 260 Train Loss: 60.54\n",
            "Epoch 11, Iteration 270 Train Loss: 92.17\n",
            "Epoch 11, Iteration 280 Train Loss: 79.06\n",
            "Epoch 11, Iteration 290 Train Loss: 89.59\n",
            "Epoch 11, Iteration 300 Train Loss: 85.08\n",
            "Epoch 11, Iteration 310 Train Loss: 74.56\n",
            "Epoch 11, Iteration 320 Train Loss: 69.50\n",
            "Epoch 11, Iteration 330 Train Loss: 84.63\n",
            "Epoch 11, Iteration 340 Train Loss: 85.58\n",
            "Epoch 11, Iteration 350 Train Loss: 52.68\n",
            "Epoch 11, Iteration 360 Train Loss: 59.34\n",
            "New Epoch! Avg loss for the last 100 iterations: 78.72490440368652\n",
            "Epoch 12, Iteration 0 Train Loss: 19209.96\n",
            "Epoch 12, Iteration 10 Train Loss: 213.36\n",
            "Epoch 12, Iteration 20 Train Loss: 59.13\n",
            "Epoch 12, Iteration 30 Train Loss: 88.82\n",
            "Epoch 12, Iteration 40 Train Loss: 77.31\n",
            "Epoch 12, Iteration 50 Train Loss: 60.85\n",
            "Epoch 12, Iteration 60 Train Loss: 86.26\n",
            "Epoch 12, Iteration 70 Train Loss: 658.94\n",
            "Epoch 12, Iteration 80 Train Loss: 54.75\n",
            "Epoch 12, Iteration 90 Train Loss: 438.05\n",
            "Epoch 12, Iteration 100 Train Loss: 74.46\n",
            "Epoch 12, Iteration 110 Train Loss: 55.08\n",
            "Epoch 12, Iteration 120 Train Loss: 88.05\n",
            "Epoch 12, Iteration 130 Train Loss: 50.71\n",
            "Epoch 12, Iteration 140 Train Loss: 55.91\n",
            "Epoch 12, Iteration 150 Train Loss: 54.88\n",
            "Epoch 12, Iteration 160 Train Loss: 30.08\n",
            "Epoch 12, Iteration 170 Train Loss: 59.53\n",
            "Epoch 12, Iteration 180 Train Loss: 59.51\n",
            "Epoch 12, Iteration 190 Train Loss: 65.61\n",
            "Epoch 12, Iteration 200 Train Loss: 80.92\n",
            "Epoch 12, Iteration 210 Train Loss: 70.37\n",
            "Epoch 12, Iteration 220 Train Loss: 47.82\n",
            "Epoch 12, Iteration 230 Train Loss: 59.49\n",
            "Epoch 12, Iteration 240 Train Loss: 59.35\n",
            "Epoch 12, Iteration 250 Train Loss: 71.70\n",
            "Epoch 12, Iteration 260 Train Loss: 62.50\n",
            "Epoch 12, Iteration 270 Train Loss: 94.21\n",
            "Epoch 12, Iteration 280 Train Loss: 81.09\n",
            "Epoch 12, Iteration 290 Train Loss: 91.53\n",
            "Epoch 12, Iteration 300 Train Loss: 87.08\n",
            "Epoch 12, Iteration 310 Train Loss: 76.54\n",
            "Epoch 12, Iteration 320 Train Loss: 71.46\n",
            "Epoch 12, Iteration 330 Train Loss: 86.59\n",
            "Epoch 12, Iteration 340 Train Loss: 87.52\n",
            "Epoch 12, Iteration 350 Train Loss: 54.61\n",
            "Epoch 12, Iteration 360 Train Loss: 61.25\n",
            "New Epoch! Avg loss for the last 100 iterations: 80.69283000946045\n",
            "Epoch 13, Iteration 0 Train Loss: 19173.73\n",
            "Epoch 13, Iteration 10 Train Loss: 200.70\n",
            "Epoch 13, Iteration 20 Train Loss: 61.13\n",
            "Epoch 13, Iteration 30 Train Loss: 90.85\n",
            "Epoch 13, Iteration 40 Train Loss: 79.34\n",
            "Epoch 13, Iteration 50 Train Loss: 62.87\n",
            "Epoch 13, Iteration 60 Train Loss: 88.28\n",
            "Epoch 13, Iteration 70 Train Loss: 631.09\n",
            "Epoch 13, Iteration 80 Train Loss: 56.76\n",
            "Epoch 13, Iteration 90 Train Loss: 415.27\n",
            "Epoch 13, Iteration 100 Train Loss: 76.46\n",
            "Epoch 13, Iteration 110 Train Loss: 57.09\n",
            "Epoch 13, Iteration 120 Train Loss: 79.92\n",
            "Epoch 13, Iteration 130 Train Loss: 52.74\n",
            "Epoch 13, Iteration 140 Train Loss: 57.92\n",
            "Epoch 13, Iteration 150 Train Loss: 46.82\n",
            "Epoch 13, Iteration 160 Train Loss: 32.04\n",
            "Epoch 13, Iteration 170 Train Loss: 61.47\n",
            "Epoch 13, Iteration 180 Train Loss: 56.52\n",
            "Epoch 13, Iteration 190 Train Loss: 67.52\n",
            "Epoch 13, Iteration 200 Train Loss: 82.94\n",
            "Epoch 13, Iteration 210 Train Loss: 72.25\n",
            "Epoch 13, Iteration 220 Train Loss: 49.83\n",
            "Epoch 13, Iteration 230 Train Loss: 61.36\n",
            "Epoch 13, Iteration 240 Train Loss: 61.22\n",
            "Epoch 13, Iteration 250 Train Loss: 73.71\n",
            "Epoch 13, Iteration 260 Train Loss: 64.38\n",
            "Epoch 13, Iteration 270 Train Loss: 96.14\n",
            "Epoch 13, Iteration 280 Train Loss: 83.03\n",
            "Epoch 13, Iteration 290 Train Loss: 93.33\n",
            "Epoch 13, Iteration 300 Train Loss: 89.00\n",
            "Epoch 13, Iteration 310 Train Loss: 78.45\n",
            "Epoch 13, Iteration 320 Train Loss: 73.36\n",
            "Epoch 13, Iteration 330 Train Loss: 88.48\n",
            "Epoch 13, Iteration 340 Train Loss: 89.31\n",
            "Epoch 13, Iteration 350 Train Loss: 56.48\n",
            "Epoch 13, Iteration 360 Train Loss: 63.12\n",
            "New Epoch! Avg loss for the last 100 iterations: 82.5810107421875\n",
            "Epoch 14, Iteration 0 Train Loss: 19138.55\n",
            "Epoch 14, Iteration 10 Train Loss: 188.70\n",
            "Epoch 14, Iteration 20 Train Loss: 63.06\n",
            "Epoch 14, Iteration 30 Train Loss: 92.75\n",
            "Epoch 14, Iteration 40 Train Loss: 81.23\n",
            "Epoch 14, Iteration 50 Train Loss: 64.75\n",
            "Epoch 14, Iteration 60 Train Loss: 90.12\n",
            "Epoch 14, Iteration 70 Train Loss: 606.06\n",
            "Epoch 14, Iteration 80 Train Loss: 58.61\n",
            "Epoch 14, Iteration 90 Train Loss: 394.36\n",
            "Epoch 14, Iteration 100 Train Loss: 78.32\n",
            "Epoch 14, Iteration 110 Train Loss: 58.92\n",
            "Epoch 14, Iteration 120 Train Loss: 72.75\n",
            "Epoch 14, Iteration 130 Train Loss: 54.56\n",
            "Epoch 14, Iteration 140 Train Loss: 59.73\n",
            "Epoch 14, Iteration 150 Train Loss: 43.32\n",
            "Epoch 14, Iteration 160 Train Loss: 33.81\n",
            "Epoch 14, Iteration 170 Train Loss: 63.21\n",
            "Epoch 14, Iteration 180 Train Loss: 57.30\n",
            "Epoch 14, Iteration 190 Train Loss: 69.22\n",
            "Epoch 14, Iteration 200 Train Loss: 84.63\n",
            "Epoch 14, Iteration 210 Train Loss: 73.93\n",
            "Epoch 14, Iteration 220 Train Loss: 51.50\n",
            "Epoch 14, Iteration 230 Train Loss: 63.05\n",
            "Epoch 14, Iteration 240 Train Loss: 62.92\n",
            "Epoch 14, Iteration 250 Train Loss: 75.19\n",
            "Epoch 14, Iteration 260 Train Loss: 65.96\n",
            "Epoch 14, Iteration 270 Train Loss: 97.82\n",
            "Epoch 14, Iteration 280 Train Loss: 84.70\n",
            "Epoch 14, Iteration 290 Train Loss: 94.89\n",
            "Epoch 14, Iteration 300 Train Loss: 90.66\n",
            "Epoch 14, Iteration 310 Train Loss: 80.11\n",
            "Epoch 14, Iteration 320 Train Loss: 75.01\n",
            "Epoch 14, Iteration 330 Train Loss: 90.11\n",
            "Epoch 14, Iteration 340 Train Loss: 90.85\n",
            "Epoch 14, Iteration 350 Train Loss: 58.10\n",
            "Epoch 14, Iteration 360 Train Loss: 64.72\n",
            "New Epoch! Avg loss for the last 100 iterations: 84.21209358215332\n",
            "Epoch 15, Iteration 0 Train Loss: 19108.16\n",
            "Epoch 15, Iteration 10 Train Loss: 180.18\n",
            "Epoch 15, Iteration 20 Train Loss: 64.70\n",
            "Epoch 15, Iteration 30 Train Loss: 94.40\n",
            "Epoch 15, Iteration 40 Train Loss: 82.88\n",
            "Epoch 15, Iteration 50 Train Loss: 66.39\n",
            "Epoch 15, Iteration 60 Train Loss: 91.77\n",
            "Epoch 15, Iteration 70 Train Loss: 586.88\n",
            "Epoch 15, Iteration 80 Train Loss: 60.25\n",
            "Epoch 15, Iteration 90 Train Loss: 375.81\n",
            "Epoch 15, Iteration 100 Train Loss: 79.98\n",
            "Epoch 15, Iteration 110 Train Loss: 60.58\n",
            "Epoch 15, Iteration 120 Train Loss: 66.28\n",
            "Epoch 15, Iteration 130 Train Loss: 56.20\n",
            "Epoch 15, Iteration 140 Train Loss: 61.35\n",
            "Epoch 15, Iteration 150 Train Loss: 42.10\n",
            "Epoch 15, Iteration 160 Train Loss: 35.39\n",
            "Epoch 15, Iteration 170 Train Loss: 64.76\n",
            "Epoch 15, Iteration 180 Train Loss: 58.84\n",
            "Epoch 15, Iteration 190 Train Loss: 70.74\n",
            "Epoch 15, Iteration 200 Train Loss: 86.14\n",
            "Epoch 15, Iteration 210 Train Loss: 75.43\n",
            "Epoch 15, Iteration 220 Train Loss: 52.99\n",
            "Epoch 15, Iteration 230 Train Loss: 64.52\n",
            "Epoch 15, Iteration 240 Train Loss: 64.38\n",
            "Epoch 15, Iteration 250 Train Loss: 76.78\n",
            "Epoch 15, Iteration 260 Train Loss: 67.46\n",
            "Epoch 15, Iteration 270 Train Loss: 99.29\n",
            "Epoch 15, Iteration 280 Train Loss: 86.16\n",
            "Epoch 15, Iteration 290 Train Loss: 96.35\n",
            "Epoch 15, Iteration 300 Train Loss: 92.11\n",
            "Epoch 15, Iteration 310 Train Loss: 81.55\n",
            "Epoch 15, Iteration 320 Train Loss: 76.44\n",
            "Epoch 15, Iteration 330 Train Loss: 91.54\n",
            "Epoch 15, Iteration 340 Train Loss: 92.29\n",
            "Epoch 15, Iteration 350 Train Loss: 59.52\n",
            "Epoch 15, Iteration 360 Train Loss: 66.13\n",
            "New Epoch! Avg loss for the last 100 iterations: 85.65530792236328\n",
            "Epoch 16, Iteration 0 Train Loss: 19081.59\n",
            "Epoch 16, Iteration 10 Train Loss: 174.44\n",
            "Epoch 16, Iteration 20 Train Loss: 66.16\n",
            "Epoch 16, Iteration 30 Train Loss: 95.85\n",
            "Epoch 16, Iteration 40 Train Loss: 84.31\n",
            "Epoch 16, Iteration 50 Train Loss: 67.82\n",
            "Epoch 16, Iteration 60 Train Loss: 93.20\n",
            "Epoch 16, Iteration 70 Train Loss: 570.30\n",
            "Epoch 16, Iteration 80 Train Loss: 61.70\n",
            "Epoch 16, Iteration 90 Train Loss: 358.99\n",
            "Epoch 16, Iteration 100 Train Loss: 81.43\n",
            "Epoch 16, Iteration 110 Train Loss: 62.01\n",
            "Epoch 16, Iteration 120 Train Loss: 61.78\n",
            "Epoch 16, Iteration 130 Train Loss: 57.60\n",
            "Epoch 16, Iteration 140 Train Loss: 62.72\n",
            "Epoch 16, Iteration 150 Train Loss: 43.47\n",
            "Epoch 16, Iteration 160 Train Loss: 36.76\n",
            "Epoch 16, Iteration 170 Train Loss: 66.09\n",
            "Epoch 16, Iteration 180 Train Loss: 60.15\n",
            "Epoch 16, Iteration 190 Train Loss: 72.03\n",
            "Epoch 16, Iteration 200 Train Loss: 87.51\n",
            "Epoch 16, Iteration 210 Train Loss: 76.71\n",
            "Epoch 16, Iteration 220 Train Loss: 54.43\n",
            "Epoch 16, Iteration 230 Train Loss: 65.82\n",
            "Epoch 16, Iteration 240 Train Loss: 65.68\n",
            "Epoch 16, Iteration 250 Train Loss: 78.01\n",
            "Epoch 16, Iteration 260 Train Loss: 68.71\n",
            "Epoch 16, Iteration 270 Train Loss: 100.60\n",
            "Epoch 16, Iteration 280 Train Loss: 87.48\n",
            "Epoch 16, Iteration 290 Train Loss: 97.57\n",
            "Epoch 16, Iteration 300 Train Loss: 93.41\n",
            "Epoch 16, Iteration 310 Train Loss: 82.85\n",
            "Epoch 16, Iteration 320 Train Loss: 77.73\n",
            "Epoch 16, Iteration 330 Train Loss: 92.83\n",
            "Epoch 16, Iteration 340 Train Loss: 93.49\n",
            "Epoch 16, Iteration 350 Train Loss: 60.79\n",
            "Epoch 16, Iteration 360 Train Loss: 67.39\n",
            "New Epoch! Avg loss for the last 100 iterations: 86.93366062164307\n",
            "Epoch 17, Iteration 0 Train Loss: 19057.69\n",
            "Epoch 17, Iteration 10 Train Loss: 169.49\n",
            "Epoch 17, Iteration 20 Train Loss: 67.46\n",
            "Epoch 17, Iteration 30 Train Loss: 97.16\n",
            "Epoch 17, Iteration 40 Train Loss: 85.62\n",
            "Epoch 17, Iteration 50 Train Loss: 69.13\n",
            "Epoch 17, Iteration 60 Train Loss: 94.48\n",
            "Epoch 17, Iteration 70 Train Loss: 555.95\n",
            "Epoch 17, Iteration 80 Train Loss: 62.99\n",
            "Epoch 17, Iteration 90 Train Loss: 344.66\n",
            "Epoch 17, Iteration 100 Train Loss: 82.71\n",
            "Epoch 17, Iteration 110 Train Loss: 63.29\n",
            "Epoch 17, Iteration 120 Train Loss: 59.92\n",
            "Epoch 17, Iteration 130 Train Loss: 58.87\n",
            "Epoch 17, Iteration 140 Train Loss: 63.99\n",
            "Epoch 17, Iteration 150 Train Loss: 44.74\n",
            "Epoch 17, Iteration 160 Train Loss: 38.02\n",
            "Epoch 17, Iteration 170 Train Loss: 67.36\n",
            "Epoch 17, Iteration 180 Train Loss: 61.38\n",
            "Epoch 17, Iteration 190 Train Loss: 73.26\n",
            "Epoch 17, Iteration 200 Train Loss: 88.74\n",
            "Epoch 17, Iteration 210 Train Loss: 77.66\n",
            "Epoch 17, Iteration 220 Train Loss: 55.57\n",
            "Epoch 17, Iteration 230 Train Loss: 66.61\n",
            "Epoch 17, Iteration 240 Train Loss: 66.40\n",
            "Epoch 17, Iteration 250 Train Loss: 78.74\n",
            "Epoch 17, Iteration 260 Train Loss: 69.30\n",
            "Epoch 17, Iteration 270 Train Loss: 101.22\n",
            "Epoch 17, Iteration 280 Train Loss: 88.05\n",
            "Epoch 17, Iteration 290 Train Loss: 97.91\n",
            "Epoch 17, Iteration 300 Train Loss: 93.88\n",
            "Epoch 17, Iteration 310 Train Loss: 83.25\n",
            "Epoch 17, Iteration 320 Train Loss: 78.08\n",
            "Epoch 17, Iteration 330 Train Loss: 93.19\n",
            "Epoch 17, Iteration 340 Train Loss: 93.75\n",
            "Epoch 17, Iteration 350 Train Loss: 61.13\n",
            "Epoch 17, Iteration 360 Train Loss: 67.71\n",
            "New Epoch! Avg loss for the last 100 iterations: 87.64445472717286\n",
            "Epoch 18, Iteration 0 Train Loss: 19034.26\n",
            "Epoch 18, Iteration 10 Train Loss: 168.29\n",
            "Epoch 18, Iteration 20 Train Loss: 68.67\n",
            "Epoch 18, Iteration 30 Train Loss: 98.36\n",
            "Epoch 18, Iteration 40 Train Loss: 86.81\n",
            "Epoch 18, Iteration 50 Train Loss: 70.31\n",
            "Epoch 18, Iteration 60 Train Loss: 95.65\n",
            "Epoch 18, Iteration 70 Train Loss: 542.69\n",
            "Epoch 18, Iteration 80 Train Loss: 64.15\n",
            "Epoch 18, Iteration 90 Train Loss: 331.52\n",
            "Epoch 18, Iteration 100 Train Loss: 83.86\n",
            "Epoch 18, Iteration 110 Train Loss: 64.45\n",
            "Epoch 18, Iteration 120 Train Loss: 58.20\n",
            "Epoch 18, Iteration 130 Train Loss: 60.03\n",
            "Epoch 18, Iteration 140 Train Loss: 65.15\n",
            "Epoch 18, Iteration 150 Train Loss: 45.87\n",
            "Epoch 18, Iteration 160 Train Loss: 39.13\n",
            "Epoch 18, Iteration 170 Train Loss: 68.45\n",
            "Epoch 18, Iteration 180 Train Loss: 62.47\n",
            "Epoch 18, Iteration 190 Train Loss: 74.34\n",
            "Epoch 18, Iteration 200 Train Loss: 89.84\n",
            "Epoch 18, Iteration 210 Train Loss: 79.00\n",
            "Epoch 18, Iteration 220 Train Loss: 56.80\n",
            "Epoch 18, Iteration 230 Train Loss: 68.10\n",
            "Epoch 18, Iteration 240 Train Loss: 67.95\n",
            "Epoch 18, Iteration 250 Train Loss: 80.32\n",
            "Epoch 18, Iteration 260 Train Loss: 70.98\n",
            "Epoch 18, Iteration 270 Train Loss: 102.87\n",
            "Epoch 18, Iteration 280 Train Loss: 89.75\n",
            "Epoch 18, Iteration 290 Train Loss: 99.79\n",
            "Epoch 18, Iteration 300 Train Loss: 95.67\n",
            "Epoch 18, Iteration 310 Train Loss: 85.10\n",
            "Epoch 18, Iteration 320 Train Loss: 79.97\n",
            "Epoch 18, Iteration 330 Train Loss: 95.06\n",
            "Epoch 18, Iteration 340 Train Loss: 95.70\n",
            "Epoch 18, Iteration 350 Train Loss: 63.00\n",
            "Epoch 18, Iteration 360 Train Loss: 69.60\n",
            "New Epoch! Avg loss for the last 100 iterations: 89.14967735290527\n",
            "Epoch 19, Iteration 0 Train Loss: 19015.97\n",
            "Epoch 19, Iteration 10 Train Loss: 160.47\n",
            "Epoch 19, Iteration 20 Train Loss: 69.70\n",
            "Epoch 19, Iteration 30 Train Loss: 99.38\n",
            "Epoch 19, Iteration 40 Train Loss: 87.82\n",
            "Epoch 19, Iteration 50 Train Loss: 71.31\n",
            "Epoch 19, Iteration 60 Train Loss: 96.67\n",
            "Epoch 19, Iteration 70 Train Loss: 531.04\n",
            "Epoch 19, Iteration 80 Train Loss: 65.21\n",
            "Epoch 19, Iteration 90 Train Loss: 319.67\n",
            "Epoch 19, Iteration 100 Train Loss: 84.92\n",
            "Epoch 19, Iteration 110 Train Loss: 65.52\n",
            "Epoch 19, Iteration 120 Train Loss: 57.00\n",
            "Epoch 19, Iteration 130 Train Loss: 61.00\n",
            "Epoch 19, Iteration 140 Train Loss: 66.10\n",
            "Epoch 19, Iteration 150 Train Loss: 46.81\n",
            "Epoch 19, Iteration 160 Train Loss: 40.06\n",
            "Epoch 19, Iteration 170 Train Loss: 69.36\n",
            "Epoch 19, Iteration 180 Train Loss: 63.37\n",
            "Epoch 19, Iteration 190 Train Loss: 75.24\n",
            "Epoch 19, Iteration 200 Train Loss: 90.69\n",
            "Epoch 19, Iteration 210 Train Loss: 79.89\n",
            "Epoch 19, Iteration 220 Train Loss: 57.59\n",
            "Epoch 19, Iteration 230 Train Loss: 68.98\n",
            "Epoch 19, Iteration 240 Train Loss: 68.83\n",
            "Epoch 19, Iteration 250 Train Loss: 81.09\n",
            "Epoch 19, Iteration 260 Train Loss: 71.79\n",
            "Epoch 19, Iteration 270 Train Loss: 103.75\n",
            "Epoch 19, Iteration 280 Train Loss: 90.62\n",
            "Epoch 19, Iteration 290 Train Loss: 100.60\n",
            "Epoch 19, Iteration 300 Train Loss: 96.54\n",
            "Epoch 19, Iteration 310 Train Loss: 85.96\n",
            "Epoch 19, Iteration 320 Train Loss: 80.84\n",
            "Epoch 19, Iteration 330 Train Loss: 95.92\n",
            "Epoch 19, Iteration 340 Train Loss: 96.49\n",
            "Epoch 19, Iteration 350 Train Loss: 63.85\n",
            "Epoch 19, Iteration 360 Train Loss: 70.45\n",
            "New Epoch! Avg loss for the last 100 iterations: 90.00585456848144\n",
            "Epoch 20, Iteration 0 Train Loss: 18999.96\n",
            "Epoch 20, Iteration 10 Train Loss: 157.14\n",
            "Epoch 20, Iteration 20 Train Loss: 70.53\n",
            "Epoch 20, Iteration 30 Train Loss: 100.21\n",
            "Epoch 20, Iteration 40 Train Loss: 88.65\n",
            "Epoch 20, Iteration 50 Train Loss: 72.14\n",
            "Epoch 20, Iteration 60 Train Loss: 97.50\n",
            "Epoch 20, Iteration 70 Train Loss: 521.23\n",
            "Epoch 20, Iteration 80 Train Loss: 66.04\n",
            "Epoch 20, Iteration 90 Train Loss: 309.71\n",
            "Epoch 20, Iteration 100 Train Loss: 85.75\n",
            "Epoch 20, Iteration 110 Train Loss: 66.34\n",
            "Epoch 20, Iteration 120 Train Loss: 57.82\n",
            "Epoch 20, Iteration 130 Train Loss: 61.82\n",
            "Epoch 20, Iteration 140 Train Loss: 66.92\n",
            "Epoch 20, Iteration 150 Train Loss: 47.63\n",
            "Epoch 20, Iteration 160 Train Loss: 40.88\n",
            "Epoch 20, Iteration 170 Train Loss: 70.18\n",
            "Epoch 20, Iteration 180 Train Loss: 64.19\n",
            "Epoch 20, Iteration 190 Train Loss: 76.05\n",
            "Epoch 20, Iteration 200 Train Loss: 91.57\n",
            "Epoch 20, Iteration 210 Train Loss: 80.69\n",
            "Epoch 20, Iteration 220 Train Loss: 58.54\n",
            "Epoch 20, Iteration 230 Train Loss: 69.79\n",
            "Epoch 20, Iteration 240 Train Loss: 69.63\n",
            "Epoch 20, Iteration 250 Train Loss: 82.02\n",
            "Epoch 20, Iteration 260 Train Loss: 72.66\n",
            "Epoch 20, Iteration 270 Train Loss: 104.55\n",
            "Epoch 20, Iteration 280 Train Loss: 91.42\n",
            "Epoch 20, Iteration 290 Train Loss: 101.45\n",
            "Epoch 20, Iteration 300 Train Loss: 97.34\n",
            "Epoch 20, Iteration 310 Train Loss: 86.76\n",
            "Epoch 20, Iteration 320 Train Loss: 81.63\n",
            "Epoch 20, Iteration 330 Train Loss: 96.71\n",
            "Epoch 20, Iteration 340 Train Loss: 97.33\n",
            "Epoch 20, Iteration 350 Train Loss: 64.64\n",
            "Epoch 20, Iteration 360 Train Loss: 71.23\n",
            "New Epoch! Avg loss for the last 100 iterations: 90.80974922180175\n",
            "Epoch 21, Iteration 0 Train Loss: 18985.05\n",
            "Epoch 21, Iteration 10 Train Loss: 153.92\n",
            "Epoch 21, Iteration 20 Train Loss: 71.37\n",
            "Epoch 21, Iteration 30 Train Loss: 101.05\n",
            "Epoch 21, Iteration 40 Train Loss: 89.50\n",
            "Epoch 21, Iteration 50 Train Loss: 72.99\n",
            "Epoch 21, Iteration 60 Train Loss: 98.35\n",
            "Epoch 21, Iteration 70 Train Loss: 511.77\n",
            "Epoch 21, Iteration 80 Train Loss: 66.86\n",
            "Epoch 21, Iteration 90 Train Loss: 300.81\n",
            "Epoch 21, Iteration 100 Train Loss: 86.55\n",
            "Epoch 21, Iteration 110 Train Loss: 67.12\n",
            "Epoch 21, Iteration 120 Train Loss: 58.59\n",
            "Epoch 21, Iteration 130 Train Loss: 62.58\n",
            "Epoch 21, Iteration 140 Train Loss: 67.66\n",
            "Epoch 21, Iteration 150 Train Loss: 48.36\n",
            "Epoch 21, Iteration 160 Train Loss: 41.61\n",
            "Epoch 21, Iteration 170 Train Loss: 70.91\n",
            "Epoch 21, Iteration 180 Train Loss: 64.91\n",
            "Epoch 21, Iteration 190 Train Loss: 76.77\n",
            "Epoch 21, Iteration 200 Train Loss: 92.23\n",
            "Epoch 21, Iteration 210 Train Loss: 81.41\n",
            "Epoch 21, Iteration 220 Train Loss: 59.14\n",
            "Epoch 21, Iteration 230 Train Loss: 70.50\n",
            "Epoch 21, Iteration 240 Train Loss: 70.35\n",
            "Epoch 21, Iteration 250 Train Loss: 82.61\n",
            "Epoch 21, Iteration 260 Train Loss: 73.30\n",
            "Epoch 21, Iteration 270 Train Loss: 105.26\n",
            "Epoch 21, Iteration 280 Train Loss: 92.13\n",
            "Epoch 21, Iteration 290 Train Loss: 102.09\n",
            "Epoch 21, Iteration 300 Train Loss: 98.04\n",
            "Epoch 21, Iteration 310 Train Loss: 87.46\n",
            "Epoch 21, Iteration 320 Train Loss: 82.32\n",
            "Epoch 21, Iteration 330 Train Loss: 97.40\n",
            "Epoch 21, Iteration 340 Train Loss: 97.96\n",
            "Epoch 21, Iteration 350 Train Loss: 65.33\n",
            "Epoch 21, Iteration 360 Train Loss: 71.92\n",
            "New Epoch! Avg loss for the last 100 iterations: 91.49981792449951\n",
            "Epoch 22, Iteration 0 Train Loss: 18972.12\n",
            "Epoch 22, Iteration 10 Train Loss: 151.25\n",
            "Epoch 22, Iteration 20 Train Loss: 72.03\n",
            "Epoch 22, Iteration 30 Train Loss: 101.71\n",
            "Epoch 22, Iteration 40 Train Loss: 90.15\n",
            "Epoch 22, Iteration 50 Train Loss: 73.64\n",
            "Epoch 22, Iteration 60 Train Loss: 99.00\n",
            "Epoch 22, Iteration 70 Train Loss: 503.90\n",
            "Epoch 22, Iteration 80 Train Loss: 67.53\n",
            "Epoch 22, Iteration 90 Train Loss: 292.82\n",
            "Epoch 22, Iteration 100 Train Loss: 87.22\n",
            "Epoch 22, Iteration 110 Train Loss: 67.80\n",
            "Epoch 22, Iteration 120 Train Loss: 59.26\n",
            "Epoch 22, Iteration 130 Train Loss: 63.26\n",
            "Epoch 22, Iteration 140 Train Loss: 68.34\n",
            "Epoch 22, Iteration 150 Train Loss: 49.04\n",
            "Epoch 22, Iteration 160 Train Loss: 42.29\n",
            "Epoch 22, Iteration 170 Train Loss: 71.58\n",
            "Epoch 22, Iteration 180 Train Loss: 65.59\n",
            "Epoch 22, Iteration 190 Train Loss: 77.45\n",
            "Epoch 22, Iteration 200 Train Loss: 92.97\n",
            "Epoch 22, Iteration 210 Train Loss: 82.08\n",
            "Epoch 22, Iteration 220 Train Loss: 59.95\n",
            "Epoch 22, Iteration 230 Train Loss: 71.17\n",
            "Epoch 22, Iteration 240 Train Loss: 71.01\n",
            "Epoch 22, Iteration 250 Train Loss: 83.41\n",
            "Epoch 22, Iteration 260 Train Loss: 74.03\n",
            "Epoch 22, Iteration 270 Train Loss: 105.93\n",
            "Epoch 22, Iteration 280 Train Loss: 92.79\n",
            "Epoch 22, Iteration 290 Train Loss: 102.81\n",
            "Epoch 22, Iteration 300 Train Loss: 98.70\n",
            "Epoch 22, Iteration 310 Train Loss: 88.11\n",
            "Epoch 22, Iteration 320 Train Loss: 82.98\n",
            "Epoch 22, Iteration 330 Train Loss: 98.06\n",
            "Epoch 22, Iteration 340 Train Loss: 98.67\n",
            "Epoch 22, Iteration 350 Train Loss: 65.98\n",
            "Epoch 22, Iteration 360 Train Loss: 72.57\n",
            "New Epoch! Avg loss for the last 100 iterations: 92.16909423828125\n",
            "Epoch 23, Iteration 0 Train Loss: 18959.70\n",
            "Epoch 23, Iteration 10 Train Loss: 148.56\n",
            "Epoch 23, Iteration 20 Train Loss: 72.73\n",
            "Epoch 23, Iteration 30 Train Loss: 102.40\n",
            "Epoch 23, Iteration 40 Train Loss: 90.83\n",
            "Epoch 23, Iteration 50 Train Loss: 74.31\n",
            "Epoch 23, Iteration 60 Train Loss: 99.67\n",
            "Epoch 23, Iteration 70 Train Loss: 496.76\n",
            "Epoch 23, Iteration 80 Train Loss: 68.17\n",
            "Epoch 23, Iteration 90 Train Loss: 285.70\n",
            "Epoch 23, Iteration 100 Train Loss: 87.87\n",
            "Epoch 23, Iteration 110 Train Loss: 68.45\n",
            "Epoch 23, Iteration 120 Train Loss: 59.92\n",
            "Epoch 23, Iteration 130 Train Loss: 63.89\n",
            "Epoch 23, Iteration 140 Train Loss: 68.93\n",
            "Epoch 23, Iteration 150 Train Loss: 49.62\n",
            "Epoch 23, Iteration 160 Train Loss: 42.87\n",
            "Epoch 23, Iteration 170 Train Loss: 72.16\n",
            "Epoch 23, Iteration 180 Train Loss: 66.17\n",
            "Epoch 23, Iteration 190 Train Loss: 78.02\n",
            "Epoch 23, Iteration 200 Train Loss: 93.49\n",
            "Epoch 23, Iteration 210 Train Loss: 82.65\n",
            "Epoch 23, Iteration 220 Train Loss: 60.40\n",
            "Epoch 23, Iteration 230 Train Loss: 71.73\n",
            "Epoch 23, Iteration 240 Train Loss: 71.58\n",
            "Epoch 23, Iteration 250 Train Loss: 83.85\n",
            "Epoch 23, Iteration 260 Train Loss: 74.53\n",
            "Epoch 23, Iteration 270 Train Loss: 106.49\n",
            "Epoch 23, Iteration 280 Train Loss: 93.35\n",
            "Epoch 23, Iteration 290 Train Loss: 103.30\n",
            "Epoch 23, Iteration 300 Train Loss: 99.26\n",
            "Epoch 23, Iteration 310 Train Loss: 88.67\n",
            "Epoch 23, Iteration 320 Train Loss: 83.53\n",
            "Epoch 23, Iteration 330 Train Loss: 98.61\n",
            "Epoch 23, Iteration 340 Train Loss: 99.16\n",
            "Epoch 23, Iteration 350 Train Loss: 66.53\n",
            "Epoch 23, Iteration 360 Train Loss: 73.11\n",
            "New Epoch! Avg loss for the last 100 iterations: 92.71364753723145\n",
            "Epoch 24, Iteration 0 Train Loss: 18949.49\n",
            "Epoch 24, Iteration 10 Train Loss: 146.47\n",
            "Epoch 24, Iteration 20 Train Loss: 73.24\n",
            "Epoch 24, Iteration 30 Train Loss: 102.91\n",
            "Epoch 24, Iteration 40 Train Loss: 91.34\n",
            "Epoch 24, Iteration 50 Train Loss: 74.82\n",
            "Epoch 24, Iteration 60 Train Loss: 100.18\n",
            "Epoch 24, Iteration 70 Train Loss: 490.58\n",
            "Epoch 24, Iteration 80 Train Loss: 68.68\n",
            "Epoch 24, Iteration 90 Train Loss: 279.41\n",
            "Epoch 24, Iteration 100 Train Loss: 88.39\n",
            "Epoch 24, Iteration 110 Train Loss: 68.97\n",
            "Epoch 24, Iteration 120 Train Loss: 60.44\n",
            "Epoch 24, Iteration 130 Train Loss: 64.41\n",
            "Epoch 24, Iteration 140 Train Loss: 69.46\n",
            "Epoch 24, Iteration 150 Train Loss: 50.15\n",
            "Epoch 24, Iteration 160 Train Loss: 43.40\n",
            "Epoch 24, Iteration 170 Train Loss: 72.69\n",
            "Epoch 24, Iteration 180 Train Loss: 66.69\n",
            "Epoch 24, Iteration 190 Train Loss: 78.55\n",
            "Epoch 24, Iteration 200 Train Loss: 94.08\n",
            "Epoch 24, Iteration 210 Train Loss: 83.17\n",
            "Epoch 24, Iteration 220 Train Loss: 61.05\n",
            "Epoch 24, Iteration 230 Train Loss: 72.25\n",
            "Epoch 24, Iteration 240 Train Loss: 72.10\n",
            "Epoch 24, Iteration 250 Train Loss: 84.46\n",
            "Epoch 24, Iteration 260 Train Loss: 75.07\n",
            "Epoch 24, Iteration 270 Train Loss: 107.04\n",
            "Epoch 24, Iteration 280 Train Loss: 93.90\n",
            "Epoch 24, Iteration 290 Train Loss: 103.82\n",
            "Epoch 24, Iteration 300 Train Loss: 99.81\n",
            "Epoch 24, Iteration 310 Train Loss: 89.22\n",
            "Epoch 24, Iteration 320 Train Loss: 84.08\n",
            "Epoch 24, Iteration 330 Train Loss: 99.16\n",
            "Epoch 24, Iteration 340 Train Loss: 99.67\n",
            "Epoch 24, Iteration 350 Train Loss: 67.07\n",
            "Epoch 24, Iteration 360 Train Loss: 73.65\n",
            "New Epoch! Avg loss for the last 100 iterations: 93.25893753051758\n",
            "Oracle activations: 3948\n",
            "Runtime total_E activations: 3453.388158721049\n",
            "Runtime total_E_pred activations: 97330.4867842625\n",
            "Naive total_E activations: 3453.388158721049\n",
            "Naive total_E_pred activations: 4740.710587099603\n",
            "Dataset, train set, and test set size: 3680 2943 368\n",
            "Timeframe: 60min\n",
            "Minimal Application\n",
            "Naive vs. DL succesful activations: 0.0\n",
            "Maximum possible activations: 3948\n",
            "Predicted activations: 97820\n",
            "Successful activations: 0, 0.000%\n",
            "Failed activations: 95174, 97.295%\n",
            "Missed activations: 3948, 100.000%\n",
            "Naive predicted activations (usual actual energy average): 4881\n",
            "Naive successful activations (usual actual energy average): 1049, 21.491%\n",
            "Naive failed activations (usual actual energy average): 3832, 78.509%\n",
            "Naive missed activations (usual actual energy average): 2899, 73.430%\n",
            "Voltage overestimation rate: 100.000%\n",
            "Test MAPE power: 2.886589\n",
            "Test MAPE voltage: 4.594899\n",
            "Test MAPE current: 0.373926\n"
          ]
        }
      ],
      "source": [
        "#TSRV on Type 1 Models\n",
        "power_mape = []\n",
        "voltage_mape = []\n",
        "current_mape = []\n",
        "\n",
        "E_actual_list = []\n",
        "E_pred_list = []\n",
        "\n",
        "max_act_list = []\n",
        "pred_act_list = []\n",
        "succ_act_list = []\n",
        "\n",
        "pred_act_naive_list = []\n",
        "false_act_naive_list = []\n",
        "succ_act_naive_list = []\n",
        "\n",
        "#Set parameters\n",
        "batchsize = 8\n",
        "time_frame = '60min'\n",
        "time_frame_seconds = 3600\n",
        "n = 0\n",
        "splits = TimeSeriesSplit(n_splits=4)\n",
        "for train_index, test_index in splits.split(X):\n",
        "    n += 1\n",
        "    if n >= 1:\n",
        "        #Split train and test sets\n",
        "        X_train = X.iloc[train_index]\n",
        "        X_test = X.iloc[test_index]\n",
        "        y_train = y.iloc[train_index]\n",
        "        y_test = y.iloc[test_index]\n",
        "\n",
        "        X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "        y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
        "\n",
        "        #Set dataset bounds\n",
        "        train_bound_lower = y_train.index[0]\n",
        "        train_bound_upper = y_train.index[-1]\n",
        "        valid_bound_lower = y_valid.index[0]\n",
        "        valid_bound_upper = y_valid.index[-1]\n",
        "        test_bound_lower = y_test.index[0]\n",
        "        test_bound_upper = y_test.index[-1]\n",
        "\n",
        "        #Resample data\n",
        "        X_train = X_train.resample(time_frame).mean().dropna()\n",
        "        X_valid = X_valid.resample(time_frame).mean().dropna()\n",
        "        X_test = X_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "        y_train = y_train.resample(time_frame).mean().dropna()\n",
        "        y_valid = y_valid.resample(time_frame).mean().dropna()\n",
        "        y_test = y_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "        #Reshape data\n",
        "        X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "        X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
        "        X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "        # convert to tensor\n",
        "        X_train = torch.tensor(X_train)\n",
        "        y_train_index_labels = y_train.index\n",
        "        y_train_column_labels = y_train.columns\n",
        "        y_train = torch.tensor(y_train.values)\n",
        "\n",
        "        X_test = torch.tensor(X_test)\n",
        "        y_test_index_labels = y_test.index\n",
        "        y_test_column_labels = y_test.columns\n",
        "        y_test = torch.tensor(y_test.values)\n",
        "\n",
        "        # make datasets\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "        # Define the number of time steps for the spiking\n",
        "        num_steps = 50\n",
        "        num_inputs = X_train.shape[2]\n",
        "\n",
        "        # create new inctance of the SNN Class\n",
        "        model = Net(num_inputs, num_steps).to(device)\n",
        "\n",
        "        # define optimizer\n",
        "        optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
        "\n",
        "        # define loss function\n",
        "        def quantile_loss(y_true, y_pred, quantile=0.05):\n",
        "            error = y_true - y_pred\n",
        "            loss = torch.mean(torch.max(quantile * error, (quantile - 1) * error))\n",
        "            return loss\n",
        "        loss_fn = quantile_loss\n",
        "\n",
        "        # initialize histories\n",
        "        loss_hist = []\n",
        "        avg_loss_hist = []\n",
        "        acc_hist = []\n",
        "        num_epochs = 25\n",
        "\n",
        "        # put model into train mode\n",
        "        model.train()\n",
        "\n",
        "        # Train Loop\n",
        "        for epoch in range(num_epochs):\n",
        "            for i, (data, targets) in enumerate(iter(train_loader)):\n",
        "                # move to device\n",
        "                data = data.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # change to floats\n",
        "                data = data.float()\n",
        "                targets = targets.float()\n",
        "\n",
        "                _, _, _, mem = model(data)\n",
        "\n",
        "                loss_val = loss_fn(mem[-1], targets)\n",
        "\n",
        "                # Gradient calculation + weight update\n",
        "                optimizer.zero_grad()\n",
        "                loss_val.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Store loss history for future plotting\n",
        "                loss_hist.append(loss_val.item())\n",
        "\n",
        "                # Update loss plot\n",
        "                # update_loss_plot(loss_hist)\n",
        "\n",
        "                if i%10 == 0:\n",
        "                    print(f\"Epoch {epoch}, Iteration {i} Train Loss: {loss_val.item():.2f}\")\n",
        "                if len(loss_hist) > 100:\n",
        "                    avg_loss_hist.append(sum(loss_hist[-100:])/len(loss_hist[-100:]))\n",
        "                else:\n",
        "                    avg_loss_hist.append(0)\n",
        "\n",
        "            if len(loss_hist) > 100:\n",
        "                print(f'New Epoch! Avg loss for the last 100 iterations: {avg_loss_hist[-1]}')\n",
        "\n",
        "        # predictions\n",
        "        model.eval()\n",
        "\n",
        "        train_pred = []\n",
        "        test_pred = []\n",
        "        with torch.no_grad():\n",
        "            for data, target in train_dataset:\n",
        "                data = data.to(device)\n",
        "                data = data.float()\n",
        "                _, _, _, train_mem = model(data)\n",
        "                train_pred.append(train_mem[-1])\n",
        "\n",
        "            for data, target in test_dataset:\n",
        "                data = data.to(device)\n",
        "                data = data.float()\n",
        "                _, _, _, test_mem = model(data)\n",
        "                test_pred.append(test_mem[-1])\n",
        "\n",
        "        # Convert to numpy\n",
        "        train_pred = torch.cat(train_pred, dim=0)\n",
        "        train_pred = train_pred.numpy()\n",
        "        test_pred = torch.cat(test_pred, dim=0)\n",
        "        test_pred = test_pred.numpy()\n",
        "\n",
        "        # convert tensors back to dataframe\n",
        "        y_test = pd.DataFrame(y_test, index=y_test_index_labels, columns=y_test_column_labels)\n",
        "        y_train = pd.DataFrame(y_test, index=y_train_index_labels, columns=y_test_column_labels)\n",
        "\n",
        "        #Prepare data for runtime simulation\n",
        "        y_test['power pred'] = test_pred[:, 0]\n",
        "        y_test['V1 [mV] pred'] = test_pred[:, 1]\n",
        "        y_test['I1L [μA] pred'] = test_pred[:, 2]\n",
        "\n",
        "        days  = getMFC_data(y_test, test_pred)\n",
        "\n",
        "        v_test = df.loc[(((df.index >= test_bound_lower)) & (df.index <= test_bound_upper))]['V1 [mV]']\n",
        "        v_test = v_test.drop(v_test[(v_test.index > '2021-11-11') & (v_test.index < '2021-11-22 01:00:00')].index)\n",
        "        v_test = pd.DataFrame(v_test)/1E5\n",
        "        v_avg_true = v_test['V1 [mV]'].resample(time_frame).mean().dropna()\n",
        "        v_avg_pred = y_test['V1 [mV] pred']/1E5\n",
        "        C0 = [0.007000000000000006, 0.007000000000000006, 0.007000000000000006]\n",
        "\n",
        "        #Remove first and last entries of averaged data to prevent overestimation of available energy\n",
        "        v_avg_true = v_avg_true[1:][:-1]\n",
        "        v_avg_pred = v_avg_pred[1:][:-1]\n",
        "\n",
        "        #Run oracle model\n",
        "        max_act = oracle_simulate(v_test, C0)\n",
        "\n",
        "        #Call simulate function\n",
        "        pred_act, false_act, succ_act, total_E, total_E_pred = simulate(days, v_avg_true, v_avg_pred, v_test, C0)\n",
        "\n",
        "        #Run naive model\n",
        "        v_valid = df.loc[(((df.index >= valid_bound_lower)) & (df.index < valid_bound_upper))]['V1 [mV]']/1E5\n",
        "        pred_act_naive, false_act_naive, succ_act_naive, total_E = naive_simulate(days, v_avg_true, v_valid, v_test, C0)\n",
        "        print(\"Dataset, train set, and test set size:\", len(y_train) + len(y_valid) + len(y_test), len(y_train), len(y_test))\n",
        "        print('Timeframe:', time_frame)\n",
        "\n",
        "        print('Minimal Application')\n",
        "        print(\"Naive vs. DL succesful activations:\", succ_act/succ_act_naive)\n",
        "        #print('Predicted vs. Actual percent difference: %.3f%%' % ((total_E * 100 / total_E_pred) - 100))\n",
        "        print('Maximum possible activations:', max_act)\n",
        "        print('Predicted activations:', pred_act)\n",
        "        print('Successful activations: %d, %.3f%%' % (succ_act, succ_act * 100/pred_act))\n",
        "        print('Failed activations: %d, %.3f%%' % (false_act, false_act * 100/pred_act))\n",
        "        print('Missed activations: %d, %.3f%%' % (max_act - succ_act, (max_act - succ_act) * 100/max_act))\n",
        "\n",
        "        #Naive model\n",
        "        print('Naive predicted activations (usual actual energy average):', pred_act_naive)\n",
        "        print('Naive successful activations (usual actual energy average): %d, %.3f%%' % (succ_act_naive, succ_act_naive * 100/pred_act_naive))\n",
        "        print('Naive failed activations (usual actual energy average): %d, %.3f%%' % (false_act_naive, false_act_naive * 100/pred_act_naive))\n",
        "        print('Naive missed activations (usual actual energy average): %d, %.3f%%' % (max_act - succ_act_naive, (max_act - succ_act_naive) * 100/max_act))\n",
        "\n",
        "\n",
        "        print('Voltage overestimation rate: %.3f%%' % ((y_test['V1 [mV]'].values <= y_test['V1 [mV] pred']).mean() * 100))\n",
        "        print(\"Test MAPE power: %3f\" %  MAPE(y_test['power'].values.ravel(), y_test['power pred']))\n",
        "        print(\"Test MAPE voltage: %3f\" % MAPE(y_test['V1 [mV]'], y_test['V1 [mV] pred']))\n",
        "        print(\"Test MAPE current: %3f\" % MAPE(y_test['I1L [μA]'], y_test['I1L [μA] pred']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH8h9XNL7_IY"
      },
      "source": [
        "Old Untouched Code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqz6FbuV2biX",
        "outputId": "0b60520a-f0de-4cec-aebe-c75309e03e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            Power (uW)  Voltage (mV)  Current (uA)  \\\n",
            "timestamp                                                            \n",
            "2021-12-12 01:00:00-08:00  1110.313583   4311.206733   2564.759600   \n",
            "2021-12-12 02:00:00-08:00  1300.544442   4627.458561   2836.013973   \n",
            "2021-12-12 03:00:00-08:00  1252.099795   4729.874121   2649.654299   \n",
            "2021-12-12 04:00:00-08:00  1292.220860   4544.956065   2849.791582   \n",
            "2021-12-12 05:00:00-08:00  1242.752035   4392.519322   2855.785962   \n",
            "...                                ...           ...           ...   \n",
            "2022-01-03 20:00:00-08:00   322.122927   2027.706947   1588.639468   \n",
            "2022-01-03 21:00:00-08:00   367.964290   2024.127371   1832.313019   \n",
            "2022-01-03 22:00:00-08:00   384.467106   1967.737174   1973.739727   \n",
            "2022-01-03 23:00:00-08:00   351.081527   1870.863342   1873.352369   \n",
            "2022-01-04 00:00:00-08:00   397.646956   1770.018188   2241.944761   \n",
            "\n",
            "                           power_pred_med  voltage_pred_med  current_pred_med  \n",
            "timestamp                                                                      \n",
            "2021-12-12 01:00:00-08:00      696.517700       2931.670898       2921.070312  \n",
            "2021-12-12 02:00:00-08:00      687.016541       2881.141113       2935.356689  \n",
            "2021-12-12 03:00:00-08:00      687.016541       2881.141357       2935.356689  \n",
            "2021-12-12 04:00:00-08:00      687.016541       2881.141113       2935.356689  \n",
            "2021-12-12 05:00:00-08:00      691.397339       2829.531982       2985.304443  \n",
            "...                                   ...               ...               ...  \n",
            "2022-01-03 20:00:00-08:00      563.613342       2173.316895       2840.848633  \n",
            "2022-01-03 21:00:00-08:00      553.744751       2132.231689       2725.687988  \n",
            "2022-01-03 22:00:00-08:00      553.744751       2132.231689       2725.688232  \n",
            "2022-01-03 23:00:00-08:00      564.805969       2160.607910       2686.448975  \n",
            "2022-01-04 00:00:00-08:00      574.392090       2262.890625       3076.218750  \n",
            "\n",
            "[552 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "#Load and graph pretrained SNN models\n",
        "from keras.models import load_model\n",
        "\n",
        "time_frame = '60min'\n",
        "batchsize = 8\n",
        "\n",
        "X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "#X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis = 1)\n",
        "\n",
        "#Normalize Data\n",
        "X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
        "\n",
        "#Split train and test sets\n",
        "X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
        "y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
        "\n",
        "X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
        "\n",
        "#Resample data\n",
        "\n",
        "X_valid = X_valid.resample(time_frame).mean().dropna()\n",
        "y_valid = y_valid.resample(time_frame).mean().dropna()\n",
        "\n",
        "X_test = X_test.resample(time_frame).mean().dropna()\n",
        "y_test = y_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "#Define mv1\n",
        "mv1 = y_valid\n",
        "\n",
        "#Reshape data\n",
        "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
        "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# convert to tensor\n",
        "X_train = torch.tensor(X_train)\n",
        "y_train = torch.tensor(y_train.values)\n",
        "X_valid = torch.tensor(X_valid)\n",
        "y_valid = torch.tensor(y_valid.values)\n",
        "X_test = torch.tensor(X_test)\n",
        "y_test = torch.tensor(y_test.values)\n",
        "\n",
        "# make datasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "num_steps = 50\n",
        "num_inputs = X_train.shape[2]\n",
        "\n",
        "# create new inctance of the SNN Class\n",
        "model = Net(num_inputs, num_steps).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"drive/MyDrive/jLab Shared Docs/MFC Modeling/snn_60min_quant50\", map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "actuals = []\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, targets in valid_loader:\n",
        "\n",
        "        # prepare data\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        data = data.float()\n",
        "        targets = targets.float()\n",
        "\n",
        "        _, _, _, output = model(data)\n",
        "\n",
        "        output = output.cpu()\n",
        "        output = output.squeeze(1).detach()\n",
        "\n",
        "        prediction = output[-1]\n",
        "\n",
        "        actuals.append(targets)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "# Convert lists to tensors\n",
        "actuals = torch.cat(actuals, dim=0)\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "\n",
        "mv1[\"power_pred_med\"] = predictions[:, 0]\n",
        "mv1[\"voltage_pred_med\"] = predictions[:, 1]\n",
        "mv1[\"current_pred_med\"] = predictions[:, 2]\n",
        "#mv1 = mv1.loc[(mv1.index > '2022-01-04') & (mv1.index < '2022-01-06')]\n",
        "mv1 = mv1.loc[(mv1.index > '2021-12-12') & (mv1.index < '2021-12-14')]\n",
        "mv2 = mv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "nt7cvQdx4jlC",
        "outputId": "ca68fc26-2c18-4426-e64d-2bf9c0a6c1de"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib as mpl\n",
        "from matplotlib.ticker import FixedLocator\n",
        "mpl.use('Agg')\n",
        "#mpl.rc('font', **font)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.dates as md\n",
        "import datetime\n",
        "import numpy as np\n",
        "from pytz import timezone\n",
        "import pandas as pd\n",
        "import arrow\n",
        "import os\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "%matplotlib inline\n",
        "\n",
        "# Limits for graphs\n",
        "VOLTAGE_LIM = 400\n",
        "CURRENT_LIM = 150\n",
        "POWER_LIM = 40\n",
        "\n",
        "line_width = 0.5\n",
        "plt.title(\"1 hour prediction interval\")\n",
        "\n",
        "plt.close()\n",
        "plt.xlabel(\"Time\")\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3,figsize=(4,3), sharex=False)\n",
        "fig.autofmt_xdate()\n",
        "\n",
        "volt_color= 'tab:blue'\n",
        "\n",
        "amp_color = 'tab:red'\n",
        "\n",
        "\n",
        "volt_color1= 'tab:blue'\n",
        "volt_style1 = 'dashed'\n",
        "volt_color2= 'tab:green'\n",
        "volt_style2 = 'dashed'\n",
        "volt_color3= 'tab:red'\n",
        "volt_style3 = 'dashed'\n",
        "volt_color4= 'tab:orange'\n",
        "volt_style4 = 'dashed'\n",
        "\n",
        "#amp_color1 = 'tab:red'\n",
        "#amp_style1='dashed'\n",
        "#amp_color2 = 'tab:orange'\n",
        "#amp_style2='dashdot'\n",
        "\n",
        "ax1.fmt_xdata = md.DateFormatter('%m-%d')\n",
        "ax1.xaxis.set_major_formatter(md.DateFormatter('%m-%d'))\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.fmt_xdata = md.DateFormatter('%m-%d')\n",
        "ax2.xaxis.set_major_formatter(md.DateFormatter('%m-%d'))\n",
        "ax2.grid(True)\n",
        "\n",
        "ax3.fmt_xdata = md.DateFormatter('%m-%d')\n",
        "ax3.xaxis.set_major_formatter(md.DateFormatter('%m-%d'))\n",
        "ax3.grid(True)\n",
        "\n",
        "ax1.set_ylabel(\"Power (μW)\", fontsize=7.5, labelpad = 10)\n",
        "ax2.set_ylabel(\"Voltage (mV)\", fontsize=7.5, labelpad = 7)\n",
        "ax3.set_ylabel(\"Current (μA)\", fontsize=7.5, labelpad = 7)\n",
        "#ax3.set_xlabel(\"Date\", fontsize=10)\n",
        "\n",
        "#ax1.set_ylim(-10, 60)\n",
        "#ax2.set_ylim(-25, 450)\n",
        "#ax3.set_ylim(-10, 200)\n",
        "\n",
        "#ax3.set_xlim([datetime.date(2021, 12, 12), datetime.date(2021, 12, 14)])\n",
        "ax1.set_xlim([mv1.index[0], mv1.index[-1]])\n",
        "ax2.set_xlim([mv1.index[0], mv1.index[-1]])\n",
        "ax3.set_xlim([mv1.index[0], mv1.index[-1]])\n",
        "\n",
        "#ax1.plot(mv1.index, mv2[\"power_pred_lower\"], color='tab:red', ls = volt_style3, linewidth=line_width)\n",
        "#ax1.plot(mv1.index, mv2[\"power_pred_upper\"], color='tab:orange', ls = volt_style4, linewidth=line_width)\n",
        "ax1.plot(mv1.index, mv2[\"Power (uW)\"], color='tab:blue', ls = 'solid', linewidth=line_width)\n",
        "ax1.plot(mv1.index, mv2[\"power_pred_med\"], color='tab:green', ls = volt_style2, linewidth=line_width)\n",
        "#ax1.fill_between(mv1.index, mv2[\"power_pred_lower\"], mv2[\"power_pred_upper\"], color='grey', alpha=0.5)\n",
        "#ax1.legend(['lower bound', 'upper bound', 'actual', 'median predictions'], loc='lower center', prop={'size': 5.5}, ncol=2)\n",
        "\n",
        "#ax2.plot(mv1.index, mv2[\"voltage_pred_lower\"], color='tab:red', ls = volt_style3, linewidth=line_width)\n",
        "#ax2.plot(mv1.index, mv2[\"voltage_pred_upper\"], color='tab:orange', ls = volt_style4, linewidth=line_width)\n",
        "ax2.plot(mv1.index, mv2['Voltage (mV)'], color='tab:blue', ls = 'solid', linewidth=line_width)\n",
        "ax2.plot(mv1.index, mv2[\"voltage_pred_med\"], color='tab:green', ls = volt_style2, linewidth=line_width)\n",
        "#ax2.fill_between(mv1.index, mv2[\"voltage_pred_lower\"], mv2[\"voltage_pred_upper\"], color='grey', alpha=0.5)\n",
        "ax2.legend(['ground truth', 'median prediction'], loc='lower left', prop={'size': 6.6}, ncol=2, columnspacing=0.5)\n",
        "\n",
        "#ax3.plot(mv1.index, mv2[\"current_pred_lower\"], color='tab:red', ls = volt_style3, linewidth=line_width)\n",
        "#ax3.plot(mv1.index, mv2[\"current_pred_upper\"], color='tab:orange', ls = volt_style4, linewidth=line_width)\n",
        "ax3.plot(mv1.index, mv2['Current (uA)'], color='tab:blue', ls = 'solid', linewidth=line_width)\n",
        "ax3.plot(mv1.index, mv2[\"current_pred_med\"], color='tab:green', ls = volt_style2, linewidth=line_width)\n",
        "#ax3.fill_between(mv1.index, mv2[\"current_pred_lower\"], mv2[\"current_pred_upper\"], color='grey', alpha=0.5)\n",
        "#ax3.legend(['lower bound', 'upper bound', 'actual', 'median predictions'], loc='upper right', prop={'size': 5.5}, ncol=1)\n",
        "\n",
        "\n",
        "#Plot error\n",
        "#ax3.plot(mv1['timestamp'], mv1['error']/mv1['power'], color=volt_color2, ls = volt_style2)\n",
        "#ax3.legend(['error'], loc='upper right', prop={'size': 6})\n",
        "\n",
        "ax3.tick_params(axis='x', labelsize=7.5, rotation=30, pad = 0.1,)\n",
        "ax3.set_xticks(list(ax3.get_xticks()))\n",
        "for label in ax3.get_xticklabels():\n",
        "    label.set_horizontalalignment('center')\n",
        "\n",
        "ax1.tick_params(axis='y', labelsize=7.5, rotation=0, pad = 0.1)\n",
        "ax2.tick_params(axis='y', labelsize=7.5, rotation=0, pad = 0.1)\n",
        "ax3.tick_params(axis='y', labelsize=7.5, rotation=0, pad = 0.1)\n",
        "\n",
        "plt.tight_layout(pad=0.3, w_pad=0.5, h_pad=0.18)\n",
        "#plt.subplots_adjust(hspace=0.15)\n",
        "plt.savefig('twobat.pdf')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o7no5x6w7vP",
        "outputId": "b0991991-f166-49b0-de9c-b9bab6304ba3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/snn_3min_quant50.pth\n",
            "Voltage overestimation rate: 85.231%\n",
            "Test MAPE power: 0.537724\n",
            "Test MAPE voltage: 0.290864\n",
            "Test MAPE current: 0.316785\n",
            "Predicted vs. Actual Total Energy Percent Difference: 23.419%\n",
            "Predicted vs. Actual Total Voltage Percent Difference: 21.857%\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/snn_5min_quant50.pth\n",
            "Voltage overestimation rate: 73.179%\n",
            "Test MAPE power: 0.394733\n",
            "Test MAPE voltage: 0.214422\n",
            "Test MAPE current: 0.241269\n",
            "Predicted vs. Actual Total Energy Percent Difference: 16.691%\n",
            "Predicted vs. Actual Total Voltage Percent Difference: 12.812%\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/snn_15min_quant50.pth\n",
            "Voltage overestimation rate: 80.689%\n",
            "Test MAPE power: 0.328628\n",
            "Test MAPE voltage: 0.257437\n",
            "Test MAPE current: 0.227218\n",
            "Predicted vs. Actual Total Energy Percent Difference: 17.349%\n",
            "Predicted vs. Actual Total Voltage Percent Difference: 17.903%\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/snn_30min_quant50.pth\n",
            "Voltage overestimation rate: 87.489%\n",
            "Test MAPE power: 0.344126\n",
            "Test MAPE voltage: 0.280642\n",
            "Test MAPE current: 0.203606\n",
            "Predicted vs. Actual Total Energy Percent Difference: 21.921%\n",
            "Predicted vs. Actual Total Voltage Percent Difference: 21.535%\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/snn_60min_quant50.pth\n",
            "Voltage overestimation rate: 86.775%\n",
            "Test MAPE power: 0.404176\n",
            "Test MAPE voltage: 0.210382\n",
            "Test MAPE current: 0.183518\n",
            "Predicted vs. Actual Total Energy Percent Difference: 32.517%\n",
            "Predicted vs. Actual Total Voltage Percent Difference: 16.774%\n"
          ]
        }
      ],
      "source": [
        "#SNN Peformance Metrics\n",
        "from keras.models import load_model\n",
        "\n",
        "#Set parameters\n",
        "batchsize_list = [300, 150, 50, 20, 8]\n",
        "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
        "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
        "n = 0\n",
        "\n",
        "snn_power_mape_list = []\n",
        "snn_volt_mape_list = []\n",
        "snn_curr_mape_list = []\n",
        "\n",
        "for j in range(len(batchsize_list)):\n",
        "    batchsize = batchsize_list[j]\n",
        "    time_frame = time_frame_list[j]\n",
        "    time_frame_seconds = time_frame_seconds_list[j]\n",
        "\n",
        "    X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "    #X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "    y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis = 1)\n",
        "\n",
        "    #Normalize Data\n",
        "    X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
        "\n",
        "    #Split train and test sets\n",
        "    X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
        "    y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
        "\n",
        "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
        "\n",
        "    #Calculate actual energy generated in test set\n",
        "    E_actual = 0\n",
        "    for i in range(len(y_test) - 1):\n",
        "      t = (y_test.index[i+1] - y_test.index[i]).total_seconds()\n",
        "      if t < 180:\n",
        "        E_actual += y_test['Power (uW)'][i] * t\n",
        "\n",
        "    #Resample data\n",
        "\n",
        "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
        "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
        "\n",
        "    X_test = X_test.resample(time_frame).mean().dropna()\n",
        "    y_test = y_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "    #Define mv1\n",
        "    mv1 = y_test\n",
        "\n",
        "    #Reshape data\n",
        "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
        "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    # convert to tensor\n",
        "    X_train = torch.tensor(X_train)\n",
        "    y_train = torch.tensor(y_train.values)\n",
        "    X_valid = torch.tensor(X_valid)\n",
        "    y_valid = torch.tensor(y_valid.values)\n",
        "    X_test = torch.tensor(X_test)\n",
        "    y_test = torch.tensor(y_test.values)\n",
        "\n",
        "    # make datasets\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    num_steps = 50\n",
        "    num_inputs = X_train.shape[2]\n",
        "\n",
        "    # create new inctance of the SNN Class\n",
        "    model = Net(num_inputs, num_steps).to(device)\n",
        "\n",
        "    file = 'drive/MyDrive/jLab Shared Docs/MFC Modeling/snn_' + time_frame_list[j] + '_quant50.pth'\n",
        "    print(file)\n",
        "\n",
        "    checkpoint = torch.load(file, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    #model.load_state_dict(torch.load(file, map_location=torch.device('cpu'))['model_state_dict'])\n",
        "    model.eval()\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "\n",
        "            # prepare data\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            data = data.float()\n",
        "            targets = targets.float()\n",
        "\n",
        "            _, _, _, output = model(data)\n",
        "\n",
        "            output = output.cpu()\n",
        "            output = output.squeeze(1).detach()\n",
        "\n",
        "            prediction = output[-1]\n",
        "\n",
        "            actuals.append(targets)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    actuals = torch.cat(actuals, dim=0)\n",
        "    predictions = torch.cat(predictions, dim=0)\n",
        "\n",
        "    mv1[\"power_pred_med_\" + time_frame_list[j]] = predictions[:, 0].numpy()\n",
        "    mv1[\"voltage_pred_med_\" + time_frame_list[j]] = predictions[:, 1].numpy()\n",
        "    mv1[\"current_pred_med_\" + time_frame_list[j]] = predictions[:, 2].numpy()\n",
        "\n",
        "    print('Voltage overestimation rate: %.3f%%' % ((mv1['Voltage (mV)'].values <= mv1[\"voltage_pred_med_\" + time_frame_list[j]]).mean() * 100))\n",
        "    print(\"Test MAPE power: %3f\" %  MAPE(mv1['Power (uW)'].values.ravel(), mv1[\"power_pred_med_\" + time_frame_list[j]]))\n",
        "    print(\"Test MAPE voltage: %3f\" % MAPE(mv1['Voltage (mV)'], mv1[\"voltage_pred_med_\" + time_frame_list[j]]))\n",
        "    print(\"Test MAPE current: %3f\" % MAPE(mv1['Current (uA)'], mv1[\"current_pred_med_\" + time_frame_list[j]]))\n",
        "\n",
        "    E_pred = 0\n",
        "    for i in range(len(mv1) - 1):\n",
        "      t = (mv1.index[i+1] - mv1.index[i]).total_seconds()\n",
        "      if t <= time_frame_seconds + 50:\n",
        "        E_pred += mv1[\"power_pred_med_\" + time_frame_list[j]][i] * t\n",
        "\n",
        "    print('Predicted vs. Actual Total Energy Percent Difference: %.3f%%' % ((E_pred - E_actual) * 100 / E_actual))\n",
        "\n",
        "    V_actual = mv1['Voltage (mV)'].mean()\n",
        "    V_pred = mv1[\"voltage_pred_med_\" + time_frame_list[j]].mean()\n",
        "    print('Predicted vs. Actual Total Voltage Percent Difference: %.3f%%' % ((V_pred - V_actual) * 100 / V_actual))\n",
        "    #print(mv1)\n",
        "    #mv1 = mv1.loc[(mv1.index > '2022-01-04') & (mv1.index < '2022-01-06')]\n",
        "    #mv1 = mv1.loc[(mv1.index > '2021-12-12') & (mv1.index < '2021-12-14')]\n",
        "    #mv2 = mv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTUrclS17xSf"
      },
      "outputs": [],
      "source": [
        "#Get total spikes for pretrained models\n",
        "from keras.models import load_model\n",
        "import time\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#Set parameters\n",
        "batchsize_list = [300, 150, 50, 20, 8]\n",
        "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
        "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
        "n = 0\n",
        "\n",
        "power_mape_list = []\n",
        "volt_mape_list = []\n",
        "curr_mape_list = []\n",
        "total_spk_snn_list = []\n",
        "\n",
        "for j in range(len(batchsize_list)):\n",
        "  if j>=0:\n",
        "    batchsize = batchsize_list[j]\n",
        "    time_frame = time_frame_list[j]\n",
        "    time_frame_seconds = time_frame_seconds_list[j]\n",
        "\n",
        "    X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "    #X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "    y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis = 1)\n",
        "\n",
        "    #Normalize Data\n",
        "    X_normalized = ((X - X.min()) / (X.max() - X.min()))\n",
        "\n",
        "    #Split train and test sets\n",
        "    X_train, X_test = train_test_split(X_normalized, test_size=0.3, shuffle=False)\n",
        "    y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
        "\n",
        "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
        "\n",
        "    #Resample data\n",
        "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
        "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
        "\n",
        "    X_test = X_test.resample(time_frame).mean().dropna()\n",
        "    y_test = y_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "    #Reshape data\n",
        "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
        "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    # convert to tensor\n",
        "    X_train = torch.tensor(X_train)\n",
        "    y_train = torch.tensor(y_train.values)\n",
        "    X_valid = torch.tensor(X_valid)\n",
        "    y_valid = torch.tensor(y_valid.values)\n",
        "    X_test = torch.tensor(X_test)\n",
        "    y_test = torch.tensor(y_test.values)\n",
        "\n",
        "    # make datasets\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=False)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    num_steps = 50\n",
        "    num_inputs = X_train.shape[2]\n",
        "\n",
        "    # create new inctance of the SNN Class\n",
        "    model = Net(num_inputs, num_steps).to(device)\n",
        "\n",
        "    file = 'drive/MyDrive/jLab Shared Docs/MFC Modeling/snn_' + time_frame_list[j] + '_quant50.pth'\n",
        "\n",
        "    checkpoint = torch.load(file, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "    #model.load_state_dict(torch.load(file, map_location=torch.device('cpu'))['model_state_dict'])\n",
        "    model.eval()\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "\n",
        "    total_spk_snn = 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            n += 1\n",
        "            if n==1:\n",
        "              # prepare data\n",
        "              data = data.to(device)\n",
        "              targets = targets.to(device)\n",
        "\n",
        "              data = data.float()\n",
        "              targets = targets.float()\n",
        "\n",
        "              data = data[0]\n",
        "\n",
        "              spk1, spk2, spk3, output = model(data)\n",
        "\n",
        "              total_spk_snn = (spk1.sum() + spk2.sum() + spk3.sum() + output.sum()).item() # count up the total number of spikes\n",
        "\n",
        "    total_spk_snn_list.append(total_spk_snn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awiw5827BxeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1388a33f-78ab-4f94-c5ef-f4517da58ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/lstm8_3min_quant50\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_12_input (InputLayer)  [(None, 1, 20)]           0         \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, 200)               176800    \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 197203 (770.32 KB)\n",
            "Trainable params: 197203 (770.32 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/lstm8_5min_quant50\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4_input (InputLayer)   [(None, 1, 20)]           0         \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 200)               176800    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 197203 (770.32 KB)\n",
            "Trainable params: 197203 (770.32 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/lstm8_15min_quant50\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_22_input (InputLayer)  [(None, 1, 20)]           0         \n",
            "                                                                 \n",
            " lstm_22 (LSTM)              (None, 200)               176800    \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 197203 (770.32 KB)\n",
            "Trainable params: 197203 (770.32 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/lstm8_30min_quant50\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2_input (InputLayer)   [(None, 1, 20)]           0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 200)               176800    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 197203 (770.32 KB)\n",
            "Trainable params: 197203 (770.32 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "drive/MyDrive/jLab Shared Docs/MFC Modeling/lstm8_60min_quant50\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_5_input (InputLayer)   [(None, 1, 20)]           0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 200)               176800    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 197203 (770.32 KB)\n",
            "Trainable params: 197203 (770.32 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#Compass model flops\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "power_mape = []\n",
        "voltage_mape = []\n",
        "current_mape = []\n",
        "\n",
        "E_actual_list = []\n",
        "E_pred_list = []\n",
        "\n",
        "max_act_list = []\n",
        "pred_act_list = []\n",
        "succ_act_list = []\n",
        "\n",
        "pred_act_naive_list = []\n",
        "false_act_naive_list = []\n",
        "succ_act_naive_list = []\n",
        "\n",
        "#Set parameters\n",
        "batchsize_list = [300, 150, 50, 20, 8]\n",
        "time_frame_list = ['3min', '5min', '15min', '30min', '60min']\n",
        "time_frame_seconds_list = [180, 300, 900, 1800, 3600]\n",
        "n = 0\n",
        "\n",
        "for j in range(len(batchsize_list)):\n",
        "  if j>=0:\n",
        "    batchsize = batchsize_list[j]\n",
        "    time_frame = time_frame_list[j]\n",
        "    time_frame_seconds = time_frame_seconds_list[j]\n",
        "\n",
        "    X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"],df[\"EC - 1h\"], df[\"EC - 2h\"], df[\"EC - 3h\"], df[\"raw_VWC - 1h\"], df[\"raw_VWC - 2h\"], df[\"raw_VWC - 3h\"], df[\"temp - 1h\"], df[\"temp - 2h\"], df[\"temp - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "    #X = pd.concat([df[\"power - 1h\"], df[\"power - 2h\"], df[\"power - 3h\"], df[\"V1 - 1h\"], df[\"V1 - 2h\"], df[\"V1 - 3h\"], df[\"I1L - 1h\"], df[\"I1L - 2h\"], df[\"I1L - 3h\"], df[\"tsd\"], df[\"hour\"]], axis = 1)\n",
        "    y = pd.concat([df[\"Power (uW)\"], df['Voltage (mV)'], df['Current (uA)']], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "    #Split train and test sets\n",
        "    X_train, X_test = train_test_split(X, test_size=0.3, shuffle=False)\n",
        "    y_train, y_test = train_test_split(y, test_size=0.3, shuffle=False)\n",
        "\n",
        "    X_valid, X_test = train_test_split(X_test, test_size=0.5, shuffle=False)\n",
        "    y_valid, y_test = train_test_split(y_test, test_size=0.5, shuffle=False)\n",
        "\n",
        "    #Resample data\n",
        "    X_valid = X_valid.resample(time_frame).mean().dropna()\n",
        "    y_valid = y_valid.resample(time_frame).mean().dropna()\n",
        "\n",
        "    X_test = X_test.resample(time_frame).mean().dropna()\n",
        "    y_test = y_test.resample(time_frame).mean().dropna()\n",
        "\n",
        "    #Reshape data\n",
        "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_valid = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))\n",
        "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    def quantile_loss(y_true, y_pred, quantile = 0.5):\n",
        "      error = y_true - y_pred\n",
        "      return K.mean(K.maximum(quantile * error, (quantile - 1) * error), axis=-1)\n",
        "\n",
        "    file = 'drive/MyDrive/jLab Shared Docs/MFC Modeling/lstm8_' + time_frame_list[j] + '_quant50'\n",
        "    print(file)\n",
        "    model = load_model(file, custom_objects={'quantile_loss': quantile_loss})\n",
        "\n",
        "    layers = list(map(lambda x: x.name, model.layers))\n",
        "    intermediate_output = tf.keras.Model(model.input, model.output)\n",
        "    print(intermediate_output.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inf_freq = ['60min', '30min', '15min', '5min', '3min']\n",
        "#inf_freq = [1, 2, 4, 12, 20]\n",
        "micro_J_per_hour_local = [633.6, 1267.2, 2534.4, 7603.2, 12672]\n",
        "micro_J_per_hour_edge = [940, 1880, 3760, 11280, 18800]\n",
        "energy_save = list(reversed([val/197203 for val in total_spk_snn_list]))\n",
        "\n",
        "snn_micro_J_per_hour_local = [micro_J_per_hour_local[i] * energy_save[i] for i in range(len(micro_J_per_hour_local))]\n",
        "snn_micro_J_per_hour_edge = [micro_J_per_hour_edge[i] * energy_save[i] for i in range(len(micro_J_per_hour_edge))]"
      ],
      "metadata": {
        "id": "rXWE2UTcZ6m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(inf_freq, micro_J_per_hour_local, marker='o', label=\"Local\")\n",
        "plt.plot(inf_freq, micro_J_per_hour_edge, marker='o', label=\"Edge\")\n",
        "#plt.plot(inf_freq, snn_micro_J_per_hour_local, marker='o')\n",
        "#plt.plot(inf_freq, snn_micro_J_per_hour_edge, marker='o')\n",
        "\n",
        "#plt.xticks([1, 2, 4, 12, 20], ['60min', '30min', '15min', '5min', '3min'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Model time horizon/inference frequency')\n",
        "plt.ylabel('Energy consumption per hour (μJ)')\n",
        "plt.legend(loc='upper left', fontsize='small', title='Legend')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "hre2aXrKfX8J",
        "outputId": "384316f6-73f9-4238-beef-4ef80f2e4679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(inf_freq, micro_J_per_hour_local, marker='o')\n",
        "#plt.plot(inf_freq, micro_J_per_hour_edge, marker='o')\n",
        "plt.plot(inf_freq, snn_micro_J_per_hour_local, marker='o', label=\"SNN Local\")\n",
        "plt.plot(inf_freq, snn_micro_J_per_hour_edge, marker='o', label=\"SNN Edge\")\n",
        "\n",
        "#plt.xticks([1, 2, 4, 12, 20], ['60min', '30min', '15min', '5min', '3min'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Model time horizon/inference frequency')\n",
        "plt.ylabel('Energy consumption per hour (μJ)')\n",
        "plt.legend(loc='upper left', fontsize='small', title='Legend')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "_8V-u56yjmrN",
        "outputId": "e86b306f-0cb8-4807-83db-73f90f522e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}